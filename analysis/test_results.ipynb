{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lpips\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/e/e0425222/CS4243-project\")\n",
    "from utils.dataset_utils.AnimalDataset import AnimalDataset\n",
    "from utils.train_utils.model_utils import Conv2dBlock, GatedConv2d, GatedConv2dBlock, GatedUpConv2dBlock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "In this notebook, we calculate statistics and get sample images using the test dataset, and provide literature and context on each model trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:3\"\n",
    "SEED = 0\n",
    "SAMPLE_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_test.txt\",\n",
    "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
    "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
    "    file_prefix = \"frogs_\",\n",
    "    image_dimension = 64,\n",
    "    concat_mask = True,\n",
    "    random_noise = False,\n",
    "    require_init = False,\n",
    "    drops = [],\n",
    "    seed = SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /home/e/e0425222/miniconda3/envs/env/lib/python3.7/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "VGG_LPIPS = lpips.LPIPS(net = 'vgg').to(DEVICE)\n",
    "METRICS = {\n",
    "    \"Peak SnR (Whole)\" : lambda img, gt, mask : torchmetrics.functional.peak_signal_noise_ratio(img * (1-mask) + gt * mask, gt),\n",
    "    \"L2 loss (Whole)\" : lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask) + gt * mask, gt),\n",
    "    \"L2 loss (Mask)\" : lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum(),\n",
    "    \"L1 loss (Whole)\" : lambda img, gt, mask : nn.functional.l1_loss(img * (1-mask) + gt * mask, gt),\n",
    "    \"L1 loss (Mask)\" : lambda img, gt, mask : nn.functional.l1_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum(),\n",
    "    \"LPIPS (Whole)\" : (lambda img, gt, mask : VGG_LPIPS(img * (1-mask) + gt * mask, gt).mean()),\n",
    "}\n",
    "\n",
    "def sample_images(generator, device, dataset, filename, sample_size = 16):\n",
    "    loader = DataLoader(dataset, batch_size = sample_size, shuffle = False, worker_init_fn = lambda id: np.random.seed(seed))\n",
    "    generator.eval()\n",
    "    generator.to(device)\n",
    "\n",
    "    batch = next(iter(loader))\n",
    "    with torch.no_grad():\n",
    "        # input and ground truth\n",
    "        input_batched = batch[\"image\"]\n",
    "        ground_truth_batched = batch[\"reconstructed\"]\n",
    "        mask_batched = batch[\"mask\"]\n",
    "\n",
    "        # move tensors to device\n",
    "        input_batched = input_batched.to(device)\n",
    "        ground_truth_batched = ground_truth_batched.to(device)\n",
    "        mask_batched = mask_batched.to(device)\n",
    "\n",
    "        # ===== FORWARD PASS =====\n",
    "\n",
    "        # 1. reshape to channel first\n",
    "        input_batched = input_batched.permute(0, 3, 1, 2)\n",
    "        ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
    "        mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
    "\n",
    "        # 2. predict    \n",
    "        output_batched = generator(input_batched)\n",
    "        spliced_batched = ((1-mask_batched) * output_batched) + (mask_batched * ground_truth_batched) \n",
    "\n",
    "        batched_predictions = torch.cat([\n",
    "            input_batched[:sample_size, 0:3,:,:], # can be 4 channels\n",
    "            ground_truth_batched[:sample_size,:,:,:],  # 3 channels\n",
    "            spliced_batched[:sample_size,:,:,:]], dim = 0) \n",
    "        \n",
    "        image_array = torchvision.utils.make_grid(batched_predictions, nrow = sample_size, padding = 10)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image_array.permute(1,2,0).cpu()) # plot with channel first\n",
    "        plt.savefig(filename, dpi = 2000)\n",
    "        plt.close()\n",
    "                \n",
    "\n",
    "def compute_metrics(generator, device, metrics, dataset):\n",
    "    loader = DataLoader(dataset, batch_size = 64)\n",
    "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
    "    generator.eval()\n",
    "    generator.to(device)\n",
    "    with torch.no_grad():\n",
    "            batches = 0\n",
    "            for _, batch in enumerate(loader, 1):\n",
    "                \n",
    "                batches += 1\n",
    "\n",
    "                # input and ground truth\n",
    "                input_batched = batch[\"image\"]\n",
    "                ground_truth_batched = batch[\"reconstructed\"]\n",
    "                mask_batched = batch[\"mask\"]\n",
    "\n",
    "                # move tensors to device\n",
    "                input_batched = input_batched.to(device)\n",
    "                ground_truth_batched = ground_truth_batched.to(device)\n",
    "                mask_batched = mask_batched.to(device)\n",
    "\n",
    "                # ===== FORWARD PASS =====\n",
    "\n",
    "                # 1. reshape to channel first\n",
    "                input_batched = input_batched.permute(0, 3, 1, 2)\n",
    "                ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
    "                mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
    "\n",
    "                # 2. predict    \n",
    "                output_batched = generator(input_batched)\n",
    "\n",
    "                # 3. evaluate\n",
    "                for key, func in metrics.items():\n",
    "                    running_results[key] += func(output_batched, ground_truth_batched, mask_batched).detach().item()\n",
    "\n",
    "                args = \"\"\n",
    "                for key, _ in running_results.items():\n",
    "                    args += key + \": \" + str(running_results[key]/batches) + \"   \"\n",
    "                print(f\"\\r{batches}/{len(loader)}: \" + args, end = '', flush = True)\n",
    "                \n",
    "    # normalise numbers by batch\n",
    "    for key, _ in running_results.items():\n",
    "        running_results[key] /= batches\n",
    "\n",
    "    return running_results   \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric as torch_g\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "\n",
    "class GNNBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, activation = nn.ReLU):\n",
    "\n",
    "        super(GNNBlock, self).__init__()\n",
    "        self.conv = gnn.GINConv(nn.Sequential(nn.Linear(in_channels, out_channels), activation()))\n",
    "        self.post1 = nn.Sequential(nn.Linear(out_channels, out_channels), activation())\n",
    "        self.post2 = nn.Sequential(nn.Linear(out_channels, out_channels), activation())\n",
    "\n",
    "    def forward(self, input_tensor, adj):\n",
    "        # convert adj to sparse\n",
    "        device = input_tensor.device\n",
    "        edge_index, edge_attr = torch_g.utils.dense_to_sparse(adj)\n",
    "        edge_index = edge_index.long().to(device)\n",
    "        b, hw, c = input_tensor.shape\n",
    "        x = input_tensor.reshape(b * hw, c) # (b x hw x c) -> (bhw x c)\n",
    "\n",
    "        # forward\n",
    "        x = self.conv(x, edge_index)\n",
    "        x = self.post1(x)\n",
    "        x = self.post2(x)\n",
    "\n",
    "        # reshape back\n",
    "        x = x.reshape(b, hw, c)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GatedGraphConvModule(nn.Module):\n",
    "    \"\"\"\n",
    "    This module implements GNN convolution on images using local, global and channel features to predict\n",
    "    the adjacency tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, kernel_size, stride, padding, dilation, activation = nn.ReLU):\n",
    "\n",
    "        super(GatedGraphConvModule, self).__init__()\n",
    "\n",
    "\n",
    "        # incidence matrix\n",
    "        self.feature_conv = GatedConv2dBlock(channels, channels, kernel_size = kernel_size, stride = stride, padding = padding, dilation = dilation, activation = activation)\n",
    "        self.edge_conv = GatedConv2dBlock(channels, channels, kernel_size = kernel_size, stride = stride, padding = padding, dilation = dilation, activation = activation)\n",
    "        self.scaleconv = GatedConv2d(channels, 1, kernel_size = 1, stride = 1, padding = 0, dilation = 1)\n",
    "        self.offsetconv = GatedConv2d(channels, 1, kernel_size = 1, stride = 1, padding = 0, dilation = 1)\n",
    "\n",
    "        # graph conv\n",
    "        self.gnn1 = GNNBlock(channels, channels, activation = activation)\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor, return_adj = False,):\n",
    "\n",
    "        # metadata\n",
    "        b, c, h, w = input_tensor.shape\n",
    "        \n",
    "        # 1. compute features for edge prediction\n",
    "        edge_features = self.edge_conv(input_tensor)\n",
    "\n",
    "        # 2. compute adjacency matrix by dot product \n",
    "        scores = edge_features.view(b, c, h*w) # reshape and normalize\n",
    "        scores = nn.functional.normalize(scores, p = 2, dim = 2) # normalize vector at each node\n",
    "        scores = torch.bmm(scores.permute(0, 2, 1), scores) # (b x hw x c) x (b x c x hw) -> (b x hw x hw)\n",
    "        adj_tensor = torch.sigmoid(scores)\n",
    "\n",
    "        # 3. compute dampening factor by affine transformations of mean of edge features\n",
    "        scale = self.scaleconv(edge_features)\n",
    "        offset = self.offsetconv(edge_features)\n",
    "        mean = scores.mean(dim = 1, keepdim = True) # (b x 1 x hw) mean of similarity scores as base\n",
    "        adjustment = torch.relu(scale.view(b,1,h*w) * mean + offset.view(b,1,h*w)) # relu to keep everything positive\n",
    "\n",
    "        # dampen\n",
    "        adj_tensor = adj_tensor - adjustment\n",
    "\n",
    "        # 4. graph conv\n",
    "        x = input_tensor.view(b, c, h*w).permute(0, 2, 1)  # -> b x hw x c\n",
    "        x = self.gnn1(x, adj_tensor) + x # -> b x hw x c\n",
    "\n",
    "        # 5. reshape back to image\n",
    "        x = x.permute(0, 2, 1).view(b, c, h, w)\n",
    "\n",
    "        if return_adj:\n",
    "            return x, adj_tensor\n",
    "            \n",
    "        return x\n",
    "\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # same -> downsample \n",
    "        self.conv0 = GatedConv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv1 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
    "\n",
    "        # same -> downsample\n",
    "        self.conv2 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv3 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
    "\n",
    "        # 2 x same conv\n",
    "        self.conv4 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv5 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "\n",
    "        # graph conv\n",
    "        self.graphconv1 = GatedGraphConvModule(hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "\n",
    "        # 4 x dilated conv\n",
    "        self.conv6 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 2, dilation = 2, activation = activation)\n",
    "        self.conv7 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 4, dilation = 4, activation = activation)\n",
    "        self.conv8 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 8, dilation = 8, activation = activation)\n",
    "        self.conv9 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 16, dilation = 16, activation = activation)\n",
    "\n",
    "        # 2 x same conv\n",
    "        self.conv10 = GatedConv2dBlock(2*hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv11 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "\n",
    "        # upsample -> same -> upsample -> same\n",
    "        self.conv12 = GatedUpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
    "        self.conv13 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv14 = GatedUpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
    "        self.conv15 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "\n",
    "        # final\n",
    "        self.final = nn.Conv2d(hidden_dim, output_dim, kernel_size = 3, stride = 1, padding = 'same')\n",
    "\n",
    "        # for contrastive learning, 1x1 conv to compress feature map into 1 channel\n",
    "        self.conv_feature = nn.Conv2d(hidden_dim, 1, kernel_size = 1, stride = 1, padding = 'same')\n",
    "\n",
    "    def forward(self, input_tensor, return_adj = False):\n",
    "        \n",
    "        # downsample\n",
    "        x = self.conv0(input_tensor)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # downsample\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # middle preprocessing layers\n",
    "        x = self.conv4(x) + x\n",
    "        x = self.conv5(x) + x\n",
    "\n",
    "        # graph conv\n",
    "        g, adj = self.graphconv1(x, return_adj = True)\n",
    "        g = g + x\n",
    "\n",
    "        # dilated conv with residual skips\n",
    "        d = self.conv6(x) + x\n",
    "        d = self.conv7(d) + d\n",
    "        d = self.conv8(d) + d\n",
    "        d = self.conv9(d) + d\n",
    "\n",
    "        # middle postprocessing layers\n",
    "        x = self.conv10(torch.cat([d,g], dim = 1)) + x\n",
    "        x = self.conv11(x) + x\n",
    "\n",
    "        # upsample\n",
    "        x = self.conv12(x)\n",
    "        x = self.conv13(x)\n",
    "        x = self.conv14(x)\n",
    "        x = self.conv15(x)\n",
    "        \n",
    "        # final\n",
    "        x = self.final(x)\n",
    "\n",
    "        if return_adj:\n",
    "            return x, adj\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH = \"/home/e/e0425222/CS4243-project/active_experiments/final/GraphGAN/AdaptiveThreshold/generator/generator_epoch20.pt\"\n",
    "FILENAME = \"GraphGAN.png\"\n",
    "model = Generator(input_dim = 4, hidden_dim = 64, output_dim = 3, activation = nn.Mish)\n",
    "model.load_state_dict(torch.load(SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "sample_images(model, DEVICE, test_dataset, filename= FILENAME, sample_size = SAMPLE_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9: Peak SnR (Whole): 33.10754606458876   L2 loss (Whole): 0.0005091961001097742   L2 loss (Mask): 0.043716352639926806   L1 loss (Whole): 0.003007827973407176   L1 loss (Mask): 0.25875557793511283   LPIPS (Whole): 0.022742397876249418    "
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Peak SnR (Whole)': 33.10754606458876,\n",
       " 'L2 loss (Whole)': 0.0005091961001097742,\n",
       " 'L2 loss (Mask)': 0.043716352639926806,\n",
       " 'L1 loss (Whole)': 0.003007827973407176,\n",
       " 'L1 loss (Mask)': 0.25875557793511283,\n",
       " 'LPIPS (Whole)': 0.022742397876249418}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(model, DEVICE, METRICS, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DilatedGatedGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # same -> down -> same -> down\n",
    "        self.conv0 = GatedConv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv1 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv2 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv3 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
    "\n",
    "        # 8 x same : 2 x normal -> 4 x dilated -> 2 x normal\n",
    "        self.conv4 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv5 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv6 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 2, dilation = 2, activation = activation)\n",
    "        self.conv7 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 4, dilation = 4, activation = activation)\n",
    "        self.conv8 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 8, dilation = 8, activation = activation)\n",
    "        self.conv9 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 16, dilation = 16, activation = activation)\n",
    "        self.conv10 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv11 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "\n",
    "        # upsample -> same -> upsample -> same\n",
    "        self.conv12 = GatedUpConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation, scale_factor = (2,2))\n",
    "        self.conv13 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv14 = GatedUpConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation, scale_factor = (2,2))\n",
    "        self.conv15 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "\n",
    "        # final\n",
    "        self.final = nn.Conv2d(hidden_dim, output_dim, kernel_size = 3, stride = 1, padding = 'same')\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "\n",
    "        x = self.conv0(input_tensor)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # middle layers\n",
    "        x = self.conv4(x) + x\n",
    "        x = self.conv5(x) + x\n",
    "        x = self.conv6(x) + x\n",
    "        x = self.conv7(x) + x\n",
    "        x = self.conv8(x) + x\n",
    "        x = self.conv9(x) + x\n",
    "        x = self.conv10(x) + x\n",
    "        x = self.conv11(x) + x\n",
    "\n",
    "        # up sample\n",
    "        x = self.conv12(x)\n",
    "        x = self.conv13(x)\n",
    "        x = self.conv14(x)\n",
    "        x = self.conv15(x)\n",
    "        \n",
    "        # final\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_PATH = \"/home/e/e0425222/CS4243-project/active_experiments/final/DilatedGated/GAN_1/generator/generator_epoch20.pt\"\n",
    "FILENAME = \"DilatedGatedGAN.png\"\n",
    "\n",
    "model = Generator(input_dim = 4, hidden_dim = 64, output_dim = 3, activation = nn.Mish)\n",
    "model.load_state_dict(torch.load(SAVE_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9: Peak SnR (Whole): 32.67547946506076   L2 loss (Whole): 0.0005427065609385156   L2 loss (Mask): 0.046924981806013316   L1 loss (Whole): 0.0030987938452098104   L1 loss (Mask): 0.2680983328157001   LPIPS (Whole): 0.023552156777845487    "
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Peak SnR (Whole)': 32.67547946506076,\n",
       " 'L2 loss (Whole)': 0.0005427065609385156,\n",
       " 'L2 loss (Mask)': 0.046924981806013316,\n",
       " 'L1 loss (Whole)': 0.0030987938452098104,\n",
       " 'L1 loss (Mask)': 0.2680983328157001,\n",
       " 'LPIPS (Whole)': 0.023552156777845487}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_images(model, DEVICE, test_dataset, filename= FILENAME, sample_size = SAMPLE_SIZE) \n",
    "compute_metrics(model, DEVICE, METRICS, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9319ae1760e4d45d7d7c835d46b6f1010677823a105cd126e5ae41ed73f571cc"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
