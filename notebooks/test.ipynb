{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv9Nytklkhyd"
   },
   "source": [
    "# Introduction \n",
    "\n",
    "This notebook documents one of the two main classes of models for image inpainting and generation, a basic deep convolutional GAN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIEkShUFq9FA"
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ek1yIysk0dFf"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tg6kdj7imCuM",
    "outputId": "a21d6c6e-1e58-4bbe-e0fe-4b58c5fb3cc6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# functional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import lpips\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# wandb\n",
    "import wandb\n",
    "\n",
    "# images\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "\n",
    "# metrics\n",
    "import torchmetrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDxlOnFF0h9r"
   },
   "source": [
    "## Import custom methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yk-htrdf570l",
    "outputId": "747f4cef-38bb-460c-85ca-af102652c3be"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/e/e0425222/CS4243/\")\n",
    "from utils.dataset_utils.AnimalDataset import AnimalDataset\n",
    "from utils.train_utils.train_utils import visualize_results, sample_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIHoVHlLkhGG"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAEU9gtRrStE"
   },
   "source": [
    "## Define model\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Jq6RUUsrSC2"
   },
   "outputs": [],
   "source": [
    "class GatedConv2d(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    This class implements a gated convolution. It works by applying a convolution filter \n",
    "    to the input feature tensor, then using the sigmoid function to map each score to a \n",
    "    pixel validity weight. The weights are then multiplied to the activation map from a \n",
    "    separate convolution filter.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation):\n",
    "        super(GatedConv2d, self).__init__()\n",
    "        self.image_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation = dilation)\n",
    "        self.gate_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation = dilation)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor, return_mask = False):\n",
    "\n",
    "        mask = self.sigmoid(self.gate_conv(input_tensor))\n",
    "        x = self.image_conv(input_tensor)\n",
    "        x = torch.mul(x, mask) # apply mask\n",
    "\n",
    "        if return_mask:\n",
    "            return x, mask\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Conv2dBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, \n",
    "        kernel_size = 3, stride = 1, padding = 'same', dilation = 1, \n",
    "        activation = nn.ReLU, partial = True):\n",
    "\n",
    "        super(Conv2dBlock, self).__init__()\n",
    "\n",
    "        if partial:\n",
    "            self.conv = GatedConv2d(input_dim, output_dim, kernel_size, stride, padding, dilation)\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride, padding, dilation)\n",
    "        self.activation = activation()\n",
    "        self.bn = nn.BatchNorm2d(output_dim)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \n",
    "        x = input_tensor\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class UpConv2dBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, \n",
    "        kernel_size = 3, stride = 1, padding = 'same', dilation = 1, \n",
    "        activation = nn.ReLU, \n",
    "        scale_factor = (2,2)):\n",
    "\n",
    "        super(UpConv2dBlock, self).__init__()\n",
    "\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor = scale_factor, mode = 'bicubic')\n",
    "        self.conv = GatedConv2d(input_dim, output_dim, kernel_size, stride, padding, dilation)\n",
    "        self.activation = activation()\n",
    "        self.bn = nn.BatchNorm2d(output_dim)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \n",
    "        x = input_tensor\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, activation = nn.ReLU):\n",
    "        super(LinearBlock, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = activation()\n",
    "        self.bn = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \n",
    "        x = input_tensor\n",
    "        x = self.linear(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # same -> downsample -> same -> downsample\n",
    "        self.conv0 = Conv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv1 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv2 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv3 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
    "\n",
    "        # 2 x same conv\n",
    "        self.conv4 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv5 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "\n",
    "        # 4 x dilated conv\n",
    "        self.conv6 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 2, dilation = 2, activation = activation)\n",
    "        self.conv7 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 4, dilation = 4, activation = activation)\n",
    "        self.conv8 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 8, dilation = 8, activation = activation)\n",
    "        self.conv9 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 16, dilation = 16, activation = activation)\n",
    "\n",
    "        # 2 x same conv\n",
    "        self.conv10 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv11 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "\n",
    "        # upsample -> same -> upsample -> same\n",
    "        self.conv12 = UpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1)\n",
    "        self.conv13 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "        self.conv14 = UpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1)\n",
    "        self.conv15 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
    "\n",
    "        # final\n",
    "        self.final = nn.Conv2d(hidden_dim, output_dim, kernel_size = 3, stride = 1, padding = 'same')\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "\n",
    "        x = self.conv0(input_tensor)\n",
    "        \n",
    "        # downsample\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # middle layers\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        # dilated conv with residual skips\n",
    "        x = self.conv6(x) + x\n",
    "        x = self.conv7(x) + x\n",
    "        x = self.conv8(x) + x\n",
    "        x = self.conv9(x) + x\n",
    "\n",
    "        # middle layers\n",
    "        x = self.conv10(x)\n",
    "        x = self.conv11(x)\n",
    "\n",
    "        # upsample\n",
    "        x = self.conv12(x)\n",
    "        x = self.conv13(x)\n",
    "        x = self.conv14(x)\n",
    "        x = self.conv15(x)\n",
    "        \n",
    "        # final\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, input_shape, activation):\n",
    "\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # 5 layers down\n",
    "        self.conv0 = Conv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation, partial = False)\n",
    "        self.conv1 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation, partial = False)\n",
    "        self.conv2 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation, partial = False)\n",
    "        self.conv3 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation, partial = False)\n",
    "        self.conv4 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation, partial = False)\n",
    "\n",
    "        # linear to predict classes\n",
    "        LATENT_H = input_shape//(2**5)\n",
    "        self.linear = nn.Linear(LATENT_H**2 * hidden_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \n",
    "        x = self.conv0(input_tensor)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        # scores\n",
    "        x = torch.flatten(x, start_dim = 1)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def summary(model: nn.Module, verbose = False):\n",
    "    count = 0\n",
    "    if verbose:\n",
    "        print(model)\n",
    "\n",
    "    for name, params in model.named_parameters():\n",
    "        num_params = params.flatten().size()[0]\n",
    "        count += num_params\n",
    "        if verbose:\n",
    "            print(f\"\\nlayer: {name}\")\n",
    "            print(f\"number of params: {num_params}\")\n",
    "            print(f\"params shape: {params.size()}\")\n",
    "\n",
    "    print(f\"model has {count/1e6} million parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pN1aWwAirTol"
   },
   "source": [
    "## Initialize model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9HDdG8KrXT2",
    "outputId": "975b9e54-658e-4d2b-f39c-af88c8d436fb"
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "image_dim = 96\n",
    "in_channels = 4\n",
    "out_channels = 3\n",
    "hidden_channels = 64\n",
    "activation = nn.PReLU\n",
    "\n",
    "# create model\n",
    "generator = Generator(in_channels, hidden_channels, out_channels, activation)\n",
    "discriminator = Discriminator(out_channels, hidden_channels, image_dim, activation)\n",
    "\n",
    "summary(generator)\n",
    "summary(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YT5zY9vNrYG8"
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = \"/home/e/e0425222/CS4243/model_weights/GatedConv_DilatedConv_GAN\"\n",
    "\n",
    "# save path\n",
    "MODEL_NAMES = {\"generator\": \"GatedConv_DilatedConv_GANGenerator_hidden64_prelu\", \n",
    "                \"discriminator\": \"GatedConv_DilatedConv_GAN_Discriminator_hidden64_prelu\"}\n",
    "SAVE_PATHS = {f\"{key}\" : f\"{MODEL_FOLDER}/{val}\" for key, val in MODEL_NAMES.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrDuLW2urZlN"
   },
   "source": [
    "# Training Details\n",
    "For training, we define all details here, and wrap it into two main dictionaries, `training_params` and `metrics`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_segmented_loss(output, target, mask, loss_function = nn.functional.mse_loss):\n",
    "    \"\"\"\n",
    "    This is a custom functional loss function that considers where the mask is, and\n",
    "    applies the loss function seperately. \n",
    "    \"\"\"\n",
    "    \n",
    "    LOSS = loss_function # can use l1 loss or coarse to fine refinement\n",
    "\n",
    "\n",
    "    def weighted_sum():\n",
    "        MASKED_WEIGHT = 1\n",
    "        UNMASKED_WEIGHT = 1\n",
    "\n",
    "        # weights by mask, unmasked as 1\n",
    "        masked_weights = (mask - 1) * -1 * MASKED_WEIGHT # invert the mask in a differentiable way\n",
    "        unmasked_weights = mask * UNMASKED_WEIGHT\n",
    "        \n",
    "        # weighted sum\n",
    "        loss_masked = LOSS(output, target, reduction = 'none')\n",
    "        loss_unmasked = LOSS(output, target, reduction = 'none')\n",
    "        loss = torch.mul(loss_masked, masked_weights) + torch.mul(loss_unmasked, unmasked_weights)\n",
    "        loss = torch.mean(loss)\n",
    "        return loss\n",
    "\n",
    "    def only_masked():\n",
    "        loss = LOSS(output, target, reduction = 'none')\n",
    "        loss = torch.mul(loss, (-mask + 1))\n",
    "        loss = torch.sum(loss)/torch.sum(-mask + 1)\n",
    "        return loss\n",
    "\n",
    "    def masked_with_unmaskedgt():\n",
    "        img = (-mask + 1) * output + mask * target\n",
    "        loss = LOSS(img, target, reduction = 'mean')\n",
    "        return loss\n",
    "\n",
    "    # only masked\n",
    "    loss = only_masked()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VElJr7yvrnvu"
   },
   "source": [
    "## Training parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "parallel = False\n",
    "device = 'cuda:2'  \n",
    "devices = [2, 3]\n",
    "\n",
    "if not parallel:\n",
    "    generator = generator.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "else:\n",
    "    generator = nn.DataParallel(generator, device_ids = devices)\n",
    "    discriminator = nn.DataParallel(discriminator, device_ids = devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndGZyFYJrnRD"
   },
   "outputs": [],
   "source": [
    "# define training parameters\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer parameters\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# balance loss\n",
    "alpha = 0.05\n",
    "\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr = learning_rate)\n",
    "generator_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(generator_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
    "\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr = learning_rate)\n",
    "discriminator_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(discriminator_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
    "\n",
    "# loss function \n",
    "generator_loss_function = lambda img, gt, mask : mask_segmented_loss(img, gt, mask, nn.functional.l1_loss)\n",
    "discriminator_loss_function = nn.functional.binary_cross_entropy_with_logits\n",
    "\n",
    "\n",
    "# wrap into dictionary\n",
    "training_params = {\n",
    "    \"generator_model\" : generator,\n",
    "    \"discriminator_model\" : discriminator,\n",
    "    \"num_epochs\" : num_epochs,\n",
    "    \"batch_size\" : batch_size, \n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"generator_optimizer\" : generator_optimizer,\n",
    "    \"generator_scheduler\" : generator_scheduler,\n",
    "    \"generator_loss_function\" : generator_loss_function,\n",
    "    \"discriminator_optimizer\" : discriminator_optimizer,\n",
    "    \"discriminator_scheduler\" : discriminator_scheduler,\n",
    "    \"discriminator_loss_function\" : discriminator_loss_function,\n",
    "    \"alpha\": alpha,\n",
    "    \"schedule_every\" : 1,\n",
    "    \"save_path\" : SAVE_PATHS,\n",
    "    \"sample_size\" : 16,\n",
    "    \"log_every\" : 20\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtnlBdXjrqqj"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVM4T_Csrml2"
   },
   "outputs": [],
   "source": [
    "# instantiate metric objects if needed\n",
    "VGG_LPIPS = lpips.LPIPS(net = 'vgg').to(device)\n",
    "\n",
    "# define metrics\n",
    "metrics = {\n",
    "    \"Peak SnR (Whole)\" : lambda img, gt, mask : torchmetrics.functional.peak_signal_noise_ratio(img * (-mask + 1) + gt * mask, gt),\n",
    "    \"L2 loss (Whole)\" : lambda img, gt, mask : nn.functional.mse_loss(img * (-mask + 1) + gt * mask, gt),\n",
    "    \"L2 loss (Mask)\" : lambda img, gt, mask : mask_segmented_loss(img, gt, mask, nn.functional.mse_loss),\n",
    "    \"L1 loss (Whole)\" : lambda img, gt, mask : nn.functional.mse_loss(img * (-mask + 1) + gt * mask, gt),\n",
    "    \"L1 loss (Mask)\" : lambda img, gt, mask : mask_segmented_loss(img, gt, mask, nn.functional.l1_loss),\n",
    "    \"LPIPS (Whole)\" : (lambda img, gt, mask : VGG_LPIPS(img * (-mask + 1) + gt * mask, gt).mean())\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEPLOIhtrr0S"
   },
   "source": [
    "## Load Frogs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFzmtDtortA_",
    "outputId": "732402b5-4d8b-4e9b-f0d2-eb23076899aa"
   },
   "outputs": [],
   "source": [
    "concat_mask = True\n",
    "dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243/dataset/frogs/frogs_wanted_indices_url.txt\",\n",
    "                        root_dir_path = \"/home/e/e0425222/CS4243/dataset/frogs/images\",\n",
    "                        local_dir_path = \"/home/e/e0425222/CS4243/dataset/frogs/preprocessed_96\",\n",
    "                        file_prefix = \"frog_\",\n",
    "                        image_dimension = image_dim,\n",
    "                        concat_mask = concat_mask,\n",
    "                        require_init = False,\n",
    "                        drops = [3839])\n",
    "\n",
    "# train-test split\n",
    "VALID_SIZE = 0.2\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "indices = torch.arange(len(dataset))\n",
    "train_indices, validation_indices = train_test_split(indices, test_size = VALID_SIZE)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "validation_dataset = Subset(dataset, validation_indices)\n",
    "\n",
    "sample_batch(dataset, sample_size = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training config file\n",
    "config = {\n",
    "    \"model_names\" : MODEL_NAMES,\n",
    "    \"model_descriptions\" : {\n",
    "        \"generator\": \"Follows GLCIC model. Gated convolutions used instead of convolutions.\",\n",
    "        \"discriminator\": \"Follows GLCIC model, but with kernel size of 3, not 5.\"\n",
    "    },\n",
    "    \"dataset_details\" : {\n",
    "        \"img_dim\" : image_dim,\n",
    "        \"indices_file\" : \"frogs_wanted_indices_url.txt\",\n",
    "        \"random_crop\" : True,\n",
    "        \"concat_mask\" : True\n",
    "    },\n",
    "    \"train_details\" : {\n",
    "        \"num_epochs\" : num_epochs,\n",
    "        \"batch_size\" : batch_size, \n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"optimizer\" : \"ADAM\",\n",
    "        \"scheduler\" : {\n",
    "            \"name\" : \"ReduceLROnPlateau\",\n",
    "            \"params\" : ['min', 0.5, 3, 1e-6],\n",
    "        \"schedule_every\" : 1,\n",
    "        \"alpha\" : alpha\n",
    "        },\n",
    "        \"loss_functions\" : {\n",
    "            \"description\" : \"Generator uses l1 masked reconstruction loss. Discrimnator binary cross entropy loss. Weighted by alpha.\",\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# wandb logging\n",
    "wandb.init(project=\"GatedConv_DilatedConv_GAN\", entity=\"cs4243_project\")\n",
    "wandb.config = config\n",
    "\n",
    "wandb.watch(\n",
    "    (generator, discriminator),\n",
    "    criterion = None,\n",
    "    log = 'all',\n",
    "    log_freq = 1,\n",
    "    idx = 0, \n",
    "    log_graph = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cafLR4dBruM7"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def train_epoch(device, train_dataloader, training_params : dict, metrics : dict, log_wandb = True):\n",
    "    \n",
    "    # ===== INITIALIZE =====\n",
    "    # constants\n",
    "    GENERATOR_LOSS_FUNCTION = training_params[\"generator_loss_function\"]\n",
    "    GENERATOR_OPTIMIZER = training_params[\"generator_optimizer\"]\n",
    "    DISCRIMINATOR_LOSS_FUNCTION = training_params[\"discriminator_loss_function\"]\n",
    "    DISCRIMINATOR_OPTIMIZER = training_params[\"discriminator_optimizer\"]\n",
    "    BATCH_EVALUATE_EVERY = 1\n",
    "    LOG_EVERY = training_params[\"log_every\"]\n",
    "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
    "    ALPHA = training_params[\"alpha\"]\n",
    "\n",
    "    # models\n",
    "    generator = training_params[\"generator_model\"].to(device).train()\n",
    "    discriminator = training_params[\"discriminator_model\"].to(device).train()\n",
    "\n",
    "    # epoch metrics\n",
    "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
    "    running_results[\"loss\"] = 0.0\n",
    "    running_results[\"loss_discriminator\"] = 0.0\n",
    "    running_results[\"loss_generator\"] = 0.0\n",
    "    running_results[\"loss_generator_r\"] = 0.0\n",
    "    running_results[\"loss_generator_adv\"] = 0.0\n",
    "\n",
    "    # ===== TRAIN EPOCH =====\n",
    "    num_batches = 0\n",
    "    for index, batch in enumerate(train_dataloader, 1):\n",
    "\n",
    "            # ===== INITIALIZE =====\n",
    "            num_batches += 1\n",
    "\n",
    "            # input and ground truth\n",
    "            input_batched = batch[\"image\"]\n",
    "            ground_truth_batched = batch[\"reconstructed\"]\n",
    "            mask_batched = batch[\"mask\"]\n",
    "\n",
    "            # sanity check\n",
    "            assert input_batched.shape[0] == ground_truth_batched.shape[0]\n",
    "\n",
    "            # move tensors to device\n",
    "            input_batched = input_batched.to(device)\n",
    "            ground_truth_batched = ground_truth_batched.to(device)\n",
    "            mask_batched = mask_batched.to(device)\n",
    "\n",
    "            # Set the gradients to zeros\n",
    "            GENERATOR_OPTIMIZER.zero_grad()\n",
    "            DISCRIMINATOR_OPTIMIZER.zero_grad()\n",
    "\n",
    "            # ===== FORWARD PASS =====\n",
    "            # 1. reshape to channel first\n",
    "            input_batched = input_batched.permute(0, 3, 1, 2)\n",
    "            ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
    "            mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
    "\n",
    "            # 2. forward pass by generator to produce images\n",
    "            input_batched.requires_grad_()\n",
    "            output_batched = generator(input_batched)\n",
    "\n",
    "            # 3. splice generated images to that patch is only change\n",
    "            spliced_batched = ((1-mask_batched) * output_batched) + (mask_batched * ground_truth_batched) \n",
    "            \n",
    "            # 4. generate labels for discriminator\n",
    "            b, _, _, _ = input_batched.shape\n",
    "            label_real = torch.ones(b, 1).to(device)\n",
    "            label_fake = torch.zeros(b, 1).to(device)\n",
    "\n",
    "            # 5. forward and backward pass on discriminator\n",
    "            pred_real = discriminator(ground_truth_batched)\n",
    "            loss_real = DISCRIMINATOR_LOSS_FUNCTION(pred_real, label_real)\n",
    "            pred_fake = discriminator(spliced_batched)\n",
    "            loss_fake = DISCRIMINATOR_LOSS_FUNCTION(pred_fake, label_fake)\n",
    "            loss_d = ALPHA * (loss_real + loss_fake)\n",
    "            loss_d.backward()\n",
    "            DISCRIMINATOR_OPTIMIZER.step()\n",
    "\n",
    "            # 6. adverserial and reconstruction loss on generator, repeated for computation graph\n",
    "            # 6.1 forward pass by generator to produce images, splice them\n",
    "            output_batched = generator(input_batched)\n",
    "            spliced_batched = ((1-mask_batched) * output_batched) + (mask_batched * ground_truth_batched) \n",
    "            \n",
    "            # 6.2 reconstruction and adverserial loss\n",
    "            loss_g_reconstruction = GENERATOR_LOSS_FUNCTION(output_batched, ground_truth_batched, mask_batched)\n",
    "            pred_adverserial = discriminator(spliced_batched)\n",
    "            loss_g_adverserial = DISCRIMINATOR_LOSS_FUNCTION(pred_adverserial, label_real) # want it to classify all as real\n",
    "            loss_g = loss_g_reconstruction + ALPHA * loss_g_adverserial\n",
    "            loss_g.backward()\n",
    "            GENERATOR_OPTIMIZER.step()\n",
    "\n",
    "            # ===== COMPUTE STATISTICS, USING TORCH METRICS =====  \n",
    "            # 1. compute losses\n",
    "            loss = loss_g + loss_d * ALPHA\n",
    "            running_results[\"loss\"] += loss.detach().item()\n",
    "            running_results[\"loss_generator\"] += loss_g.detach().item()\n",
    "            running_results[\"loss_generator_r\"] += loss_g_reconstruction.detach().item()\n",
    "            running_results[\"loss_generator_adv\"] += loss_g_adverserial.detach().item()\n",
    "\n",
    "            running_results[\"loss_discriminator\"] += loss_d.detach().item()\n",
    "\n",
    "            # 2. for each key, compute, add item to results dictionary\n",
    "            for key, func in metrics.items():\n",
    "                running_results[key] += func(output_batched, ground_truth_batched, mask_batched).detach().item()\n",
    "\n",
    "            # 3. log with wandb\n",
    "            if log_wandb and (num_batches % LOG_EVERY == 0):\n",
    "                batched_predictions = torch.cat([\n",
    "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
    "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
    "                    spliced_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
    "                \n",
    "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
    "                images = wandb.Image(\n",
    "                    image_array, \n",
    "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: Reconstructed, 4th row: spliced\")\n",
    "\n",
    "                # log images and some metadata\n",
    "                wandb.log( {\n",
    "                    \"images\" : images,\n",
    "                    \"lr_generator\" : GENERATOR_OPTIMIZER.param_groups[0]['lr'],\n",
    "                    \"lr_discriminator\" : DISCRIMINATOR_OPTIMIZER.param_groups[0]['lr']\n",
    "\n",
    "                })\n",
    "\n",
    "                # log all metrics\n",
    "                wandb.log(\n",
    "                    {key : item/num_batches for key, item in running_results.items()}\n",
    "                )\n",
    "        \n",
    "\n",
    "            # ===== HOUSEKEEPING =====\n",
    "            del loss\n",
    "            del loss_g\n",
    "            del loss_d\n",
    "            del input_batched\n",
    "            del output_batched\n",
    "            del spliced_batched\n",
    "\n",
    "            # print results every some batches\n",
    "            if num_batches % BATCH_EVALUATE_EVERY == 0: \n",
    "\n",
    "                args = \"\"\n",
    "                for key, val in running_results.items():\n",
    "                    args += key + \": \" + str(running_results[key]/num_batches) + \"   \"\n",
    "                print(f\"\\r{num_batches}/{len(train_dataloader)}: \" + args, end = '', flush = True)\n",
    "\n",
    "    # normalise numbers by batch\n",
    "    for key, val in running_results.items():\n",
    "        running_results[key] /= num_batches\n",
    "\n",
    "    return running_results\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_epoch(model, device, validation_dataloader, training_params : dict, metrics : dict):\n",
    "\n",
    "    # ===== INITIALIZE =====\n",
    "    # constants\n",
    "    LOSS_FUNCTION = training_params[\"generator_loss_function\"]\n",
    "\n",
    "    # to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # epoch statistics\n",
    "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
    "    running_results[\"loss_generator\"] = 0.0\n",
    "\n",
    "    # ===== EVALUATE EPOCH =====\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        batches = 0\n",
    "        for index, batch in enumerate(validation_dataloader, 1):\n",
    "            \n",
    "            batches += 1\n",
    "\n",
    "            # input and ground truth\n",
    "            input_batched = batch[\"image\"]\n",
    "            ground_truth_batched = batch[\"reconstructed\"]\n",
    "            mask_batched = batch[\"mask\"]\n",
    "\n",
    "            # move tensors to device\n",
    "            input_batched = input_batched.to(device)\n",
    "            ground_truth_batched = ground_truth_batched.to(device)\n",
    "            mask_batched = mask_batched.to(device)\n",
    "\n",
    "            # ===== FORWARD PASS =====\n",
    "\n",
    "            # 1. reshape to channel first\n",
    "            input_batched = input_batched.permute(0, 3, 1, 2)\n",
    "            ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
    "            mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
    "\n",
    "            # 2. predict    \n",
    "            output_batched = model(input_batched)\n",
    "\n",
    "            # 3. evaluate\n",
    "            loss = LOSS_FUNCTION(output_batched, ground_truth_batched, mask_batched).detach().item()\n",
    "            running_results[\"loss_generator\"] += loss\n",
    "            \n",
    "            # ===== COMPUTE STATISTICS, USING TORCH METRICS =====\n",
    "            for key, func in metrics.items():\n",
    "                running_results[key] += func(output_batched, ground_truth_batched, mask_batched).detach().item()\n",
    "\n",
    "            args = \"\"\n",
    "            for key, val in running_results.items():\n",
    "                args += key + \": \" + str(running_results[key]/batches) + \"   \"\n",
    "            print(f\"\\r{batches}/{len(validation_dataloader)}: \" + args, end = '', flush = True)\n",
    "\n",
    "            # delete to ensure memory footprint\n",
    "            del loss\n",
    "            del input_batched\n",
    "            del output_batched\n",
    "\n",
    "    # normalise numbers by batch\n",
    "    for key, val in running_results.items():\n",
    "        running_results[key] /= batches\n",
    "\n",
    "    return running_results\n",
    "\n",
    "def train_evaluate(device, train_dataset, validation_dataset, training_params: dict, metrics: dict, start_epoch = 0, log_wandb = True):\n",
    "\n",
    "    # ===== INITIALIZE =====\n",
    "    # constants\n",
    "    NUM_EPOCHS = training_params[\"num_epochs\"]\n",
    "    BATCH_SIZE = training_params[\"batch_size\"]\n",
    "    GENERATOR_SCHEDULER = training_params[\"generator_scheduler\"]\n",
    "    DISCRIMINATOR_SCHEDULER = training_params[\"discriminator_scheduler\"]\n",
    "    SAVE_PATHS = training_params[\"save_path\"]\n",
    "    NUM_WORKERS = 2\n",
    "    START_EPOCH = start_epoch\n",
    "\n",
    "    # models for saving\n",
    "    generator = training_params[\"generator_model\"].to(device).train()\n",
    "    discriminator = training_params[\"discriminator_model\"].to(device).train()\n",
    "\n",
    "    # variables losses\n",
    "    train_results = {list(metrics.keys())[i] : [] for i in range(len(metrics)) } \n",
    "    train_results[\"loss\"] = []\n",
    "    train_results[\"loss_generator\"] = []\n",
    "    train_results[\"loss_generator_r\"] = []\n",
    "    train_results[\"loss_generator_adv\"] = []\n",
    "    train_results[\"loss_discriminator\"] = []\n",
    "\n",
    "    eval_results = {list(metrics.keys())[i] : [] for i in range(len(metrics)) } \n",
    "    eval_results[\"loss\"] = []\n",
    "    eval_results[\"loss_generator\"] = []\n",
    "    eval_results[\"loss_generator\"] = []\n",
    "    eval_results[\"loss_generator_r\"] = []\n",
    "\n",
    "\n",
    "    # dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS)\n",
    "\n",
    "    # ===== TRAIN =====\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # train\n",
    "        print(f\"\\n===== Epoch: {START_EPOCH + epoch + 1} ===== \")\n",
    "        num_batches = 0\n",
    "\n",
    "        # train every epoch\n",
    "        results = train_epoch(device, train_dataloader, training_params, metrics, log_wandb = log_wandb)\n",
    "        for key, val in results.items():\n",
    "            train_results[key].append(val)\n",
    "\n",
    "        # evaluate every epoch\n",
    "        print()\n",
    "        results = evaluate_epoch(generator, device, validation_dataloader, training_params, metrics)\n",
    "        for key, val in results.items():\n",
    "            eval_results[key].append(val)\n",
    "\n",
    "        # ===== EPOCH RESULTS =====\n",
    "        print(f\"\\nCompleted epoch {START_EPOCH + epoch + 1}! Took {(time.time() - start)/60} min\")\n",
    "\n",
    "        # ===== HOUSEKEEPING =====\n",
    "\n",
    "        # scheduler every epoch\n",
    "        if DISCRIMINATOR_SCHEDULER is not None:\n",
    "            DISCRIMINATOR_SCHEDULER.step(eval_results[\"loss_generator\"][epoch])\n",
    "        if GENERATOR_SCHEDULER is not None:\n",
    "            GENERATOR_SCHEDULER.step(eval_results[\"loss_generator\"][epoch])\n",
    "            \n",
    "        # save every epoch\n",
    "        SAVE = f\"{SAVE_PATHS['generator']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
    "        torch.save(generator.state_dict(), SAVE)\n",
    "        SAVE = f\"{SAVE_PATHS['discriminator']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
    "        torch.save(discriminator.state_dict(), SAVE)  \n",
    "\n",
    "        print(\"Saved models!\")\n",
    "\n",
    "    return train_results, eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VGWU06i_Gznc",
    "outputId": "a243ec87-4663-4503-c93a-2c42d8041802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188: Peak SnR (Whole): 31.410151796138035   L2 loss (Whole): 0.000729074139715212   L2 loss (Mask): 0.06280993860452733   L1 loss (Whole): 0.000729074139715212   L1 loss (Mask): 0.32471473832079706   LPIPS (Whole): 0.04294200760728501   loss: 0.3631626597427307   loss_discriminator: 0.06920389097897296   loss_generator: 0.35970246585759713   loss_generator_r: 0.32471473832079706   loss_generator_adv: 0.6997545526382771       \n",
      "47/47: Peak SnR (Whole): 31.66871208840228   L2 loss (Whole): 0.0006847119100015373   L2 loss (Mask): 0.05881180875795953   L1 loss (Whole): 0.0006847119100015373   L1 loss (Mask): 0.3063572093527368   LPIPS (Whole): 0.0403004822895882   loss_generator: 0.3063572093527368        \n",
      "Completed epoch 2! Took 3.6925358096758525 min\n",
      "Saved models!\n",
      "\n",
      "===== Epoch: 3 ===== \n",
      "28/188: Peak SnR (Whole): 31.496934277670725   L2 loss (Whole): 0.0007146510698865833   L2 loss (Mask): 0.06222171722246068   L1 loss (Whole): 0.0007146510698865833   L1 loss (Mask): 0.31992535080228535   LPIPS (Whole): 0.040629906579852104   loss: 0.3584234895450728   loss_discriminator: 0.06895215596471514   loss_generator: 0.35497588344982695   loss_generator_r: 0.31992535080228535   loss_generator_adv: 0.7010106316634587   "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# train\n",
    "running_train_results, running_eval_results = train_evaluate(device, train_dataset, validation_dataset, training_params, metrics, log_wandb = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_train_results, running_eval_results = train_evaluate(device, train_dataset, validation_dataset, training_params, metrics, start_epoch = num_epochs, log_wandb = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9PF_yqn578G"
   },
   "source": [
    "# Results and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "060KqhlANEnv"
   },
   "outputs": [],
   "source": [
    "visualize_results(generator, device, running_train_results, running_eval_results, test_dataset = validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 24\n",
    "loader = DataLoader(train_dataset, batch_size = SAMPLE_SIZE, shuffle = True)\n",
    "batch = next(iter(loader))\n",
    "\n",
    "# predict and plot\n",
    "generator.eval()\n",
    "output = generator(batch[\"image\"].to(device).permute(0, 3, 1, 2))\n",
    "generator.train()\n",
    "\n",
    "high = output.max().detach().cpu()\n",
    "low = output.min().detach().cpu()\n",
    "output =(output.detach().cpu().permute(0, 2, 3, 1))\n",
    "mask = batch[\"mask\"].detach().cpu()\n",
    "output = (1-mask) * output + mask * batch[\"reconstructed\"].detach().cpu()\n",
    "fig, ax = plt.subplots(3, SAMPLE_SIZE, figsize = (SAMPLE_SIZE * 5, 15, ))\n",
    "for i in range(SAMPLE_SIZE):\n",
    "    image = batch[\"image\"][i]\n",
    "    reconstructed = batch[\"reconstructed\"][i]\n",
    "    predicted = output[i]\n",
    "\n",
    "    if image.shape[-1] > 3: \n",
    "        image = image[:, :, 0:3] # take rgb if more than 3 channels\n",
    "        \n",
    "    ax[0][i].imshow(image)\n",
    "    ax[1][i].imshow(reconstructed)\n",
    "    ax[2][i].imshow(predicted)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "UNet+partialConv",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "f4f7f947c0c10dbb0a0cbed913b0979009eb3e6ba609f5e9249ce35af21d6b04"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('env')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
