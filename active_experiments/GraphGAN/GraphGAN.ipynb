{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H98D0BHAEioA"
      },
      "source": [
        "# Initialize notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lpips\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchmetrics\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/home/e/e0425222/CS4243-project\")\n",
        "from utils.dataset_utils.AnimalDataset import AnimalDataset\n",
        "from utils.train_utils.train_utils import sample_batch, summary\n",
        "from utils.train_utils.model_utils import Conv2dBlock, GatedConv2dBlock, GatedUpConv2dBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pKzaTBy__baX"
      },
      "outputs": [],
      "source": [
        "train_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_train.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n",
        "\n",
        "valid_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_val.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n",
        "\n",
        "test_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_test.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouUSbxVkE92G"
      },
      "source": [
        "# Introduction\n",
        "This notebook aims to explore how the use of graphs to encode global information can improve capturing global context in image inpainting. The relevant references are:\n",
        "1. Hyperrealistic Image Inpainting with Hypergraphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oL0U4VbFaeJ"
      },
      "source": [
        "# Model experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch_geometric as torch_g\n",
        "import torch_geometric.nn as gnn\n",
        "\n",
        "class HypergraphConvModule(nn.Module):\n",
        "    \"\"\"\n",
        "    This module implements the hypergraph convolution module referenced in: \n",
        "    1. Hypergraph Convolution and Hypergraph attention (https://arxiv.org/abs/1901.08150)\n",
        "    2. Hyperrealistic Image Inpainting with Hypergraphs (http://arxiv.org/abs/2011.02904)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, num_hyperedges, kernel_size, stride, padding, dilation):\n",
        "\n",
        "        super(HypergraphConvModule, self).__init__()\n",
        "\n",
        "        # metdata\n",
        "        self.num_hyperedges = num_hyperedges\n",
        "\n",
        "        # incidence matrix\n",
        "        self.feature_conv = nn.Conv2d(in_channels, num_hyperedges, kernel_size = kernel_size, stride = stride, padding = 'same')\n",
        "        self.channel_conv = nn.Conv2d(in_channels, num_hyperedges, kernel_size = 1, stride = 1, padding = 'same')\n",
        "        self.global_feature_conv = nn.Conv2d(in_channels, num_hyperedges, kernel_size = kernel_size, stride = stride, padding = padding, dilation = dilation)\n",
        "\n",
        "        # graph conv\n",
        "        self.hypergraph_conv = gnn.HypergraphConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "\n",
        "        # metadata\n",
        "        b, c, h, w = input_tensor.shape\n",
        "        device = input_tensor.device\n",
        "        \n",
        "        # hyper incidence matrix prediction\n",
        "        features = self.feature_conv(input_tensor)\n",
        "        \n",
        "        # get channel features with 1x1 conv, reshape to b x c x c diagonal\n",
        "        channel_features = self.channel_conv(input_tensor).view(b * self.num_hyperedges)\n",
        "        channel_features = torch.zeros(size = (self.num_hyperedges, self.num_hyperedges))\n",
        "\n",
        "\n",
        "\n",
        "        # graph conv\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h2hEL0kCFbt_"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, activation):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        # same -> downsample -> same -> downsample\n",
        "        self.conv0 = GatedConv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv1 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv2 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv3 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # 2 x down conv\n",
        "        self.conv4 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv5 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # hypergraph conv\n",
        "\n",
        "        # 4 x same conv\n",
        "        self.conv6 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 2, dilation = 2, activation = activation)\n",
        "        self.conv7 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 4, dilation = 4, activation = activation)\n",
        "        self.conv8 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 8, dilation = 8, activation = activation)\n",
        "        self.conv9 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 16, dilation = 16, activation = activation)\n",
        "\n",
        "        # 2 x up conv\n",
        "        self.conv10 = GatedUpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
        "        self.conv11 = GatedUpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
        "\n",
        "        # hypergraph conv\n",
        "        \n",
        "\n",
        "        # upsample -> same -> upsample -> same\n",
        "        self.conv12 = GatedUpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
        "        self.conv13 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv14 = GatedUpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
        "        self.conv15 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # final\n",
        "        self.final = nn.Conv2d(hidden_dim, output_dim, kernel_size = 3, stride = 1, padding = 'same')\n",
        "\n",
        "        # for contrastive learning, 1x1 conv to compress feature map into 1 channel\n",
        "        self.conv_feature = nn.Conv2d(hidden_dim, 1, kernel_size = 1, stride = 1, padding = 'same')\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "\n",
        "        x = self.conv0(input_tensor)\n",
        "        \n",
        "        # downsample\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # middle layers\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        # dilated conv with residual skips\n",
        "        x = self.conv6(x) + x\n",
        "        x = self.conv7(x) + x\n",
        "        x = self.conv8(x) + x\n",
        "        x = self.conv9(x) + x\n",
        "\n",
        "        # extract for contrastive loss\n",
        "        x_feature = self.conv_feature(x)\n",
        "\n",
        "        # middle layers\n",
        "        x = self.conv10(x)\n",
        "        x = self.conv11(x)\n",
        "\n",
        "        # upsample\n",
        "        x = self.conv12(x)\n",
        "        x = self.conv13(x)\n",
        "        x = self.conv14(x)\n",
        "        x = self.conv15(x)\n",
        "        \n",
        "        # final\n",
        "        x = self.final(x)\n",
        "\n",
        "        return x, x_feature\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, input_shape, activation):\n",
        "\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # 5 layers down\n",
        "        self.conv0 = Conv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv1 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv2 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv3 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv4 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # linear to predict classes\n",
        "        LATENT_H = input_shape//(2**5)\n",
        "        self.linear = nn.Linear(LATENT_H**2 * hidden_dim, 1)\n",
        "\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \n",
        "        x = self.conv0(input_tensor)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        # scores\n",
        "        x = torch.flatten(x, start_dim = 1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSjQKMf6EmH2"
      },
      "source": [
        "# Training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "K26yI7ABEVmI"
      },
      "outputs": [],
      "source": [
        "MODEL_PARAMS = {\n",
        "    \"description\" : \"\"\"\n",
        "        Base structure same as the GLCIC model. Gated convolutions replace all convolutions in the generator, \n",
        "        and a 1x1 convolution is used to flatten the output of the 4th dilated convolution layer. \n",
        "        This is the latent vector used for contrastive learning.\"\"\",\n",
        "    \"hidden_dim\" : 64,\n",
        "    \"activation\" : nn.Mish,\n",
        "}\n",
        "\n",
        "SAVE_PATHS = {\n",
        "    \"generator1\" : \"/content/drive/MyDrive/NUS/CS4243/ContrastiveGAN/generator1\",\n",
        "    \"generator2\" : \"/content/drive/MyDrive/NUS/CS4243/ContrastiveGAN/generator1\",\n",
        "    \"discriminator\" : \"/content/drive/MyDrive/NUS/CS4243/ContrastiveGAN/discriminator\"\n",
        "}\n",
        "\n",
        "TRAINING_PARAMS = {\n",
        "    \"num_epochs\" : 40,\n",
        "    \"batch_size\" : 128, \n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"alpha\": 0.1,\n",
        "    \"beta\" : 0.01,\n",
        "    \"schedule_every\" : 5,\n",
        "    \"sample_size\" : 16,\n",
        "    \"log_every\" : 10,\n",
        "    \"save_paths\" : SAVE_PATHS,\n",
        "}\n",
        "\n",
        "LOGGING_CONFIG = {\n",
        "    \"model_params\" : MODEL_PARAMS,\n",
        "}\n",
        "LOGGING_CONFIG.update(TRAINING_PARAMS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UEXbNHjGvaE"
      },
      "source": [
        "# Experiment intialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbIsqCWTGvGE",
        "outputId": "a1df7c8d-a350-4914-ce06-e4b0da2b8599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model has 1.11642 million parameters\n",
            "model has 1.11642 million parameters\n",
            "model has 0.150401 million parameters\n",
            "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
            "Loading model from: /home/e/e0425222/miniconda3/envs/env/lib/python3.7/site-packages/lpips/weights/v0.1/vgg.pth\n"
          ]
        }
      ],
      "source": [
        "# 1. initialize model\n",
        "generator1 = Generator(input_dim = 4, hidden_dim = MODEL_PARAMS[\"hidden_dim\"], output_dim = 3, activation = MODEL_PARAMS[\"activation\"])\n",
        "generator2 = Generator(input_dim = 4, hidden_dim = MODEL_PARAMS[\"hidden_dim\"], output_dim = 3, activation = MODEL_PARAMS[\"activation\"])\n",
        "discriminator = Discriminator(input_dim = 3, hidden_dim = MODEL_PARAMS[\"hidden_dim\"], input_shape = 64, activation = MODEL_PARAMS[\"activation\"])\n",
        "\n",
        "summary(generator1)\n",
        "summary(generator2)\n",
        "summary(discriminator)\n",
        "\n",
        "# 2. device\n",
        "parallel = True\n",
        "device = 'cuda:0'  \n",
        "devices = [0,1,2,3]\n",
        "\n",
        "if not parallel:\n",
        "    generator1 = generator1.to(device)\n",
        "    generator2 = generator2.to(device)\n",
        "    discriminator = discriminator.to(device)\n",
        "else:\n",
        "    generator1 = nn.DataParallel(generator1, device_ids = devices)\n",
        "    generator2 = nn.DataParallel(generator2, device_ids = devices)\n",
        "    discriminator = nn.DataParallel(discriminator, device_ids = devices)\n",
        "\n",
        "# 3. initialize loss functions\n",
        "recon_loss_function = lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask), gt * (1-mask), reduction = 'none').sum()/(1-mask).sum()\n",
        "contrastive_loss_function_same = lambda x1, x2 : nn.functional.mse_loss(x1, x2)\n",
        "discriminator_loss_function = nn.functional.binary_cross_entropy_with_logits\n",
        "\n",
        "# 4. initialize metrics\n",
        "VGG_LPIPS = lpips.LPIPS(net = 'vgg').to(device)\n",
        "METRICS = {\n",
        "    \"Peak SnR (Whole)\" : lambda img, gt, mask : torchmetrics.functional.peak_signal_noise_ratio(img * (1-mask) + gt * mask, gt),\n",
        "    \"L2 loss (Whole)\" : lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask) + gt * mask, gt),\n",
        "    \"L2 loss (Mask)\" : lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum(),\n",
        "    \"L1 loss (Whole)\" : lambda img, gt, mask : nn.functional.l1_loss(img * (1-mask) + gt * mask, gt),\n",
        "    \"L1 loss (Mask)\" : lambda img, gt, mask : nn.functional.l1_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum(),\n",
        "    \"LPIPS (Whole)\" : (lambda img, gt, mask : VGG_LPIPS(img * (1-mask) + gt * mask, gt).mean()),\n",
        "}\n",
        "\n",
        "\n",
        "# 5. initialize optimizers\n",
        "generator1_optimizer = torch.optim.Adam(generator1.parameters(), lr = TRAINING_PARAMS[\"learning_rate\"])\n",
        "generator1_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(generator1_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
        "\n",
        "generator2_optimizer = torch.optim.Adam(generator2.parameters(), lr = TRAINING_PARAMS[\"learning_rate\"])\n",
        "generator2_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(generator2_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
        "\n",
        "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr = TRAINING_PARAMS[\"learning_rate\"])\n",
        "discriminator_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(discriminator_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
        "\n",
        "# 6. wrap into training dictionary\n",
        "TRAINING_PARAMS[\"generator1_model\"] = generator1\n",
        "TRAINING_PARAMS[\"generator1_optimizer\"] = generator1_optimizer\n",
        "TRAINING_PARAMS[\"generator1_scheduler\"] = generator1_scheduler\n",
        "TRAINING_PARAMS[\"generator2_model\"] = generator2\n",
        "TRAINING_PARAMS[\"generator2_optimizer\"] = generator2_optimizer\n",
        "TRAINING_PARAMS[\"generator2_scheduler\"] = generator2_scheduler\n",
        "TRAINING_PARAMS[\"discriminator_model\"] = discriminator\n",
        "TRAINING_PARAMS[\"discriminator_optimizer\"] = discriminator_optimizer\n",
        "TRAINING_PARAMS[\"discriminator_scheduler\"] = discriminator_scheduler\n",
        "TRAINING_PARAMS[\"discriminator_loss_function\"] = discriminator_loss_function\n",
        "TRAINING_PARAMS[\"contrastive_loss_function\"] = contrastive_loss_function_same\n",
        "TRAINING_PARAMS[\"recon_loss_function\"] = recon_loss_function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "6rbEmO4xoVR4",
        "outputId": "75c4f786-f5b7-42de-b20d-5e6f78eaaccf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:38490klj) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab0e98228aa144dfb0b3ae30ccc79dd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">honest-firefly-20</strong>: <a href=\"https://wandb.ai/cs4243_project/ContrastiveGAN/runs/38490klj\" target=\"_blank\">https://wandb.ai/cs4243_project/ContrastiveGAN/runs/38490klj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220412_141302-38490klj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:38490klj). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.14 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/e/e0425222/CS4243-project/active_experiments/ContrastiveGAN/wandb/run-20220412_141630-349xi38g</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs4243_project/ContrastiveGAN/runs/349xi38g\" target=\"_blank\">iconic-dew-21</a></strong> to <a href=\"https://wandb.ai/cs4243_project/ContrastiveGAN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# initialize wandb logging\n",
        "\n",
        "wandb.init(project=\"ContrastiveGAN\", entity=\"cs4243_project\")\n",
        "wandb.config = LOGGING_CONFIG\n",
        "\n",
        "wandb.watch(\n",
        "    (generator1, generator2, discriminator),\n",
        "    criterion = None,\n",
        "    log = 'all',\n",
        "    log_freq = 1,\n",
        "    idx = 0, \n",
        "    log_graph = False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsM_LVhrKiE8"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIMfSPuZaJpQ"
      },
      "source": [
        "## Train functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "QFJP7gjJSGFH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_epoch(device, train_dataloader, training_params : dict, metrics : dict, log_wandb = True):\n",
        "    \n",
        "    # ===== INITIALIZE =====\n",
        "    # constants\n",
        "    RECONSTRUCTION_LOSS = training_params[\"recon_loss_function\"]\n",
        "    DISCRIMINATOR_LOSS_FUNCTION = training_params[\"discriminator_loss_function\"]\n",
        "    CONTRASTIVE_LOSS_FUNCTION = training_params[\"contrastive_loss_function\"]\n",
        "\n",
        "    GENERATOR1_OPTIMIZER = training_params[\"generator1_optimizer\"]\n",
        "    GENERATOR2_OPTIMIZER = training_params[\"generator2_optimizer\"]\n",
        "    DISCRIMINATOR_OPTIMIZER = training_params[\"discriminator_optimizer\"]\n",
        "\n",
        "    BATCH_EVALUATE_EVERY = 5\n",
        "    LOG_EVERY = training_params[\"log_every\"]\n",
        "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
        "    BATCH_SIZE = training_params[\"batch_size\"]\n",
        "    ALPHA = training_params[\"alpha\"]\n",
        "    BETA = training_params[\"beta\"]\n",
        "\n",
        "    # models\n",
        "    generator1 = training_params[\"generator1_model\"].to(device).train()\n",
        "    generator2 = training_params[\"generator2_model\"].to(device).train()\n",
        "    discriminator = training_params[\"discriminator_model\"].to(device).train()\n",
        "\n",
        "    # epoch metrics\n",
        "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
        "    running_results[\"loss_discriminator\"] = 0.0\n",
        "    running_results[\"loss_generator1\"] = 0.0\n",
        "    running_results[\"loss_generator1_r\"] = 0.0\n",
        "    running_results[\"loss_generator1_adv\"] = 0.0\n",
        "    running_results[\"loss_generator2\"] = 0.0\n",
        "    running_results[\"loss_generator2_r\"] = 0.0\n",
        "    running_results[\"loss_generator2_adv\"] = 0.0\n",
        "    running_results[\"loss_contrastive\"] = 0.0\n",
        "\n",
        "    # ===== TRAIN EPOCH =====\n",
        "    num_batches = 0\n",
        "    for _, batch in enumerate(train_dataloader, 1):\n",
        "\n",
        "            # ===== INITIALIZE =====\n",
        "            num_batches += 1\n",
        "\n",
        "            # input and ground truth\n",
        "            input_batched = batch[\"image\"]\n",
        "            ground_truth_batched = batch[\"reconstructed\"]\n",
        "            mask_batched = batch[\"mask\"]\n",
        "\n",
        "            # sanity check\n",
        "            assert input_batched.shape[0] == ground_truth_batched.shape[0]\n",
        "\n",
        "            # move tensors to device\n",
        "            input_batched = input_batched.to(device)\n",
        "            ground_truth_batched = ground_truth_batched.to(device)\n",
        "            mask_batched = mask_batched.to(device)\n",
        "\n",
        "            # set the gradients to zeros\n",
        "            GENERATOR1_OPTIMIZER.zero_grad()\n",
        "            GENERATOR2_OPTIMIZER.zero_grad()\n",
        "            DISCRIMINATOR_OPTIMIZER.zero_grad()\n",
        "\n",
        "            # reshape to channel first\n",
        "            input_batched = input_batched.permute(0, 3, 1, 2)\n",
        "            ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
        "            mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
        "\n",
        "            # ===== FORWARD PASS =====\n",
        "\n",
        "            # 1. train discriminator\n",
        "            \n",
        "            # 1.1 generate images\n",
        "            input_batched.requires_grad_()\n",
        "            output1_batched, _ = generator1(input_batched)\n",
        "            shuffled_indices = torch.randperm(n = BATCH_SIZE)\n",
        "            output2_batched, _ = generator2(input_batched[shuffled_indices]) # random permutation\n",
        "\n",
        "            # 1.2 splice with ground truth\n",
        "            spliced1_batched = ((1-mask_batched) * output1_batched) + (mask_batched * ground_truth_batched) \n",
        "            spliced2_batched = ((1-mask_batched[shuffled_indices]) * output2_batched) + (mask_batched[shuffled_indices] * ground_truth_batched[shuffled_indices]) \n",
        "\n",
        "            # 1.3 feed into discriminator\n",
        "            label_real = torch.ones(BATCH_SIZE * 2, 1).to(device)\n",
        "            label_fake = torch.zeros(BATCH_SIZE * 2, 1).to(device)\n",
        "\n",
        "            pred_real = discriminator(torch.cat([ground_truth_batched, ground_truth_batched[shuffled_indices]], dim = 0))\n",
        "            loss_real = DISCRIMINATOR_LOSS_FUNCTION(pred_real, label_real)\n",
        "\n",
        "            pred_fake = discriminator(torch.cat([spliced1_batched, spliced2_batched[shuffled_indices]], dim = 0))\n",
        "            loss_fake = DISCRIMINATOR_LOSS_FUNCTION(pred_fake, label_fake)\n",
        "            loss_d = ALPHA * (loss_real + loss_fake)\n",
        "            loss_d.backward()\n",
        "            DISCRIMINATOR_OPTIMIZER.step()\n",
        "\n",
        "            # 2. train generator 1 (reconstruction, adverserial, contrastive)\n",
        "            # 2.1 forward pass by generator to produce images, splice them\n",
        "            output1_batched, _ = generator1(input_batched)\n",
        "            spliced1_batched = ((1-mask_batched) * output1_batched) + (mask_batched * ground_truth_batched) \n",
        "            \n",
        "            # 2.2 reconstruction loss\n",
        "            loss_g1_reconstruction = RECONSTRUCTION_LOSS(output1_batched, ground_truth_batched, mask_batched)\n",
        "\n",
        "            # 2.3 adverserial loss\n",
        "            label_real = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "            pred1_adverserial = discriminator(spliced1_batched)\n",
        "            loss_g1_adverserial = DISCRIMINATOR_LOSS_FUNCTION(pred1_adverserial, label_real) # want it to classify all as real\n",
        "\n",
        "            # 2.4 contrastive loss\n",
        "            shuffled_indices = torch.randperm(n = BATCH_SIZE)\n",
        "            copied_input_batched = torch.cat([ground_truth_batched * mask_batched[shuffled_indices], mask_batched[shuffled_indices]], dim = 1)\n",
        "            _, x1 = generator1(input_batched)\n",
        "            _, x2 = generator2(copied_input_batched) # same image, different masks\n",
        "            loss_contrastive1 = CONTRASTIVE_LOSS_FUNCTION(x1, x2)\n",
        "\n",
        "            # 2.5 backprop\n",
        "            loss_g1 = loss_g1_reconstruction + ALPHA * loss_g1_adverserial + BETA * loss_contrastive1\n",
        "            loss_g1.backward()\n",
        "            GENERATOR1_OPTIMIZER.step()\n",
        "\n",
        "            # 3.  train generator 2 (reconstruction, adverserial, contrastive)\n",
        "            # 3.1 forward pass by generator to produce images, reconstruction loss\n",
        "            output2_batched, _ = generator2(input_batched)\n",
        "            spliced2_batched = ((1-mask_batched) * output2_batched) + (mask_batched * ground_truth_batched) \n",
        "            loss_g2_reconstruction = RECONSTRUCTION_LOSS(output2_batched, ground_truth_batched, mask_batched)\n",
        "\n",
        "            # 3.2 adverserial loss\n",
        "            label_real = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "            pred2_adverserial = discriminator(spliced2_batched)\n",
        "            loss_g2_adverserial = DISCRIMINATOR_LOSS_FUNCTION(pred2_adverserial, label_real) # want it to classify all as real\n",
        "\n",
        "            # 3.3 contrastive loss\n",
        "            shuffled_indices = torch.randperm(n = BATCH_SIZE)\n",
        "            copied_input_batched = torch.cat([ground_truth_batched * mask_batched[shuffled_indices], mask_batched[shuffled_indices]], dim = 1)\n",
        "            _, x1 = generator1(input_batched)\n",
        "            _, x2 = generator2(copied_input_batched) # same image, different masks\n",
        "            loss_contrastive2 = CONTRASTIVE_LOSS_FUNCTION(x1, x2)\n",
        "\n",
        "            # 3.4 backprop\n",
        "            loss_g2 = loss_g2_reconstruction + ALPHA * loss_g2_adverserial + BETA * loss_contrastive2\n",
        "            loss_g2.backward()\n",
        "            GENERATOR2_OPTIMIZER.step()\n",
        "\n",
        "\n",
        "            # ===== COMPUTE STATISTICS, USING TORCH METRICS =====  \n",
        "            # 1. compute losses\n",
        "\n",
        "            running_results[\"loss_generator1\"] += loss_g1.detach().item()\n",
        "            running_results[\"loss_generator1_r\"] += loss_g1_reconstruction.detach().item()\n",
        "            running_results[\"loss_generator1_adv\"] += loss_g1_adverserial.detach().item()\n",
        "            running_results[\"loss_generator2\"] += loss_g2.detach().item()\n",
        "            running_results[\"loss_generator2_r\"] += loss_g2_reconstruction.detach().item()\n",
        "            running_results[\"loss_generator2_adv\"] += loss_g2_adverserial.detach().item()\n",
        "            running_results[\"loss_contrastive\"] += (loss_contrastive1.detach().item() + loss_contrastive2.detach().item())/2\n",
        "            running_results[\"loss_discriminator\"] += loss_d.detach().item()\n",
        "\n",
        "            # 2. for each key, compute, add item to results dictionary (take average of 2 generators)\n",
        "            for key, func in metrics.items():\n",
        "                res1 = func(output1_batched, ground_truth_batched, mask_batched).detach().item()\n",
        "                res2 = func(output2_batched, ground_truth_batched[shuffled_indices], mask_batched[shuffled_indices]).detach().item()\n",
        "                running_results[key] += (res1 + res2)/2\n",
        "\n",
        "            # 3. log with wandb\n",
        "            if log_wandb and (num_batches % LOG_EVERY == 0):\n",
        "\n",
        "                # generator 1\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced1_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images1 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "                \n",
        "                # generator 2\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[shuffled_indices][:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[shuffled_indices][:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced2_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images2 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "\n",
        "                # log images and some metadata\n",
        "                wandb.log( {\n",
        "                    \"generator1_train_images\" : images1,\n",
        "                    \"generator2_train_images\" : images2,\n",
        "                    \"lr_generator1\" : GENERATOR1_OPTIMIZER.param_groups[0]['lr'],\n",
        "                    \"lr_generator2\" : GENERATOR2_OPTIMIZER.param_groups[0]['lr'],\n",
        "                    \"lr_discriminator\" : DISCRIMINATOR_OPTIMIZER.param_groups[0]['lr']\n",
        "\n",
        "                })\n",
        "\n",
        "                # log all metrics\n",
        "                wandb.log(\n",
        "                    {key : item/num_batches for key, item in running_results.items()}\n",
        "                )\n",
        "        \n",
        "\n",
        "            # ===== HOUSEKEEPING =====\n",
        "            del loss_g2\n",
        "            del loss_g1\n",
        "            del loss_d\n",
        "            del input_batched\n",
        "\n",
        "            # print results every some batches\n",
        "            if num_batches % BATCH_EVALUATE_EVERY == 0: \n",
        "\n",
        "                args = \"\"\n",
        "                for key, val in running_results.items():\n",
        "                    args += key + \": \" + str(running_results[key]/num_batches) + \"   \"\n",
        "                print(f\"\\r{num_batches}/{len(train_dataloader)}: \" + args, end = '', flush = True)\n",
        "\n",
        "    # normalise numbers by batch\n",
        "    for key, val in running_results.items():\n",
        "        running_results[key] /= num_batches\n",
        "\n",
        "    return running_results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4CuiHVAjKitr"
      },
      "outputs": [],
      "source": [
        "def evaluate_epoch(device, validation_dataloader, training_params : dict, metrics : dict, log_wandb = True):\n",
        "\n",
        "    # ===== INITIALIZE =====\n",
        "    # models\n",
        "    generator1 = training_params[\"generator1_model\"].to(device).eval()\n",
        "    generator2 = training_params[\"generator2_model\"].to(device).eval()\n",
        "\n",
        "    # constants\n",
        "    BATCH_SIZE = training_params[\"batch_size\"]\n",
        "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
        "\n",
        "    # epoch statistics\n",
        "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
        "\n",
        "    # ===== EVALUATE EPOCH =====\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batches = 0\n",
        "        for index, batch in enumerate(validation_dataloader, 1):\n",
        "            \n",
        "            batches += 1\n",
        "\n",
        "            # input and ground truth\n",
        "            input_batched = batch[\"image\"]\n",
        "            ground_truth_batched = batch[\"reconstructed\"]\n",
        "            mask_batched = batch[\"mask\"]\n",
        "\n",
        "            # move tensors to device\n",
        "            input_batched = input_batched.to(device)\n",
        "            ground_truth_batched = ground_truth_batched.to(device)\n",
        "            mask_batched = mask_batched.to(device)\n",
        "\n",
        "            # get shuffled indices\n",
        "            shuffled_indices = torch.randperm(n = BATCH_SIZE).long()\n",
        "\n",
        "            # ===== FORWARD PASS =====\n",
        "\n",
        "            # 1. reshape to channel first\n",
        "            input_batched = input_batched.permute(0, 3, 1, 2)\n",
        "            ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
        "            mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
        "\n",
        "            # 2. predict    \n",
        "            output1_batched = generator1(input_batched)\n",
        "            output2_batched = generator2(input_batched[shuffled_indices])           \n",
        "            spliced1_batched = ((1-mask_batched) * output1_batched) + (mask_batched * ground_truth_batched) \n",
        "            spliced2_batched = ((1-mask_batched[shuffled_indices]) * output2_batched) + (mask_batched[shuffled_indices] * ground_truth_batched[shuffled_indices]) \n",
        "\n",
        "            # 3. evaluate\n",
        "            for key, func in metrics.items():\n",
        "                running_results[key] += (func(output1_batched, ground_truth_batched, mask_batched).detach().item() + func(output2_batched, ground_truth_batched[shuffled_indices], mask_batched[shuffled_indices]).detach().item())/2\n",
        "\n",
        "            args = \"\"\n",
        "            for key, val in running_results.items():\n",
        "                args += key + \": \" + str(running_results[key]/batches) + \"   \"\n",
        "            print(f\"\\r{batches}/{len(validation_dataloader)}: \" + args, end = '', flush = True)\n",
        "\n",
        "            # 4. log \n",
        "            if log_wandb:\n",
        "\n",
        "                # generator 1\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced1_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images1 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "                \n",
        "                # generator 2\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[shuffled_indices][:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[shuffled_indices][:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced2_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images2 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "\n",
        "                # log images and some metadata\n",
        "                wandb.log( {\n",
        "                    \"generator1_val_images\" : images1,\n",
        "                    \"generator2_val_images\" : images2\n",
        "                })\n",
        "\n",
        "                # log all metrics\n",
        "                wandb.log(\n",
        "                    {key : item/batches for key, item in running_results.items()}\n",
        "                )\n",
        "\n",
        "    # normalise numbers by batch\n",
        "    for key, val in running_results.items():\n",
        "        running_results[key] /= batches\n",
        "\n",
        "    return running_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "O_mQDSLXaPdz"
      },
      "outputs": [],
      "source": [
        "def train_evaluate(device, train_dataset, validation_dataset, training_params: dict, metrics: dict, start_epoch = 0, log_wandb = True):\n",
        "\n",
        "    # ===== INITIALIZE =====\n",
        "    # constants\n",
        "    NUM_EPOCHS = training_params[\"num_epochs\"]\n",
        "    BATCH_SIZE = training_params[\"batch_size\"]\n",
        "    GENERATOR1_SCHEDULER = training_params[\"generator1_scheduler\"]\n",
        "    GENERATOR2_SCHEDULER = training_params[\"generator2_scheduler\"]\n",
        "    DISCRIMINATOR_SCHEDULER = training_params[\"discriminator_scheduler\"]\n",
        "    SAVE_PATHS = training_params[\"save_paths\"]\n",
        "    NUM_WORKERS = 2\n",
        "    START_EPOCH = start_epoch\n",
        "\n",
        "    # models for saving\n",
        "    generator1 = training_params[\"generator1_model\"].to(device).train()\n",
        "    generator2 = training_params[\"generator2_model\"].to(device).train()\n",
        "    discriminator = training_params[\"discriminator_model\"].to(device).train()\n",
        "\n",
        "    # variable losses\n",
        "    train_results = {list(metrics.keys())[i] : [] for i in range(len(metrics)) } \n",
        "\n",
        "    train_results[\"loss_discriminator\"] = []\n",
        "    train_results[\"loss_generator1\"] = []\n",
        "    train_results[\"loss_generator1_r\"] = []\n",
        "    train_results[\"loss_generator1_adv\"] = []\n",
        "    train_results[\"loss_generator2\"] = []\n",
        "    train_results[\"loss_generator2_r\"] = []\n",
        "    train_results[\"loss_generator2_adv\"] = []\n",
        "    train_results[\"loss_contrastive\"] = []\n",
        "\n",
        "    eval_results = {list(metrics.keys())[i] : [] for i in range(len(metrics)) } \n",
        "\n",
        "    # dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS, drop_last = True)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS, drop_last = True)\n",
        "\n",
        "    # ===== TRAIN =====\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # train\n",
        "        print(f\"\\n===== Epoch: {START_EPOCH + epoch + 1} ===== \")\n",
        "        num_batches = 0\n",
        "\n",
        "        # train every epoch\n",
        "        results = train_epoch(device, train_dataloader, training_params, metrics, log_wandb = log_wandb)\n",
        "        for key, val in results.items():\n",
        "            train_results[key].append(val)\n",
        "\n",
        "        # evaluate every epoch\n",
        "        print()\n",
        "        results = evaluate_epoch(device, validation_dataloader, training_params, metrics, log_wandb = log_wandb)\n",
        "        for key, val in results.items():\n",
        "            eval_results[key].append(val)\n",
        "\n",
        "        # ===== EPOCH RESULTS =====\n",
        "        print(f\"\\nCompleted epoch {START_EPOCH + epoch + 1}! Took {(time.time() - start)/60} min\")\n",
        "\n",
        "        # ===== HOUSEKEEPING =====\n",
        "\n",
        "        # scheduler every epoch\n",
        "        if DISCRIMINATOR_SCHEDULER is not None:\n",
        "            DISCRIMINATOR_SCHEDULER.step(eval_results[\"L2 loss (Mask)\"][epoch])\n",
        "        if GENERATOR1_SCHEDULER is not None:\n",
        "            GENERATOR1_SCHEDULER.step(eval_results[\"L2 loss (Mask)\"][epoch])\n",
        "        if GENERATOR2_SCHEDULER is not None:\n",
        "            GENERATOR2_SCHEDULER.step(eval_results[\"L2 loss (Mask)\"][epoch])\n",
        "            \n",
        "        # save every epoch\n",
        "        SAVE = f\"{SAVE_PATHS['generator1']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
        "        torch.save(generator1.state_dict(), SAVE)\n",
        "        SAVE = f\"{SAVE_PATHS['generator2']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
        "        torch.save(generator2.state_dict(), SAVE)\n",
        "        SAVE = f\"{SAVE_PATHS['discriminator']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
        "        torch.save(discriminator.state_dict(), SAVE)  \n",
        "\n",
        "        print(\"Saved models!\")\n",
        "\n",
        "    return train_results, eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8IkxVhsVsf4"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_IuN9J8Jfp9j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Epoch: 1 ===== \n",
            "15/78: Peak SnR (Whole): 20.090901565551757   L2 loss (Whole): 0.011802663002163172   L2 loss (Mask): 1.0151111821333567   L1 loss (Whole): 0.015279846607397   L1 loss (Mask): 1.317723375558853   LPIPS (Whole): 0.13512772992253302   loss_discriminator: 0.13884005149205525   loss_generator1: 1.2622060855229695   loss_generator1_r: 1.1654757142066956   loss_generator1_adv: 0.6655683716138204   loss_generator2: 0.8371228893597921   loss_generator2_r: 0.7419055034716924   loss_generator2_adv: 0.652996853987376   loss_contrastive: 3.004560875892639     "
          ]
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "train_evaluate(device, train_dataset, valid_dataset, TRAINING_PARAMS, METRICS, start_epoch = 0, log_wandb = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrCk_e1oU44V"
      },
      "outputs": [],
      "source": [
        "# train_epoch(device, DataLoader(train_dataset, batch_size = 16), TRAINING_PARAMS, METRICS, log_wandb = True)\n",
        "# evaluate_epoch(device, DataLoader(valid_dataset, batch_size = 16),, TRAINING_PARAMS, metrics, log_wandb = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "fdEKy-uZU6fu",
        "outputId": "87ad1772-6436-4fb6-80df-2db4b44cd03c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ContrastiveGAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
