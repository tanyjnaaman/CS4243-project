{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H98D0BHAEioA"
      },
      "source": [
        "# Initialize notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lpips\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchmetrics\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/home/e/e0425222/CS4243-project\")\n",
        "from utils.dataset_utils.AnimalDataset import AnimalDataset\n",
        "from utils.train_utils.train_utils import sample_batch, summary\n",
        "from utils.train_utils.model_utils import Conv2dBlock, GatedConv2dBlock, GatedUpConv2dBlock, LinearBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKzaTBy__baX"
      },
      "outputs": [],
      "source": [
        "train_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_train.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n",
        "\n",
        "valid_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_val.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n",
        "\n",
        "test_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_test.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sanity check \n",
        "sample_batch(train_dataset, sample_size = 6)\n",
        "sample_batch(valid_dataset, sample_size = 6)\n",
        "sample_batch(test_dataset, sample_size = 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouUSbxVkE92G"
      },
      "source": [
        "# Introduction\n",
        "This notebook aims to explore how the use of graphs to encode global information can improve capturing global context in image inpainting. The relevant references are:\n",
        "1. How Powerful are Graph Neural Networks? (https://arxiv.org/abs/1810.00826)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oL0U4VbFaeJ"
      },
      "source": [
        "# Model experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch_geometric as torch_g\n",
        "import torch_geometric.nn as gnn\n",
        "\n",
        "class GraphConvModule(nn.Module):\n",
        "    \"\"\"\n",
        "    This module implements GIN convolution on images using local, global and channel features to predict\n",
        "    the adjacency tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, kernel_size, stride, padding, dilation, activation = nn.ReLU):\n",
        "\n",
        "        super(GraphConvModule, self).__init__()\n",
        "\n",
        "\n",
        "        # incidence matrix\n",
        "        self.feature_conv = nn.Conv2d(channels, channels, kernel_size = kernel_size, stride = stride, padding = 'same')\n",
        "        self.channel_conv = nn.Conv2d(channels, channels, kernel_size = 1, stride = 1, padding = 'same')\n",
        "        self.global_feature_conv = nn.Conv2d(channels, channels, kernel_size = kernel_size, stride = stride, padding = padding, dilation = dilation)\n",
        "\n",
        "        # graph conv\n",
        "        self.gnn_linear = nn.Sequential(\n",
        "            nn.Linear(channels, channels),\n",
        "            activation()\n",
        "        )\n",
        "        self.gnn = gnn.DenseGINConv(self.gnn_linear)\n",
        "\n",
        "    def forward(self, input_tensor, return_adj = False):\n",
        "\n",
        "        # metadata\n",
        "        b, c, h, w = input_tensor.shape\n",
        "        \n",
        "        # 1. compute local and channel features, normalize\n",
        "        local_features = self.feature_conv(input_tensor).view(b, c, h * w) # b x c x hw\n",
        "        channel_features = torch.mean(self.channel_conv(input_tensor), dim = [2,3]).unsqueeze(-1) # b x c, global mean pool\n",
        "        local_channel_features = (local_features * channel_features).permute(0, 2, 1) # -> b x hw x c\n",
        "        local_channel_features = nn.functional.normalize(local_channel_features, p = 2, dim = 2) # normalize vector at each node\n",
        "\n",
        "        # 2. compute gloabl features, normalize\n",
        "        global_features = self.global_feature_conv(input_tensor).view(b, c, h * w) # b x c x hw\n",
        "        global_features = nn.functional.normalize(global_features, p = 2, dim = 2) # normalize vector at each node\n",
        "\n",
        "        # 3. predict adj tensor\n",
        "        adj_tensor = torch.sigmoid(torch.bmm(local_channel_features, global_features)) # (b x hw x c) x (b x c x hw) -> (b x hw x hw)\n",
        "\n",
        "        # 4. graph conv\n",
        "        x = input_tensor.view(b, c, h*w).permute(0, 2, 1)  # -> b x hw x c\n",
        "        x = self.gnn(x, adj_tensor) # -> b x hw x c\n",
        "\n",
        "        # 5. reshape back to image\n",
        "        x = x.permute(0, 2, 1).view(b, c, h, w)\n",
        "\n",
        "        if return_adj:\n",
        "            return x, adj_tensor\n",
        "            \n",
        "        return x\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "h2hEL0kCFbt_"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, activation):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        # same -> downsample -> same -> downsample\n",
        "        self.conv0 = GatedConv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv1 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv2 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv3 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # graph conv\n",
        "        self.graphconv1 = GraphConvModule(hidden_dim, kernel_size = 3, stride = 1, padding = 2, dilation = 2, activation = activation)\n",
        "\n",
        "        # 2 x down conv\n",
        "        self.conv4 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv5 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # graph conv\n",
        "        self.graphconv2 = GraphConvModule(hidden_dim, kernel_size = 3, stride = 1, padding = 2, dilation = 2, activation = activation)\n",
        "\n",
        "        # 4 x same conv\n",
        "        self.conv6 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv7 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv8 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv9 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # 2 x up conv\n",
        "        self.conv10 = GatedUpConv2dBlock(hidden_dim * 2, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
        "        self.conv11 = GatedUpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
        "\n",
        "        # upsample -> same -> upsample -> same\n",
        "        self.conv12 = GatedUpConv2dBlock(hidden_dim * 2, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
        "        self.conv13 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv14 = GatedUpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
        "        self.conv15 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # final\n",
        "        self.final = nn.Conv2d(hidden_dim, output_dim, kernel_size = 3, stride = 1, padding = 'same')\n",
        "\n",
        "        # for contrastive learning, 1x1 conv to compress feature map into 1 channel\n",
        "        self.conv_feature = nn.Conv2d(hidden_dim, 1, kernel_size = 1, stride = 1, padding = 'same')\n",
        "\n",
        "    def forward(self, input_tensor, return_adj = False):\n",
        "\n",
        "        x = self.conv0(input_tensor)\n",
        "        \n",
        "        # downsample\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # graph conv\n",
        "        g1, adj1 = self.graphconv1(x, return_adj = True)\n",
        "\n",
        "        # middle downsample layers\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        # graph conv\n",
        "        g2, adj2 = self.graphconv2(x, return_adj = True)\n",
        "\n",
        "        # dilated conv with residual skips\n",
        "        x = self.conv6(x) + x\n",
        "        x = self.conv7(x) + x\n",
        "        x = self.conv8(x) + x\n",
        "        x = self.conv9(x) + x\n",
        "\n",
        "        # middle upsamplelayers\n",
        "        x = self.conv10(torch.cat([x, g2], dim = 1))\n",
        "        x = self.conv11(x)\n",
        "\n",
        "        # upsample\n",
        "        x = self.conv12(torch.cat([x, g1], dim = 1))\n",
        "        x = self.conv13(x)\n",
        "        x = self.conv14(x)\n",
        "        x = self.conv15(x)\n",
        "        \n",
        "        # final\n",
        "        x = self.final(x)\n",
        "\n",
        "        if return_adj:\n",
        "            return x, adj1, adj2\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, input_shape, activation):\n",
        "\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # 5 layers down\n",
        "        self.conv0 = Conv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv1 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv2 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv3 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv4 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # linear to predict classes\n",
        "        LATENT_H = input_shape//(2**5)\n",
        "        self.linear = nn.Linear(LATENT_H**2 * hidden_dim, 1)\n",
        "\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \n",
        "        x = self.conv0(input_tensor)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        # scores\n",
        "        x = torch.flatten(x, start_dim = 1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSjQKMf6EmH2"
      },
      "source": [
        "# Training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "K26yI7ABEVmI"
      },
      "outputs": [],
      "source": [
        "MODEL_PARAMS = {\n",
        "    \"description\" : \"\"\"\n",
        "        Base structure same as the GLCIC model, but with one added round of downsampling. \n",
        "        Gated convolutions replace all convolutions in the generator. No dilated convolutions used.\n",
        "        Graph-based features are created at each downsampling, and concatenated to corresponding\n",
        "        upsampling features.\"\"\",\n",
        "    \"hidden_dim\" : 64,\n",
        "    \"activation\" : nn.Mish,\n",
        "}\n",
        "\n",
        "SAVE_PATHS = {\n",
        "    \"generator\" : \"/home/e/e0425222/CS4243-project/active_experiments/GraphGAN/experiment_concat_nodilation_alpha0.1/generator/generator\",\n",
        "    \"discriminator\" : \"/home/e/e0425222/CS4243-project/active_experiments/GraphGAN/experiment_concat_nodilation_alpha0.1/discriminator/discriminator\"\n",
        "}\n",
        "\n",
        "TRAINING_PARAMS = {\n",
        "    \"num_epochs\" : 20,\n",
        "    \"batch_size\" : 64, \n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"alpha\": 0.1,\n",
        "    \"schedule_every\" : 5,\n",
        "    \"sample_size\" : 16,\n",
        "    \"log_every\" : 10,\n",
        "    \"save_paths\" : SAVE_PATHS,\n",
        "}\n",
        "\n",
        "LOGGING_CONFIG = {\n",
        "    \"model_params\" : MODEL_PARAMS,\n",
        "}\n",
        "LOGGING_CONFIG.update(TRAINING_PARAMS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UEXbNHjGvaE"
      },
      "source": [
        "# Experiment intialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbIsqCWTGvGE",
        "outputId": "a1df7c8d-a350-4914-ce06-e4b0da2b8599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model has 1.428228 million parameters\n",
            "model has 0.150401 million parameters\n",
            "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
            "Loading model from: /home/e/e0425222/miniconda3/envs/env/lib/python3.7/site-packages/lpips/weights/v0.1/vgg.pth\n"
          ]
        }
      ],
      "source": [
        "# 1. initialize model\n",
        "generator = Generator(input_dim = 4, hidden_dim = MODEL_PARAMS[\"hidden_dim\"], output_dim = 3, activation = MODEL_PARAMS[\"activation\"])\n",
        "discriminator = Discriminator(input_dim = 3, hidden_dim = MODEL_PARAMS[\"hidden_dim\"], input_shape = 64, activation = MODEL_PARAMS[\"activation\"])\n",
        "\n",
        "summary(generator)\n",
        "summary(discriminator)\n",
        "\n",
        "# 2. device\n",
        "parallel = True\n",
        "device = 'cuda:3'  \n",
        "devices = [3,1,2,0]\n",
        "\n",
        "if not parallel:\n",
        "    generator1 = generator.to(device)\n",
        "    discriminator = discriminator.to(device)\n",
        "else:\n",
        "    generator1 = nn.DataParallel(generator, device_ids = devices)\n",
        "    discriminator = nn.DataParallel(discriminator, device_ids = devices)\n",
        "\n",
        "# 3. initialize loss functions\n",
        "recon_loss_function = lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask), gt * (1-mask), reduction = 'none').sum()/(1-mask).sum()\n",
        "discriminator_loss_function = nn.functional.binary_cross_entropy_with_logits\n",
        "\n",
        "# 4. initialize metrics\n",
        "VGG_LPIPS = lpips.LPIPS(net = 'vgg').to(device)\n",
        "METRICS = {\n",
        "    \"Peak SnR (Whole)\" : lambda img, gt, mask : torchmetrics.functional.peak_signal_noise_ratio(img * (1-mask) + gt * mask, gt),\n",
        "    \"L2 loss (Whole)\" : lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask) + gt * mask, gt),\n",
        "    \"L2 loss (Mask)\" : lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum(),\n",
        "    \"L1 loss (Whole)\" : lambda img, gt, mask : nn.functional.l1_loss(img * (1-mask) + gt * mask, gt),\n",
        "    \"L1 loss (Mask)\" : lambda img, gt, mask : nn.functional.l1_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum(),\n",
        "    \"LPIPS (Whole)\" : (lambda img, gt, mask : VGG_LPIPS(img * (1-mask) + gt * mask, gt).mean()),\n",
        "}\n",
        "\n",
        "\n",
        "# 5. initialize optimizers\n",
        "generator_optimizer = torch.optim.Adam(generator.parameters(), lr = TRAINING_PARAMS[\"learning_rate\"])\n",
        "generator_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(generator_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
        "\n",
        "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr = TRAINING_PARAMS[\"learning_rate\"])\n",
        "discriminator_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(discriminator_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
        "\n",
        "# 6. wrap into training dictionary\n",
        "TRAINING_PARAMS[\"generator_model\"] = generator\n",
        "TRAINING_PARAMS[\"generator_optimizer\"] = generator_optimizer\n",
        "TRAINING_PARAMS[\"generator_scheduler\"] = generator_scheduler\n",
        "TRAINING_PARAMS[\"discriminator_model\"] = discriminator\n",
        "TRAINING_PARAMS[\"discriminator_optimizer\"] = discriminator_optimizer\n",
        "TRAINING_PARAMS[\"discriminator_scheduler\"] = discriminator_scheduler\n",
        "TRAINING_PARAMS[\"discriminator_loss_function\"] = discriminator_loss_function\n",
        "TRAINING_PARAMS[\"recon_loss_function\"] = recon_loss_function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialize logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "6rbEmO4xoVR4",
        "outputId": "75c4f786-f5b7-42de-b20d-5e6f78eaaccf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:1rqap8w6) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd21035ed65c48ccb41cb1979ab81d37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">visionary-field-9</strong>: <a href=\"https://wandb.ai/cs4243_project/GraphGAN/runs/1rqap8w6\" target=\"_blank\">https://wandb.ai/cs4243_project/GraphGAN/runs/1rqap8w6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220412_231802-1rqap8w6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:1rqap8w6). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.14 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/e/e0425222/CS4243-project/active_experiments/GraphGAN/experiment_concat_nodilation_alpha0.1/wandb/run-20220412_232033-mut2qpzk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs4243_project/GraphGAN/runs/mut2qpzk\" target=\"_blank\">glad-morning-10</a></strong> to <a href=\"https://wandb.ai/cs4243_project/GraphGAN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(project=\"GraphGAN\", entity=\"cs4243_project\")\n",
        "wandb.config = LOGGING_CONFIG\n",
        "\n",
        "wandb.watch(\n",
        "    (generator, discriminator),\n",
        "    criterion = None,\n",
        "    log = 'all',\n",
        "    log_freq = 1,\n",
        "    idx = 0, \n",
        "    log_graph = False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsM_LVhrKiE8"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIMfSPuZaJpQ"
      },
      "source": [
        "## Train functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QFJP7gjJSGFH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_epoch(device, train_dataloader, training_params : dict, metrics : dict, log_wandb = True):\n",
        "    \n",
        "    # ===== INITIALIZE =====\n",
        "    # constants\n",
        "    RECONSTRUCTION_LOSS = training_params[\"recon_loss_function\"]\n",
        "    DISCRIMINATOR_LOSS_FUNCTION = training_params[\"discriminator_loss_function\"]\n",
        "\n",
        "    GENERATOR_OPTIMIZER = training_params[\"generator_optimizer\"]\n",
        "    DISCRIMINATOR_OPTIMIZER = training_params[\"discriminator_optimizer\"]\n",
        "\n",
        "    BATCH_EVALUATE_EVERY = 5\n",
        "    LOG_EVERY = training_params[\"log_every\"]\n",
        "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
        "    BATCH_SIZE = training_params[\"batch_size\"]\n",
        "    ALPHA = training_params[\"alpha\"]\n",
        "\n",
        "    # models\n",
        "    generator = training_params[\"generator_model\"].to(device).train()\n",
        "    discriminator = training_params[\"discriminator_model\"].to(device).train()\n",
        "\n",
        "    # epoch metrics\n",
        "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
        "    running_results[\"loss_discriminator\"] = 0.0\n",
        "    running_results[\"loss_generator\"] = 0.0\n",
        "    running_results[\"loss_generator_r\"] = 0.0\n",
        "    running_results[\"loss_generator_adv\"] = 0.0\n",
        "\n",
        "    # ===== TRAIN EPOCH =====\n",
        "    num_batches = 0\n",
        "    for _, batch in enumerate(train_dataloader, 1):\n",
        "\n",
        "            # ===== INITIALIZE =====\n",
        "            num_batches += 1\n",
        "\n",
        "            # input and ground truth\n",
        "            input_batched = batch[\"image\"]\n",
        "            ground_truth_batched = batch[\"reconstructed\"]\n",
        "            mask_batched = batch[\"mask\"]\n",
        "\n",
        "            # sanity check\n",
        "            assert input_batched.shape[0] == ground_truth_batched.shape[0]\n",
        "\n",
        "            # move tensors to device\n",
        "            input_batched = input_batched.to(device)\n",
        "            ground_truth_batched = ground_truth_batched.to(device)\n",
        "            mask_batched = mask_batched.to(device)\n",
        "\n",
        "            # set the gradients to zeros\n",
        "            GENERATOR_OPTIMIZER.zero_grad()\n",
        "            DISCRIMINATOR_OPTIMIZER.zero_grad()\n",
        "\n",
        "            # reshape to channel first\n",
        "            input_batched = input_batched.permute(0, 3, 1, 2)\n",
        "            ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
        "            mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
        "\n",
        "            # ===== FORWARD PASS =====\n",
        "\n",
        "            # 1. train discriminator\n",
        "            \n",
        "            # 1.1 generate images\n",
        "            input_batched.requires_grad_()\n",
        "            output_batched = generator(input_batched)\n",
        "\n",
        "            # 1.2 splice with ground truth\n",
        "            spliced_batched = ((1-mask_batched) * output_batched) + (mask_batched * ground_truth_batched) \n",
        "\n",
        "            # 1.3 feed into discriminator\n",
        "            label_real = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "            label_fake = torch.zeros(BATCH_SIZE, 1).to(device)\n",
        "\n",
        "            pred_real = discriminator(ground_truth_batched)\n",
        "            loss_real = DISCRIMINATOR_LOSS_FUNCTION(pred_real, label_real)\n",
        "\n",
        "            pred_fake = discriminator(spliced_batched)\n",
        "            loss_fake = DISCRIMINATOR_LOSS_FUNCTION(pred_fake, label_fake)\n",
        "            loss_d = ALPHA * (loss_real + loss_fake)\n",
        "            loss_d.backward()\n",
        "            DISCRIMINATOR_OPTIMIZER.step()\n",
        "\n",
        "            # 2. train generator (reconstruction, adverserial)\n",
        "            # 2.1 forward pass by generator to produce images, splice them\n",
        "            output_batched, adj1, adj2 = generator(input_batched, return_adj = True)\n",
        "            spliced_batched = ((1-mask_batched) * output_batched) + (mask_batched * ground_truth_batched) \n",
        "            \n",
        "            # 2.2 reconstruction loss\n",
        "            loss_g_reconstruction = RECONSTRUCTION_LOSS(output_batched, ground_truth_batched, mask_batched)\n",
        "\n",
        "            # 2.3 adverserial loss\n",
        "            label_real = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "            pred_adverserial = discriminator(spliced_batched)\n",
        "            loss_g_adverserial = DISCRIMINATOR_LOSS_FUNCTION(pred_adverserial, label_real) # want it to classify all as real\n",
        "\n",
        "            # 2.5 backprop\n",
        "            loss_g = loss_g_reconstruction + ALPHA * loss_g_adverserial\n",
        "            loss_g.backward()\n",
        "            GENERATOR_OPTIMIZER.step()\n",
        "\n",
        "            # ===== COMPUTE STATISTICS, USING TORCH METRICS =====  \n",
        "            # 1. compute losses\n",
        "\n",
        "            running_results[\"loss_generator\"] += loss_g.detach().item()\n",
        "            running_results[\"loss_generator_r\"] += loss_g_reconstruction.detach().item()\n",
        "            running_results[\"loss_generator_adv\"] += loss_g_adverserial.detach().item()\n",
        "            running_results[\"loss_discriminator\"] += loss_d.detach().item()\n",
        "\n",
        "            # 2. for each key, compute, add item to results dictionary (take average of 2 generators)\n",
        "            for key, func in metrics.items():\n",
        "                running_results[key] += func(output_batched, ground_truth_batched, mask_batched).detach().item()\n",
        "\n",
        "            # 3. log with wandb\n",
        "            if log_wandb and (num_batches % LOG_EVERY == 0):\n",
        "\n",
        "                # generator 1\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "                \n",
        "\n",
        "                adj1_array = torchvision.utils.make_grid(adj1[:SAMPLE_SIZE].unsqueeze(1), nrow = SAMPLE_SIZE, padding = 50)\n",
        "                adj1_images = wandb.Image(\n",
        "                    adj1_array, \n",
        "                    caption = \"First adjacency tensor\")\n",
        "                adj2_array = torchvision.utils.make_grid(adj2[:SAMPLE_SIZE].unsqueeze(1), nrow = SAMPLE_SIZE, padding = 50)\n",
        "                adj2_images = wandb.Image(\n",
        "                    adj2_array, \n",
        "                    caption = \"Second adjacency tensor\")\n",
        "\n",
        "                # log images and some metadata\n",
        "                wandb.log( {\n",
        "                    \"generator_train_images\" : images,\n",
        "                    \"adj1_images\" : adj1_images,\n",
        "                    \"adj2_images\" : adj2_images,\n",
        "                    \"lr_generator\" : GENERATOR_OPTIMIZER.param_groups[0]['lr'],\n",
        "                    \"lr_discriminator\" : DISCRIMINATOR_OPTIMIZER.param_groups[0]['lr']\n",
        "\n",
        "                })\n",
        "\n",
        "                # log all metrics\n",
        "                wandb.log(\n",
        "                    {key : item/num_batches for key, item in running_results.items()}\n",
        "                )\n",
        "        \n",
        "            # ===== HOUSEKEEPING =====\n",
        "            del loss_g\n",
        "            del loss_d\n",
        "            del input_batched\n",
        "\n",
        "            # print results every some batches\n",
        "            if num_batches % BATCH_EVALUATE_EVERY == 0: \n",
        "\n",
        "                args = \"\"\n",
        "                for key, _ in running_results.items():\n",
        "                    args += key + \": \" + str(running_results[key]/num_batches) + \"   \"\n",
        "                print(f\"\\r{num_batches}/{len(train_dataloader)}: \" + args, end = '', flush = True)\n",
        "\n",
        "    # normalise numbers by batch\n",
        "    for key, _ in running_results.items():\n",
        "        running_results[key] /= num_batches\n",
        "\n",
        "    return running_results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4CuiHVAjKitr"
      },
      "outputs": [],
      "source": [
        "def evaluate_epoch(device, validation_dataloader, training_params : dict, metrics : dict, log_wandb = True):\n",
        "\n",
        "    # ===== INITIALIZE =====\n",
        "    # models\n",
        "    generator = training_params[\"generator_model\"].to(device).eval()\n",
        "\n",
        "    # constants\n",
        "    BATCH_SIZE = training_params[\"batch_size\"]\n",
        "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
        "\n",
        "    # epoch statistics\n",
        "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
        "\n",
        "    # ===== EVALUATE EPOCH =====\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batches = 0\n",
        "        for index, batch in enumerate(validation_dataloader, 1):\n",
        "            \n",
        "            batches += 1\n",
        "\n",
        "            # input and ground truth\n",
        "            input_batched = batch[\"image\"]\n",
        "            ground_truth_batched = batch[\"reconstructed\"]\n",
        "            mask_batched = batch[\"mask\"]\n",
        "\n",
        "            # move tensors to device\n",
        "            input_batched = input_batched.to(device)\n",
        "            ground_truth_batched = ground_truth_batched.to(device)\n",
        "            mask_batched = mask_batched.to(device)\n",
        "\n",
        "            # get shuffled indices\n",
        "            shuffled_indices = torch.randperm(n = BATCH_SIZE).long()\n",
        "\n",
        "            # ===== FORWARD PASS =====\n",
        "\n",
        "            # 1. reshape to channel first\n",
        "            input_batched = input_batched.permute(0, 3, 1, 2)\n",
        "            ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
        "            mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
        "\n",
        "            # 2. predict    \n",
        "            output_batched = generator(input_batched)\n",
        "            spliced_batched = ((1-mask_batched) * output_batched) + (mask_batched * ground_truth_batched) \n",
        "\n",
        "            # 3. evaluate\n",
        "            for key, func in metrics.items():\n",
        "                running_results[key] += func(output_batched, ground_truth_batched, mask_batched).detach().item()\n",
        "\n",
        "            args = \"\"\n",
        "            for key, _ in running_results.items():\n",
        "                args += key + \": \" + str(running_results[key]/batches) + \"   \"\n",
        "            print(f\"\\r{batches}/{len(validation_dataloader)}: \" + args, end = '', flush = True)\n",
        "\n",
        "            # 4. log \n",
        "            if log_wandb:\n",
        "\n",
        "                # generator 1\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "                \n",
        "                # log images and some metadata\n",
        "                wandb.log( {\n",
        "                    \"generator_val_images\" : images,\n",
        "                })\n",
        "\n",
        "                # log all metrics\n",
        "                wandb.log(\n",
        "                    {f\"val_{key}\" : item/batches for key, item in running_results.items()}\n",
        "                )\n",
        "\n",
        "    # normalise numbers by batch\n",
        "    for key, _ in running_results.items():\n",
        "        running_results[key] /= batches\n",
        "\n",
        "    return running_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "O_mQDSLXaPdz"
      },
      "outputs": [],
      "source": [
        "def train_evaluate(device, train_dataset, validation_dataset, training_params: dict, metrics: dict, start_epoch = 0, log_wandb = True):\n",
        "\n",
        "    # ===== INITIALIZE =====\n",
        "    # constants\n",
        "    NUM_EPOCHS = training_params[\"num_epochs\"]\n",
        "    BATCH_SIZE = training_params[\"batch_size\"]\n",
        "    GENERATOR_SCHEDULER = training_params[\"generator_scheduler\"]\n",
        "    DISCRIMINATOR_SCHEDULER = training_params[\"discriminator_scheduler\"]\n",
        "    SAVE_PATHS = training_params[\"save_paths\"]\n",
        "    NUM_WORKERS = 2\n",
        "    START_EPOCH = start_epoch\n",
        "\n",
        "    # models for saving\n",
        "    generator = training_params[\"generator_model\"].to(device).train()\n",
        "    discriminator = training_params[\"discriminator_model\"].to(device).train()\n",
        "\n",
        "    # variable losses\n",
        "    train_results = {list(metrics.keys())[i] : [] for i in range(len(metrics)) } \n",
        "\n",
        "    train_results[\"loss_discriminator\"] = []\n",
        "    train_results[\"loss_generator\"] = []\n",
        "    train_results[\"loss_generator_r\"] = []\n",
        "    train_results[\"loss_generator_adv\"] = []\n",
        "\n",
        "    eval_results = {list(metrics.keys())[i] : [] for i in range(len(metrics)) } \n",
        "\n",
        "    # dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS, drop_last = True)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS, drop_last = True)\n",
        "\n",
        "    # ===== TRAIN =====\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # train\n",
        "        print(f\"\\n===== Epoch: {START_EPOCH + epoch + 1} ===== \")\n",
        "\n",
        "        # train every epoch\n",
        "        results = train_epoch(device, train_dataloader, training_params, metrics, log_wandb = log_wandb)\n",
        "        for key, val in results.items():\n",
        "            train_results[key].append(val)\n",
        "\n",
        "        # evaluate every epoch\n",
        "        print()\n",
        "        results = evaluate_epoch(device, validation_dataloader, training_params, metrics, log_wandb = log_wandb)\n",
        "        for key, val in results.items():\n",
        "            eval_results[key].append(val)\n",
        "\n",
        "        # ===== EPOCH RESULTS =====\n",
        "        print(f\"\\nCompleted epoch {START_EPOCH + epoch + 1}! Took {(time.time() - start)/60} min\")\n",
        "\n",
        "        # ===== HOUSEKEEPING =====\n",
        "\n",
        "        # scheduler every epoch\n",
        "        if DISCRIMINATOR_SCHEDULER is not None:\n",
        "            DISCRIMINATOR_SCHEDULER.step(eval_results[\"L1 loss (Mask)\"][epoch])\n",
        "        if GENERATOR_SCHEDULER is not None:\n",
        "            GENERATOR_SCHEDULER.step(eval_results[\"L1 loss (Mask)\"][epoch])\n",
        "            \n",
        "        # save every epoch\n",
        "        SAVE = f\"{SAVE_PATHS['generator']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
        "        torch.save(generator.state_dict(), SAVE)\n",
        "        SAVE = f\"{SAVE_PATHS['discriminator']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
        "        torch.save(discriminator.state_dict(), SAVE)  \n",
        "\n",
        "        print(\"Saved models!\")\n",
        "\n",
        "    return train_results, eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8IkxVhsVsf4"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_IuN9J8Jfp9j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Epoch: 2 ===== \n",
            "155/156: Peak SnR (Whole): 28.055984817012664   L2 loss (Whole): 0.0020701938335062754   L2 loss (Mask): 0.17904267311096192   L1 loss (Whole): 0.006198990939845962   L1 loss (Mask): 0.5360986630762777   LPIPS (Whole): 0.06246266453977554   loss_discriminator: 0.13950272192878108   loss_generator: 0.24869074523448945   loss_generator_r: 0.17904267311096192   loss_generator_adv: 0.6964807148902646   \n",
            "15/15: Peak SnR (Whole): 30.26655476888021   L2 loss (Whole): 0.0009395812288857996   L2 loss (Mask): 0.08042539457480112   L1 loss (Whole): 0.004444854178776343   L1 loss (Mask): 0.380414217710495   LPIPS (Whole): 0.04974400848150253       \n",
            "Completed epoch 2! Took 3.1974259972572328 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 3 ===== \n",
            "155/156: Peak SnR (Whole): 30.817551631312217   L2 loss (Whole): 0.0008307834944477485   L2 loss (Mask): 0.07216023995030311   L1 loss (Whole): 0.004132188520123882   L1 loss (Mask): 0.3589296577438231   LPIPS (Whole): 0.04614444603362391   loss_discriminator: 0.13863477005112557   loss_generator: 0.14190563420134206   loss_generator_r: 0.07216023995030311   loss_generator_adv: 0.6974539283783205     \n",
            "15/15: Peak SnR (Whole): 31.199564615885418   L2 loss (Whole): 0.0007617805696402987   L2 loss (Mask): 0.0656755934158961   L1 loss (Whole): 0.0038886458923419317   L1 loss (Mask): 0.33512501120567323   LPIPS (Whole): 0.0428347905476888     \n",
            "Completed epoch 3! Took 2.8168620069821677 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 4 ===== \n",
            "155/156: Peak SnR (Whole): 31.3975954855642   L2 loss (Whole): 0.0007274731997430565   L2 loss (Mask): 0.06299012863347607   L1 loss (Whole): 0.0038348933621760338   L1 loss (Mask): 0.3321111986714025   LPIPS (Whole): 0.04255481523371512   loss_discriminator: 0.13831880957849563   loss_generator: 0.13293765749662154   loss_generator_r: 0.06299012863347607   loss_generator_adv: 0.699475271086539       \n",
            "15/15: Peak SnR (Whole): 31.67215830485026   L2 loss (Whole): 0.0006794711807742715   L2 loss (Mask): 0.05930307954549789   L1 loss (Whole): 0.0036600989600022635   L1 loss (Mask): 0.3193839649359385   LPIPS (Whole): 0.04002787594993909     \n",
            "Completed epoch 4! Took 2.8432283838589987 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 5 ===== \n",
            "155/156: Peak SnR (Whole): 31.7410890640751   L2 loss (Whole): 0.0006712216155363187   L2 loss (Mask): 0.05807612584002556   L1 loss (Whole): 0.0036753717239104934   L1 loss (Mask): 0.3180236360719127   LPIPS (Whole): 0.04002634226795166   loss_discriminator: 0.13773310328683547   loss_generator: 0.12858059324564472   loss_generator_r: 0.05807612584002556   loss_generator_adv: 0.7050446648751536       \n",
            "15/15: Peak SnR (Whole): 31.84864018758138   L2 loss (Whole): 0.0006523683822403352   L2 loss (Mask): 0.05640397220849991   L1 loss (Whole): 0.003619497238347928   L1 loss (Mask): 0.31293495297431945   LPIPS (Whole): 0.038735050708055496     \n",
            "Completed epoch 5! Took 2.9198861718177795 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 6 ===== \n",
            "155/156: Peak SnR (Whole): 32.125375157017864   L2 loss (Whole): 0.0006144505818449561   L2 loss (Mask): 0.05323595320505481   L1 loss (Whole): 0.0035007345009474985   L1 loss (Mask): 0.30331211628452426   LPIPS (Whole): 0.0379040778404282   loss_discriminator: 0.13668116340714118   loss_generator: 0.12468161621401387   loss_generator_r: 0.05323595320505481   loss_generator_adv: 0.7144566180244569      \n",
            "15/15: Peak SnR (Whole): 32.36276308695475   L2 loss (Whole): 0.0005796280573122203   L2 loss (Mask): 0.05040632908542951   L1 loss (Whole): 0.0033310139551758766   L1 loss (Mask): 0.28951255083084104   LPIPS (Whole): 0.036424472679694495    \n",
            "Completed epoch 6! Took 3.1678871075312296 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 7 ===== \n",
            "155/156: Peak SnR (Whole): 32.282525204074   L2 loss (Whole): 0.0005925468923974663   L2 loss (Mask): 0.05128082546495622   L1 loss (Whole): 0.003420321508160522   L1 loss (Mask): 0.2960212084554857   LPIPS (Whole): 0.03682949821795187   loss_discriminator: 0.13428799413865614   loss_generator: 0.12495337354560052   loss_generator_r: 0.05128082546495622   loss_generator_adv: 0.7367254684048314         \n",
            "15/15: Peak SnR (Whole): 32.33697230021159   L2 loss (Whole): 0.0005858062494856615   L2 loss (Mask): 0.05091608837246895   L1 loss (Whole): 0.0033761527544508376   L1 loss (Mask): 0.2932800710201263   LPIPS (Whole): 0.035918674618005755    \n",
            "Completed epoch 7! Took 3.3173712372779844 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 8 ===== \n",
            "155/156: Peak SnR (Whole): 32.39337627041724   L2 loss (Whole): 0.0005785147171287287   L2 loss (Mask): 0.05038564916579954   L1 loss (Whole): 0.0033747274593840682   L1 loss (Mask): 0.29384805338997994   LPIPS (Whole): 0.036268818799045775   loss_discriminator: 0.13047554771746359   loss_generator: 0.12721580454418735   loss_generator_r: 0.05038564916579954   loss_generator_adv: 0.7683015373445327   \n",
            "15/15: Peak SnR (Whole): 32.64622866312663   L2 loss (Whole): 0.0005448278175511708   L2 loss (Mask): 0.047905024886131284   L1 loss (Whole): 0.0032203523286928734   L1 loss (Mask): 0.28327778379122415   LPIPS (Whole): 0.035060956080754595   \n",
            "Completed epoch 8! Took 3.1488401055336 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 9 ===== \n",
            "155/156: Peak SnR (Whole): 32.45866199616463   L2 loss (Whole): 0.0005700106206800668   L2 loss (Mask): 0.04953682915337624   L1 loss (Whole): 0.003334219641082229   L1 loss (Mask): 0.2897519967248363   LPIPS (Whole): 0.03566287512019757   loss_discriminator: 0.12566747213563612   loss_generator: 0.1314091565147523   loss_generator_r: 0.04953682915337624   loss_generator_adv: 0.8187232563572545       \n",
            "15/15: Peak SnR (Whole): 32.45253995259603   L2 loss (Whole): 0.000568289787042886   L2 loss (Mask): 0.04892215952277183   L1 loss (Whole): 0.003279697305212418   L1 loss (Mask): 0.28231467604637145   LPIPS (Whole): 0.03445952807863553       \n",
            "Completed epoch 9! Took 3.2278204401334127 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 10 ===== \n",
            "155/156: Peak SnR (Whole): 32.51213750531596   L2 loss (Whole): 0.0005623890702883082   L2 loss (Mask): 0.048843339446090886   L1 loss (Whole): 0.0032924241025842006   L1 loss (Mask): 0.28596554744628166   LPIPS (Whole): 0.03431452506732556   loss_discriminator: 0.12178583813290442   loss_generator: 0.13456473975412309   loss_generator_r: 0.048843339446090886   loss_generator_adv: 0.8572139901499595   \n",
            "15/15: Peak SnR (Whole): 32.76176147460937   L2 loss (Whole): 0.0005303206872971107   L2 loss (Mask): 0.04546012878417969   L1 loss (Whole): 0.0031762319461752972   L1 loss (Mask): 0.27233012318611144   LPIPS (Whole): 0.032405341044068336   \n",
            "Completed epoch 10! Took 3.5131092071533203 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 11 ===== \n",
            "155/156: Peak SnR (Whole): 32.494673427458736   L2 loss (Whole): 0.0005655286103231652   L2 loss (Mask): 0.048988287895917894   L1 loss (Whole): 0.0033075810470167668   L1 loss (Mask): 0.2865837268290981   LPIPS (Whole): 0.03368039572190854   loss_discriminator: 0.12066555479841848   loss_generator: 0.13726488621004165   loss_generator_r: 0.048988287895917894   loss_generator_adv: 0.882765959539721   \n",
            "15/15: Peak SnR (Whole): 32.594757080078125   L2 loss (Whole): 0.0005510275387981286   L2 loss (Mask): 0.0482743298014005   L1 loss (Whole): 0.003179040364921093   L1 loss (Mask): 0.2786117851734161   LPIPS (Whole): 0.031061134363214175     \n",
            "Completed epoch 11! Took 3.2390385309855145 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 12 ===== \n",
            "155/156: Peak SnR (Whole): 32.465905404859974   L2 loss (Whole): 0.0005683315756370224   L2 loss (Mask): 0.04915768403199411   L1 loss (Whole): 0.003314325253989908   L1 loss (Mask): 0.2866700996314326   LPIPS (Whole): 0.03244644843282238   loss_discriminator: 0.12394671074805722   loss_generator: 0.1364055754676942   loss_generator_r: 0.04915768403199411   loss_generator_adv: 0.8724789027244814   5   \n",
            "15/15: Peak SnR (Whole): 32.84634602864583   L2 loss (Whole): 0.0005182244019427647   L2 loss (Mask): 0.04563573176662127   L1 loss (Whole): 0.0030838604861249527   L1 loss (Mask): 0.27136867145697274   LPIPS (Whole): 0.029827786361177763   \n",
            "Completed epoch 12! Took 3.2236502806345624 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 13 ===== \n",
            "155/156: Peak SnR (Whole): 32.51185062777611   L2 loss (Whole): 0.0005617893714037153   L2 loss (Mask): 0.0486439919519809   L1 loss (Whole): 0.003287925670344022   L1 loss (Mask): 0.2846823473130503   LPIPS (Whole): 0.03172314853197144   loss_discriminator: 0.12262744134472263   loss_generator: 0.13599860980626075   loss_generator_r: 0.0486439919519809   loss_generator_adv: 0.8735461577292412   37   \n",
            "15/15: Peak SnR (Whole): 32.61432088216146   L2 loss (Whole): 0.0005461006502931316   L2 loss (Mask): 0.047326243172089257   L1 loss (Whole): 0.003203925552467505   L1 loss (Mask): 0.27767711083094276   LPIPS (Whole): 0.0301947054763635      \n",
            "Completed epoch 13! Took 3.5217201630274455 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 14 ===== \n",
            "155/156: Peak SnR (Whole): 32.56098053224625   L2 loss (Whole): 0.0005563541022579997   L2 loss (Mask): 0.04818235921763605   L1 loss (Whole): 0.003268123240841012   L1 loss (Mask): 0.2830048951410478   LPIPS (Whole): 0.031037405613929994   loss_discriminator: 0.12171905064775097   loss_generator: 0.13658699383658748   loss_generator_r: 0.04818235921763605   loss_generator_adv: 0.8840463340282441      \n",
            "15/15: Peak SnR (Whole): 32.87697931925456   L2 loss (Whole): 0.0005177127701851229   L2 loss (Mask): 0.04505835250020027   L1 loss (Whole): 0.0030761081570138534   L1 loss (Mask): 0.26776242156823477   LPIPS (Whole): 0.029169411088029545    \n",
            "Completed epoch 14! Took 3.243354884783427 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 15 ===== \n",
            "155/156: Peak SnR (Whole): 32.67155247349893   L2 loss (Whole): 0.0005423866433932656   L2 loss (Mask): 0.0470267804880296   L1 loss (Whole): 0.003210559053226344   L1 loss (Mask): 0.27834233382055834   LPIPS (Whole): 0.029420247169271593   loss_discriminator: 0.1217210138997724   loss_generator: 0.1348823562745125   loss_generator_r: 0.0470267804880296   loss_generator_adv: 0.8785557489241324   2     \n",
            "15/15: Peak SnR (Whole): 32.76189219156901   L2 loss (Whole): 0.0005329519723697255   L2 loss (Mask): 0.046332452446222305   L1 loss (Whole): 0.0031608631058285632   L1 loss (Mask): 0.2749488403399785   LPIPS (Whole): 0.02818009542922179      \n",
            "Completed epoch 15! Took 3.2735411683718363 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 16 ===== \n",
            "155/156: Peak SnR (Whole): 32.68120344838788   L2 loss (Whole): 0.0005406233095263522   L2 loss (Mask): 0.04697291637139936   L1 loss (Whole): 0.003192759070905947   L1 loss (Mask): 0.27742866075808004   LPIPS (Whole): 0.027837491143615015   loss_discriminator: 0.12610800612357356   loss_generator: 0.13273797592809122   loss_generator_r: 0.04697291637139936   loss_generator_adv: 0.8576505835979216     \n",
            "15/15: Peak SnR (Whole): 32.97272491455078   L2 loss (Whole): 0.0005067459676259508   L2 loss (Mask): 0.04365086406469345   L1 loss (Whole): 0.0031124501023441554   L1 loss (Mask): 0.26814733147621156   LPIPS (Whole): 0.02611874205370744    \n",
            "Completed epoch 16! Took 3.499381641546885 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 17 ===== \n",
            "155/156: Peak SnR (Whole): 32.94413039915023   L2 loss (Whole): 0.0005093208756372933   L2 loss (Mask): 0.04410505655311769   L1 loss (Whole): 0.003089448318426167   L1 loss (Mask): 0.2675946177974824   LPIPS (Whole): 0.025906494860687565   loss_discriminator: 0.12366902001442448   loss_generator: 0.12885987090487633   loss_generator_r: 0.04410505655311769   loss_generator_adv: 0.8475481302507462       \n",
            "15/15: Peak SnR (Whole): 32.86914825439453   L2 loss (Whole): 0.0005169353253828983   L2 loss (Mask): 0.04470570410291354   L1 loss (Whole): 0.003124160040169954   L1 loss (Mask): 0.27013400197029114   LPIPS (Whole): 0.025773566216230392     \n",
            "Completed epoch 17! Took 3.2514816323916116 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 18 ===== \n",
            "155/156: Peak SnR (Whole): 32.88357847890546   L2 loss (Whole): 0.0005157586339215237   L2 loss (Mask): 0.04475472543028093   L1 loss (Whole): 0.0031032891418304176   L1 loss (Mask): 0.26926873605097496   LPIPS (Whole): 0.025652092851458058   loss_discriminator: 0.1260092304118218   loss_generator: 0.12887175193717404   loss_generator_r: 0.04475472543028093   loss_generator_adv: 0.8411702475240154   7   \n",
            "15/15: Peak SnR (Whole): 33.093665822347006   L2 loss (Whole): 0.0004898836797413727   L2 loss (Mask): 0.04232686807711919   L1 loss (Whole): 0.0029870683948198954   L1 loss (Mask): 0.2579779277245204   LPIPS (Whole): 0.02322517099479834     \n",
            "Completed epoch 18! Took 3.22983234723409 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 19 ===== \n",
            "155/156: Peak SnR (Whole): 32.92585553815288   L2 loss (Whole): 0.0005103256964620443   L2 loss (Mask): 0.04437819882746666   L1 loss (Whole): 0.0030839320918124528   L1 loss (Mask): 0.2682154285330926   LPIPS (Whole): 0.02512394087689538   loss_discriminator: 0.1276524897063932   loss_generator: 0.12884622672873158   loss_generator_r: 0.04437819882746666   loss_generator_adv: 0.8446802643037612       \n",
            "15/15: Peak SnR (Whole): 33.05096181233724   L2 loss (Whole): 0.0004988645649670313   L2 loss (Mask): 0.04381939669450124   L1 loss (Whole): 0.0030127396962294976   L1 loss (Mask): 0.2647392570972443   LPIPS (Whole): 0.023313938950498898   \n",
            "Completed epoch 19! Took 3.229475212097168 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 20 ===== \n",
            "155/156: Peak SnR (Whole): 32.99513089579921   L2 loss (Whole): 0.0005028896129524876   L2 loss (Mask): 0.043683442353240905   L1 loss (Whole): 0.003052565769382542   L1 loss (Mask): 0.2651724274120023   LPIPS (Whole): 0.024770176975477128   loss_discriminator: 0.12697927990267355   loss_generator: 0.12727814259067657   loss_generator_r: 0.043683442353240905   loss_generator_adv: 0.8359469865598986    \n",
            "15/15: Peak SnR (Whole): 32.8686710357666   L2 loss (Whole): 0.0005166411186413219   L2 loss (Mask): 0.044661596914132434   L1 loss (Whole): 0.0030625496835758287   L1 loss (Mask): 0.26477059423923494   LPIPS (Whole): 0.023403569186727206    \n",
            "Completed epoch 20! Took 3.6785907904307047 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 21 ===== \n",
            "155/156: Peak SnR (Whole): 33.049882298131145   L2 loss (Whole): 0.0004968695344418408   L2 loss (Mask): 0.04307978480092941   L1 loss (Whole): 0.003025448048907903   L1 loss (Mask): 0.26237930386297165   LPIPS (Whole): 0.024047588136407637   loss_discriminator: 0.1254108216012678   loss_generator: 0.12630744513965422   loss_generator_r: 0.04307978480092941   loss_generator_adv: 0.8322765842560799     \n",
            "15/15: Peak SnR (Whole): 33.26997706095378   L2 loss (Whole): 0.0004730580277585735   L2 loss (Mask): 0.041410653541485466   L1 loss (Whole): 0.0029013213391105336   L1 loss (Mask): 0.2539211322863897   LPIPS (Whole): 0.023086794838309287    \n",
            "Completed epoch 21! Took 3.2158344944318134 min\n",
            "Saved models!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'Peak SnR (Whole)': [28.07500068957989,\n",
              "   30.821627494616386,\n",
              "   31.392894989404923,\n",
              "   31.745217103224533,\n",
              "   32.130826791127525,\n",
              "   32.27814537439591,\n",
              "   32.39146749789898,\n",
              "   32.456015268961586,\n",
              "   32.509153928512184,\n",
              "   32.49733567849184,\n",
              "   32.46839534319364,\n",
              "   32.51263206433027,\n",
              "   32.55354224718534,\n",
              "   32.67888284340883,\n",
              "   32.678306934161064,\n",
              "   32.94070404003828,\n",
              "   32.88340188295413,\n",
              "   32.925433488992546,\n",
              "   32.99322113623986,\n",
              "   33.0540063442328],\n",
              "  'L2 loss (Whole)': [0.0020619492853979748,\n",
              "   0.0008300450423027938,\n",
              "   0.0007282678714732472,\n",
              "   0.0006705913651668921,\n",
              "   0.0006137424652712444,\n",
              "   0.0005931493506068364,\n",
              "   0.0005787628216263002,\n",
              "   0.0005703275718797857,\n",
              "   0.0005627853496638963,\n",
              "   0.000565157773216458,\n",
              "   0.0005679847377812108,\n",
              "   0.0005616562188011953,\n",
              "   0.0005573945523699364,\n",
              "   0.0005415519303940714,\n",
              "   0.000540967334437972,\n",
              "   0.0005096790460251773,\n",
              "   0.0005157735164781722,\n",
              "   0.0005103463852906432,\n",
              "   0.0005030837206644579,\n",
              "   0.0004964017811849212],\n",
              "  'L2 loss (Mask)': [0.1783416802302385,\n",
              "   0.07211812055454804,\n",
              "   0.06305249644300112,\n",
              "   0.05802094155492691,\n",
              "   0.053178130983351134,\n",
              "   0.051335121027361125,\n",
              "   0.05041024504372707,\n",
              "   0.04956080064846155,\n",
              "   0.04888476789570772,\n",
              "   0.0489600265923983,\n",
              "   0.04912495916374983,\n",
              "   0.048635619907425,\n",
              "   0.04828167198082575,\n",
              "   0.046957333357288286,\n",
              "   0.04699759729779684,\n",
              "   0.04412635239079977,\n",
              "   0.04475623746521962,\n",
              "   0.0443793520426903,\n",
              "   0.043689969879312396,\n",
              "   0.04304636038171175],\n",
              "  'L1 loss (Whole)': [0.006184821255887166,\n",
              "   0.004129517572717025,\n",
              "   0.00383696705998423,\n",
              "   0.0036743818976295493,\n",
              "   0.003498351105894798,\n",
              "   0.003422222264373723,\n",
              "   0.0033750239168651975,\n",
              "   0.0033349301069019698,\n",
              "   0.0032930590857106904,\n",
              "   0.0033060067769092247,\n",
              "   0.003313119220248877,\n",
              "   0.0032877616811161623,\n",
              "   0.0032715202094270633,\n",
              "   0.0032077015255195782,\n",
              "   0.0031939194260093453,\n",
              "   0.003091147842590148,\n",
              "   0.0031028958291221312,\n",
              "   0.0030842263606759026,\n",
              "   0.003053257514973386,\n",
              "   0.003023636921380575],\n",
              "  'L1 loss (Mask)': [0.5349346298055772,\n",
              "   0.35881190326733464,\n",
              "   0.33225893191038036,\n",
              "   0.3179338553395027,\n",
              "   0.3031274722172664,\n",
              "   0.29619728047878313,\n",
              "   0.2938909002412588,\n",
              "   0.2897939234972,\n",
              "   0.28605866756958837,\n",
              "   0.28647021376169646,\n",
              "   0.28654922611820394,\n",
              "   0.28468725849420595,\n",
              "   0.2833479094581726,\n",
              "   0.27811435399911344,\n",
              "   0.2774997690740304,\n",
              "   0.2676837045030716,\n",
              "   0.26923601606335396,\n",
              "   0.26823685748072773,\n",
              "   0.26517118618656427,\n",
              "   0.2622679954346938],\n",
              "  'LPIPS (Whole)': [0.062395269027314126,\n",
              "   0.046137353572516866,\n",
              "   0.04256857308344199,\n",
              "   0.040031203331473544,\n",
              "   0.03788346322014546,\n",
              "   0.03682344516691489,\n",
              "   0.03626265549936738,\n",
              "   0.035637016324565195,\n",
              "   0.034311646750817694,\n",
              "   0.033670075297451176,\n",
              "   0.03243695275905805,\n",
              "   0.031701330155229725,\n",
              "   0.031051725029754333,\n",
              "   0.029403244336255085,\n",
              "   0.027846237560972955,\n",
              "   0.025911074155607283,\n",
              "   0.0256401160493111,\n",
              "   0.025119937944393128,\n",
              "   0.024780866594459765,\n",
              "   0.024023282664039962],\n",
              "  'loss_discriminator': [0.1394971781052076,\n",
              "   0.13863395173580217,\n",
              "   0.13831621503982788,\n",
              "   0.13772785673156762,\n",
              "   0.13667715703829741,\n",
              "   0.13428437528319848,\n",
              "   0.1304669321920627,\n",
              "   0.1257107017131952,\n",
              "   0.1217439537629103,\n",
              "   0.12076756401130787,\n",
              "   0.12395159818996222,\n",
              "   0.12259108869311137,\n",
              "   0.1216880929393646,\n",
              "   0.12172445038763377,\n",
              "   0.12606156789339507,\n",
              "   0.12366406777157234,\n",
              "   0.12594446597191003,\n",
              "   0.12766283898590466,\n",
              "   0.12697803343717867,\n",
              "   0.1254627151080431],\n",
              "  'loss_generator': [0.24798667650574294,\n",
              "   0.1418700425957258,\n",
              "   0.13299213125346562,\n",
              "   0.12854374854419476,\n",
              "   0.12459413635616119,\n",
              "   0.1250094801474076,\n",
              "   0.1272308357919638,\n",
              "   0.13162434521393898,\n",
              "   0.13454487623694616,\n",
              "   0.13730587046115827,\n",
              "   0.13628109582723716,\n",
              "   0.1359679386115227,\n",
              "   0.13650578919511575,\n",
              "   0.13465192637000328,\n",
              "   0.13276794008337534,\n",
              "   0.1290643979819157,\n",
              "   0.1288624917371915,\n",
              "   0.12871469896382245,\n",
              "   0.1271039231751974,\n",
              "   0.1262879140006426],\n",
              "  'loss_generator_r': [0.1783416802302385,\n",
              "   0.07211812055454804,\n",
              "   0.06305249644300112,\n",
              "   0.05802094155492691,\n",
              "   0.053178130983351134,\n",
              "   0.051335121027361125,\n",
              "   0.05041024504372707,\n",
              "   0.04956080064846155,\n",
              "   0.04888476789570772,\n",
              "   0.0489600265923983,\n",
              "   0.04912495916374983,\n",
              "   0.048635619907425,\n",
              "   0.04828167198082575,\n",
              "   0.046957333357288286,\n",
              "   0.04699759729779684,\n",
              "   0.04412635239079977,\n",
              "   0.04475623746521962,\n",
              "   0.0443793520426903,\n",
              "   0.043689969879312396,\n",
              "   0.04304636038171175],\n",
              "  'loss_generator_adv': [0.6964499568327879,\n",
              "   0.697519206465819,\n",
              "   0.6993963309587576,\n",
              "   0.7052280604839325,\n",
              "   0.7141600417403074,\n",
              "   0.7367435789261109,\n",
              "   0.7682058910528818,\n",
              "   0.8206354287954477,\n",
              "   0.8566010701350677,\n",
              "   0.8834584152851349,\n",
              "   0.8715613545515598,\n",
              "   0.8733231658354784,\n",
              "   0.882241159868546,\n",
              "   0.8769459212437655,\n",
              "   0.8577034162023128,\n",
              "   0.8493804427293631,\n",
              "   0.8410625255260712,\n",
              "   0.8433534545012009,\n",
              "   0.8341395172935265,\n",
              "   0.8324155173240564]},\n",
              " {'Peak SnR (Whole)': [30.26655476888021,\n",
              "   31.199564615885418,\n",
              "   31.67215830485026,\n",
              "   31.84864018758138,\n",
              "   32.36276308695475,\n",
              "   32.33697230021159,\n",
              "   32.64622866312663,\n",
              "   32.45253995259603,\n",
              "   32.76176147460937,\n",
              "   32.594757080078125,\n",
              "   32.84634602864583,\n",
              "   32.61432088216146,\n",
              "   32.87697931925456,\n",
              "   32.76189219156901,\n",
              "   32.97272491455078,\n",
              "   32.86914825439453,\n",
              "   33.093665822347006,\n",
              "   33.05096181233724,\n",
              "   32.8686710357666,\n",
              "   33.26997706095378],\n",
              "  'L2 loss (Whole)': [0.0009395812288857996,\n",
              "   0.0007617805696402987,\n",
              "   0.0006794711807742715,\n",
              "   0.0006523683822403352,\n",
              "   0.0005796280573122203,\n",
              "   0.0005858062494856615,\n",
              "   0.0005448278175511708,\n",
              "   0.000568289787042886,\n",
              "   0.0005303206872971107,\n",
              "   0.0005510275387981286,\n",
              "   0.0005182244019427647,\n",
              "   0.0005461006502931316,\n",
              "   0.0005177127701851229,\n",
              "   0.0005329519723697255,\n",
              "   0.0005067459676259508,\n",
              "   0.0005169353253828983,\n",
              "   0.0004898836797413727,\n",
              "   0.0004988645649670313,\n",
              "   0.0005166411186413219,\n",
              "   0.0004730580277585735],\n",
              "  'L2 loss (Mask)': [0.08042539457480112,\n",
              "   0.0656755934158961,\n",
              "   0.05930307954549789,\n",
              "   0.05640397220849991,\n",
              "   0.05040632908542951,\n",
              "   0.05091608837246895,\n",
              "   0.047905024886131284,\n",
              "   0.04892215952277183,\n",
              "   0.04546012878417969,\n",
              "   0.0482743298014005,\n",
              "   0.04563573176662127,\n",
              "   0.047326243172089257,\n",
              "   0.04505835250020027,\n",
              "   0.046332452446222305,\n",
              "   0.04365086406469345,\n",
              "   0.04470570410291354,\n",
              "   0.04232686807711919,\n",
              "   0.04381939669450124,\n",
              "   0.044661596914132434,\n",
              "   0.041410653541485466],\n",
              "  'L1 loss (Whole)': [0.004444854178776343,\n",
              "   0.0038886458923419317,\n",
              "   0.0036600989600022635,\n",
              "   0.003619497238347928,\n",
              "   0.0033310139551758766,\n",
              "   0.0033761527544508376,\n",
              "   0.0032203523286928734,\n",
              "   0.003279697305212418,\n",
              "   0.0031762319461752972,\n",
              "   0.003179040364921093,\n",
              "   0.0030838604861249527,\n",
              "   0.003203925552467505,\n",
              "   0.0030761081570138534,\n",
              "   0.0031608631058285632,\n",
              "   0.0031124501023441554,\n",
              "   0.003124160040169954,\n",
              "   0.0029870683948198954,\n",
              "   0.0030127396962294976,\n",
              "   0.0030625496835758287,\n",
              "   0.0029013213391105336],\n",
              "  'L1 loss (Mask)': [0.380414217710495,\n",
              "   0.33512501120567323,\n",
              "   0.3193839649359385,\n",
              "   0.31293495297431945,\n",
              "   0.28951255083084104,\n",
              "   0.2932800710201263,\n",
              "   0.28327778379122415,\n",
              "   0.28231467604637145,\n",
              "   0.27233012318611144,\n",
              "   0.2786117851734161,\n",
              "   0.27136867145697274,\n",
              "   0.27767711083094276,\n",
              "   0.26776242156823477,\n",
              "   0.2749488403399785,\n",
              "   0.26814733147621156,\n",
              "   0.27013400197029114,\n",
              "   0.2579779277245204,\n",
              "   0.2647392570972443,\n",
              "   0.26477059423923494,\n",
              "   0.2539211322863897],\n",
              "  'LPIPS (Whole)': [0.04974400848150253,\n",
              "   0.0428347905476888,\n",
              "   0.04002787594993909,\n",
              "   0.038735050708055496,\n",
              "   0.036424472679694495,\n",
              "   0.035918674618005755,\n",
              "   0.035060956080754595,\n",
              "   0.03445952807863553,\n",
              "   0.032405341044068336,\n",
              "   0.031061134363214175,\n",
              "   0.029827786361177763,\n",
              "   0.0301947054763635,\n",
              "   0.029169411088029545,\n",
              "   0.02818009542922179,\n",
              "   0.02611874205370744,\n",
              "   0.025773566216230392,\n",
              "   0.02322517099479834,\n",
              "   0.023313938950498898,\n",
              "   0.023403569186727206,\n",
              "   0.023086794838309287]})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "train_evaluate(device, train_dataset, valid_dataset, TRAINING_PARAMS, METRICS, start_epoch = 1, log_wandb = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OrCk_e1oU44V"
      },
      "outputs": [],
      "source": [
        "# train_epoch(device, DataLoader(train_dataset, batch_size = 16), TRAINING_PARAMS, METRICS, log_wandb = True)\n",
        "# evaluate_epoch(device, DataLoader(valid_dataset, batch_size = 16),, TRAINING_PARAMS, metrics, log_wandb = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "fdEKy-uZU6fu",
        "outputId": "87ad1772-6436-4fb6-80df-2db4b44cd03c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Epoch: 21 ===== \n",
            "155/156: Peak SnR (Whole): 33.044430874240014   L2 loss (Whole): 0.0004969713832026408   L2 loss (Mask): 0.04303628201446225   L1 loss (Whole): 0.0030289917478277797   L1 loss (Mask): 0.2623311996459961   LPIPS (Whole): 0.023857228241620527   loss_discriminator: 0.12609016582850488   loss_generator: 0.1259187826706517   loss_generator_r: 0.04303628201446225   loss_generator_adv: 0.8288249909877777     \n",
            "15/15: Peak SnR (Whole): 33.07879053751628   L2 loss (Whole): 0.0004918274624894063   L2 loss (Mask): 0.04238450676202774   L1 loss (Whole): 0.002989047688121597   L1 loss (Mask): 0.2576209048430125   LPIPS (Whole): 0.023574770117799442     \n",
            "Completed epoch 21! Took 3.2155988057454428 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 22 ===== \n",
            "155/156: Peak SnR (Whole): 33.108302528627455   L2 loss (Whole): 0.0004896123054242062   L2 loss (Mask): 0.04246975193100591   L1 loss (Whole): 0.003008432118522544   L1 loss (Mask): 0.26096243627609744   LPIPS (Whole): 0.023976146838357373   loss_discriminator: 0.1262180923454223   loss_generator: 0.12556050251568518   loss_generator_r: 0.04246975193100591   loss_generator_adv: 0.8309074882538088     \n",
            "15/15: Peak SnR (Whole): 33.15050303141276   L2 loss (Whole): 0.0004840130122223248   L2 loss (Mask): 0.04198827669024467   L1 loss (Whole): 0.0029539079405367374   L1 loss (Mask): 0.2561788260936737   LPIPS (Whole): 0.02311796707411607      \n",
            "Completed epoch 22! Took 3.4122005621592204 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 23 ===== \n",
            "140/156: Peak SnR (Whole): 33.219635827200754   L2 loss (Whole): 0.0004771276410402996   L2 loss (Mask): 0.04128404105348246   L1 loss (Whole): 0.002967238953403596   L1 loss (Mask): 0.2567484047796045   LPIPS (Whole): 0.023405064549297094   loss_discriminator: 0.12717256343790462   loss_generator: 0.12408758950020586   loss_generator_r: 0.04128404105348246   loss_generator_adv: 0.8280354714819363      "
          ]
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "train_evaluate(device, train_dataset, valid_dataset, TRAINING_PARAMS, METRICS, start_epoch = 20, log_wandb = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ContrastiveGAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
