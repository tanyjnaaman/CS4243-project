{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H98D0BHAEioA"
      },
      "source": [
        "# Initialize notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lpips\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchmetrics\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/e/e0425222/miniconda3/envs/env/lib/python3.7/site-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 8.1.2. Several security issues (CVE-2021-27921, CVE-2021-25290, CVE-2021-25291, CVE-2021-25293, and more) have been fixed in pillow 8.1.2 or higher. We recommend to upgrade this library.\n",
            "  from .collection import imread_collection_wrapper\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"/home/e/e0425222/CS4243-project\")\n",
        "from utils.dataset_utils.AnimalDataset import AnimalDataset\n",
        "from utils.train_utils.train_utils import visualize_results, sample_batch, summary\n",
        "from utils.train_utils.model_utils import Conv2dBlock, GatedConv2dBlock, GatedUpConv2dBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pKzaTBy__baX"
      },
      "outputs": [],
      "source": [
        "train_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_train.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n",
        "\n",
        "valid_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_val.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n",
        "\n",
        "test_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_test.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # sanity check \n",
        "# sample_batch(train_dataset, sample_size = 6)\n",
        "# sample_batch(valid_dataset, sample_size = 6)\n",
        "# sample_batch(test_dataset, sample_size = 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouUSbxVkE92G"
      },
      "source": [
        "# Introduction\n",
        "This notebook aims to explore how contrastive learning can improve GAN performance for image inpainting. References are: \n",
        "1. Siamese Neural Networks for One-shot Image Recognition\n",
        "2. SiGAN: Siamese Generative Adversarial Network for Identity-Preserving Face Hallucination\n",
        "3. Free-Form Image Inpainting via Contrastive Attention Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oL0U4VbFaeJ"
      },
      "source": [
        "# Model experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h2hEL0kCFbt_"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, activation):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        # same -> downsample -> same -> downsample\n",
        "        self.conv0 = GatedConv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv1 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv2 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv3 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # 2 x same conv\n",
        "        self.conv4 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv5 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # 4 x dilated conv\n",
        "        self.conv6 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 2, dilation = 2, activation = activation)\n",
        "        self.conv7 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 4, dilation = 4, activation = activation)\n",
        "        self.conv8 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 8, dilation = 8, activation = activation)\n",
        "        self.conv9 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 16, dilation = 16, activation = activation)\n",
        "\n",
        "        # 2 x same conv\n",
        "        self.conv10 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv11 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # upsample -> same -> upsample -> same\n",
        "        self.conv12 = GatedUpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
        "        self.conv13 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv14 = GatedUpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
        "        self.conv15 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # final\n",
        "        self.final = nn.Conv2d(hidden_dim, output_dim, kernel_size = 3, stride = 1, padding = 'same')\n",
        "\n",
        "        # for contrastive learning, 1x1 conv to compress feature map into 1 channel\n",
        "        self.conv_feature = nn.Conv2d(hidden_dim, 1, kernel_size = 1, stride = 1, padding = 'same')\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "\n",
        "        x = self.conv0(input_tensor)\n",
        "        \n",
        "        # downsample\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # middle layers\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        # dilated conv with residual skips\n",
        "        x = self.conv6(x) + x\n",
        "        x = self.conv7(x) + x\n",
        "        x = self.conv8(x) + x\n",
        "        x = self.conv9(x) + x\n",
        "\n",
        "        # extract for contrastive loss\n",
        "        x_feature = self.conv_feature(x)\n",
        "\n",
        "        # middle layers\n",
        "        x = self.conv10(x)\n",
        "        x = self.conv11(x)\n",
        "\n",
        "        # upsample\n",
        "        x = self.conv12(x)\n",
        "        x = self.conv13(x)\n",
        "        x = self.conv14(x)\n",
        "        x = self.conv15(x)\n",
        "        \n",
        "        # final\n",
        "        x = self.final(x)\n",
        "\n",
        "        return x, x_feature\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, input_shape, activation):\n",
        "\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # 5 layers down\n",
        "        self.conv0 = Conv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv1 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv2 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv3 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv4 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # linear to predict classes\n",
        "        LATENT_H = input_shape//(2**5)\n",
        "        self.linear = nn.Linear(LATENT_H**2 * hidden_dim, 1)\n",
        "\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \n",
        "        x = self.conv0(input_tensor)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        # scores\n",
        "        x = torch.flatten(x, start_dim = 1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, input_shape, activation):\n",
        "\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # 5 layers down\n",
        "        self.conv0 = Conv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv1 = Conv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        self.conv2 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv3 = Conv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        self.conv4 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv5 = Conv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        self.conv6 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv7 = Conv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        self.conv8 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv9 = Conv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # linear to predict classes\n",
        "        LATENT_H = input_shape//(2**5)\n",
        "        self.linear = nn.Linear(LATENT_H**2 * hidden_dim, 1)\n",
        "\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \n",
        "        x = self.conv0(input_tensor)\n",
        "        x = self.conv1(x) + x\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x) + x\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x) + x\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7(x) + x\n",
        "        x = self.conv8(x)\n",
        "        x = self.conv9(x) + x\n",
        "        \n",
        "        # scores\n",
        "        x = torch.flatten(x, start_dim = 1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSjQKMf6EmH2"
      },
      "source": [
        "# Training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K26yI7ABEVmI"
      },
      "outputs": [],
      "source": [
        "MODEL_PARAMS = {\n",
        "    \"description\" : \"\"\"\n",
        "        Base structure same as the GLCIC model. Gated convolutions replace all convolutions in the generator, \n",
        "        and a 1x1 convolution is used to flatten the output of the 4th dilated convolution layer. \n",
        "        This is the latent vector used for contrastive learning.\"\"\",\n",
        "    \"hidden_dim\" : 64,\n",
        "    \"activation\" : nn.Mish,\n",
        "}\n",
        "\n",
        "SAVE_PATHS = {\n",
        "    \"generator1\" : \"/home/e/e0425222/CS4243-project/active_experiments/ContrastiveGAN/generator1/generator1\",\n",
        "    \"generator2\" : \"/home/e/e0425222/CS4243-project/active_experiments/ContrastiveGAN/generator2/generator2\",\n",
        "    \"discriminator\" : \"/home/e/e0425222/CS4243-project/active_experiments/ContrastiveGAN/discriminator/discriminator\"\n",
        "}\n",
        "\n",
        "TRAINING_PARAMS = {\n",
        "    \"num_epochs\" : 20,\n",
        "    \"batch_size\" : 128, \n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"alpha\": 0.5,\n",
        "    \"beta\" : 0.01,\n",
        "    \"ADAM_betas\" : (0.5, 0.999),\n",
        "    \"schedule_every\" : 1,\n",
        "    \"sample_size\" : 16,\n",
        "    \"log_every\" : 10,\n",
        "    \"save_paths\" : SAVE_PATHS,\n",
        "}\n",
        "\n",
        "LOGGING_CONFIG = {\n",
        "    \"model_params\" : MODEL_PARAMS,\n",
        "}\n",
        "LOGGING_CONFIG.update(TRAINING_PARAMS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UEXbNHjGvaE"
      },
      "source": [
        "# Experiment intialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbIsqCWTGvGE",
        "outputId": "a1df7c8d-a350-4914-ce06-e4b0da2b8599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model has 1.11642 million parameters\n",
            "model has 1.11642 million parameters\n",
            "model has 0.150401 million parameters\n",
            "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
            "Loading model from: /home/e/e0425222/miniconda3/envs/env/lib/python3.7/site-packages/lpips/weights/v0.1/vgg.pth\n"
          ]
        }
      ],
      "source": [
        "# 1. initialize model\n",
        "generator1 = Generator(input_dim = 4, hidden_dim = MODEL_PARAMS[\"hidden_dim\"], output_dim = 3, activation = MODEL_PARAMS[\"activation\"])\n",
        "generator2 = Generator(input_dim = 4, hidden_dim = MODEL_PARAMS[\"hidden_dim\"], output_dim = 3, activation = MODEL_PARAMS[\"activation\"])\n",
        "discriminator = Discriminator(input_dim = 3, hidden_dim = MODEL_PARAMS[\"hidden_dim\"], input_shape = 64, activation = MODEL_PARAMS[\"activation\"])\n",
        "\n",
        "summary(generator1)\n",
        "summary(generator2)\n",
        "summary(discriminator)\n",
        "\n",
        "# 2. device\n",
        "parallel = True\n",
        "device = 'cuda:0'  \n",
        "devices = [0,1,2,3]\n",
        "\n",
        "if not parallel:\n",
        "    generator1 = generator1.to(device)\n",
        "    generator2 = generator2.to(device)\n",
        "    discriminator = discriminator.to(device)\n",
        "else:\n",
        "    generator1 = nn.DataParallel(generator1, device_ids = devices)\n",
        "    generator2 = nn.DataParallel(generator2, device_ids = devices)\n",
        "    discriminator = nn.DataParallel(discriminator, device_ids = devices)\n",
        "\n",
        "# 3. initialize loss functions\n",
        "recon_loss_function = lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum()\n",
        "contrastive_loss_function_same = lambda x1, x2 : nn.functional.mse_loss(x1, x2)\n",
        "discriminator_loss_function = nn.functional.binary_cross_entropy_with_logits\n",
        "\n",
        "# 4. initialize metrics\n",
        "VGG_LPIPS = lpips.LPIPS(net = 'vgg').to(device)\n",
        "METRICS = {\n",
        "    \"Peak SnR (Whole)\" : lambda img, gt, mask : torchmetrics.functional.peak_signal_noise_ratio(img * (1-mask) + gt * mask, gt),\n",
        "    \"L2 loss (Whole)\" : lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask) + gt * mask, gt),\n",
        "    \"L2 loss (Mask)\" : lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum(),\n",
        "    \"L1 loss (Whole)\" : lambda img, gt, mask : nn.functional.l1_loss(img * (1-mask) + gt * mask, gt),\n",
        "    \"L1 loss (Mask)\" : lambda img, gt, mask : nn.functional.l1_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum(),\n",
        "    \"LPIPS (Whole)\" : (lambda img, gt, mask : VGG_LPIPS(img * (1-mask) + gt * mask, gt).mean()),\n",
        "}\n",
        "\n",
        "\n",
        "# 5. initialize optimizers\n",
        "generator1_optimizer = torch.optim.Adam(generator1.parameters(), lr = TRAINING_PARAMS[\"learning_rate\"], betas = TRAINING_PARAMS[\"ADAM_betas\"])\n",
        "generator1_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(generator1_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
        "\n",
        "generator2_optimizer = torch.optim.Adam(generator2.parameters(), lr = TRAINING_PARAMS[\"learning_rate\"], betas = TRAINING_PARAMS[\"ADAM_betas\"])\n",
        "generator2_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(generator2_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
        "\n",
        "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr = TRAINING_PARAMS[\"learning_rate\"], betas = TRAINING_PARAMS[\"ADAM_betas\"])\n",
        "discriminator_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(discriminator_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
        "\n",
        "# 6. wrap into training dictionary\n",
        "TRAINING_PARAMS[\"generator1_model\"] = generator1\n",
        "TRAINING_PARAMS[\"generator1_optimizer\"] = generator1_optimizer\n",
        "TRAINING_PARAMS[\"generator1_scheduler\"] = generator1_scheduler\n",
        "TRAINING_PARAMS[\"generator2_model\"] = generator2\n",
        "TRAINING_PARAMS[\"generator2_optimizer\"] = generator2_optimizer\n",
        "TRAINING_PARAMS[\"generator2_scheduler\"] = generator2_scheduler\n",
        "TRAINING_PARAMS[\"discriminator_model\"] = discriminator\n",
        "TRAINING_PARAMS[\"discriminator_optimizer\"] = discriminator_optimizer\n",
        "TRAINING_PARAMS[\"discriminator_scheduler\"] = discriminator_scheduler\n",
        "TRAINING_PARAMS[\"discriminator_loss_function\"] = discriminator_loss_function\n",
        "TRAINING_PARAMS[\"contrastive_loss_function\"] = contrastive_loss_function_same\n",
        "TRAINING_PARAMS[\"recon_loss_function\"] = recon_loss_function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intialize logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "6rbEmO4xoVR4",
        "outputId": "75c4f786-f5b7-42de-b20d-5e6f78eaaccf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanyjnaaman\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/e/e0425222/CS4243-project/active_experiments/ContrastiveGAN/wandb/run-20220412_184937-20uebkim</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs4243_project/ContrastiveGAN/runs/20uebkim\" target=\"_blank\">gallant-dawn-29</a></strong> to <a href=\"https://wandb.ai/cs4243_project/ContrastiveGAN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(project=\"ContrastiveGAN\", entity=\"cs4243_project\")\n",
        "wandb.config = LOGGING_CONFIG\n",
        "\n",
        "wandb.watch(\n",
        "    (generator1, generator2, discriminator),\n",
        "    criterion = None,\n",
        "    log = 'all',\n",
        "    log_freq = 1,\n",
        "    idx = 0, \n",
        "    log_graph = False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsM_LVhrKiE8"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIMfSPuZaJpQ"
      },
      "source": [
        "## Train functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QFJP7gjJSGFH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_epoch(device, train_dataloader, training_params : dict, metrics : dict, log_wandb = True):\n",
        "    \n",
        "    # ===== INITIALIZE =====\n",
        "    # constants\n",
        "    RECONSTRUCTION_LOSS = training_params[\"recon_loss_function\"]\n",
        "    DISCRIMINATOR_LOSS_FUNCTION = training_params[\"discriminator_loss_function\"]\n",
        "    CONTRASTIVE_LOSS_FUNCTION = training_params[\"contrastive_loss_function\"]\n",
        "\n",
        "    GENERATOR1_OPTIMIZER = training_params[\"generator1_optimizer\"]\n",
        "    GENERATOR2_OPTIMIZER = training_params[\"generator2_optimizer\"]\n",
        "    DISCRIMINATOR_OPTIMIZER = training_params[\"discriminator_optimizer\"]\n",
        "\n",
        "    BATCH_EVALUATE_EVERY = 5\n",
        "    LOG_EVERY = training_params[\"log_every\"]\n",
        "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
        "    BATCH_SIZE = training_params[\"batch_size\"]\n",
        "    ALPHA = training_params[\"alpha\"]\n",
        "    BETA = training_params[\"beta\"]\n",
        "\n",
        "    # models\n",
        "    generator1 = training_params[\"generator1_model\"].to(device).train()\n",
        "    generator2 = training_params[\"generator2_model\"].to(device).train()\n",
        "    discriminator = training_params[\"discriminator_model\"].to(device).train()\n",
        "\n",
        "    # epoch metrics\n",
        "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
        "    running_results[\"loss_discriminator\"] = 0.0\n",
        "    running_results[\"loss_generator1\"] = 0.0\n",
        "    running_results[\"loss_generator1_r\"] = 0.0\n",
        "    running_results[\"loss_generator1_adv\"] = 0.0\n",
        "    running_results[\"loss_generator2\"] = 0.0\n",
        "    running_results[\"loss_generator2_r\"] = 0.0\n",
        "    running_results[\"loss_generator2_adv\"] = 0.0\n",
        "    running_results[\"loss_contrastive\"] = 0.0\n",
        "\n",
        "    # ===== TRAIN EPOCH =====\n",
        "    num_batches = 0\n",
        "    for _, batch in enumerate(train_dataloader, 1):\n",
        "\n",
        "            # ===== INITIALIZE =====\n",
        "            num_batches += 1\n",
        "\n",
        "            # input and ground truth\n",
        "            input_batched = batch[\"image\"]\n",
        "            ground_truth_batched = batch[\"reconstructed\"]\n",
        "            mask_batched = batch[\"mask\"]\n",
        "\n",
        "            # sanity check\n",
        "            assert input_batched.shape[0] == ground_truth_batched.shape[0]\n",
        "\n",
        "            # move tensors to device\n",
        "            input_batched = input_batched.to(device)\n",
        "            ground_truth_batched = ground_truth_batched.to(device)\n",
        "            mask_batched = mask_batched.to(device)\n",
        "\n",
        "            # set the gradients to zeros\n",
        "            GENERATOR1_OPTIMIZER.zero_grad()\n",
        "            GENERATOR2_OPTIMIZER.zero_grad()\n",
        "            DISCRIMINATOR_OPTIMIZER.zero_grad()\n",
        "\n",
        "            # reshape to channel first\n",
        "            input_batched = input_batched.permute(0, 3, 1, 2)\n",
        "            ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
        "            mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
        "\n",
        "            # ===== FORWARD PASS =====\n",
        "\n",
        "            # 1. train discriminator\n",
        "            \n",
        "            # 1.1 generate images\n",
        "            input_batched.requires_grad_()\n",
        "            output1_batched, _ = generator1(input_batched)\n",
        "            shuffled_indices = torch.randperm(n = BATCH_SIZE)\n",
        "            output2_batched, _ = generator2(input_batched[shuffled_indices]) # random permutation\n",
        "\n",
        "            # 1.2 splice with ground truth\n",
        "            spliced1_batched = ((1-mask_batched) * output1_batched) + (mask_batched * ground_truth_batched) \n",
        "            spliced2_batched = ((1-mask_batched[shuffled_indices]) * output2_batched) + (mask_batched[shuffled_indices] * ground_truth_batched[shuffled_indices]) \n",
        "\n",
        "            # 1.3 feed into discriminator\n",
        "            label_real = torch.ones(BATCH_SIZE * 2, 1).to(device)\n",
        "            label_fake = torch.zeros(BATCH_SIZE * 2, 1).to(device)\n",
        "\n",
        "            pred_real = discriminator(torch.cat([ground_truth_batched, ground_truth_batched[shuffled_indices]], dim = 0))\n",
        "            loss_real = DISCRIMINATOR_LOSS_FUNCTION(pred_real, label_real)\n",
        "\n",
        "            pred_fake = discriminator(torch.cat([spliced1_batched, spliced2_batched[shuffled_indices]], dim = 0))\n",
        "            loss_fake = DISCRIMINATOR_LOSS_FUNCTION(pred_fake, label_fake)\n",
        "            loss_d = ALPHA * (loss_real + loss_fake)\n",
        "            loss_d.backward()\n",
        "            DISCRIMINATOR_OPTIMIZER.step()\n",
        "\n",
        "            # 2. train generator 1 (reconstruction, adverserial, contrastive)\n",
        "            # 2.1 forward pass by generator to produce images, splice them\n",
        "            output1_batched, _ = generator1(input_batched)\n",
        "            spliced1_batched = ((1-mask_batched) * output1_batched) + (mask_batched * ground_truth_batched) \n",
        "            \n",
        "            # 2.2 reconstruction loss\n",
        "            loss_g1_reconstruction = RECONSTRUCTION_LOSS(output1_batched, ground_truth_batched, mask_batched)\n",
        "\n",
        "            # 2.3 adverserial loss\n",
        "            label_real = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "            pred1_adverserial = discriminator(spliced1_batched)\n",
        "            loss_g1_adverserial = DISCRIMINATOR_LOSS_FUNCTION(pred1_adverserial, label_real) # want it to classify all as real\n",
        "\n",
        "            # 2.4 contrastive loss\n",
        "            shuffled_indices = torch.randperm(n = BATCH_SIZE)\n",
        "            copied_input_batched = torch.cat([ground_truth_batched * mask_batched[shuffled_indices], mask_batched[shuffled_indices]], dim = 1)\n",
        "            _, x1 = generator1(input_batched)\n",
        "            _, x2 = generator2(copied_input_batched) # same image, different masks\n",
        "            loss_contrastive1 = CONTRASTIVE_LOSS_FUNCTION(x1, x2)\n",
        "\n",
        "            # 2.5 backprop\n",
        "            loss_g1 = loss_g1_reconstruction + ALPHA * loss_g1_adverserial + BETA * loss_contrastive1\n",
        "            loss_g1.backward()\n",
        "            GENERATOR1_OPTIMIZER.step()\n",
        "\n",
        "            # 3.  train generator 2 (reconstruction, adverserial, contrastive)\n",
        "            # 3.1 forward pass by generator to produce images, reconstruction loss\n",
        "            output2_batched, _ = generator2(input_batched)\n",
        "            spliced2_batched = ((1-mask_batched) * output2_batched) + (mask_batched * ground_truth_batched) \n",
        "            loss_g2_reconstruction = RECONSTRUCTION_LOSS(output2_batched, ground_truth_batched, mask_batched)\n",
        "\n",
        "            # 3.2 adverserial loss\n",
        "            label_real = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "            pred2_adverserial = discriminator(spliced2_batched)\n",
        "            loss_g2_adverserial = DISCRIMINATOR_LOSS_FUNCTION(pred2_adverserial, label_real) # want it to classify all as real\n",
        "\n",
        "            # 3.3 contrastive loss\n",
        "            shuffled_indices = torch.randperm(n = BATCH_SIZE)\n",
        "            copied_input_batched = torch.cat([ground_truth_batched * mask_batched[shuffled_indices], mask_batched[shuffled_indices]], dim = 1)\n",
        "            _, x1 = generator1(input_batched)\n",
        "            _, x2 = generator2(copied_input_batched) # same image, different masks\n",
        "            loss_contrastive2 = CONTRASTIVE_LOSS_FUNCTION(x1, x2)\n",
        "\n",
        "            # 3.4 backprop\n",
        "            loss_g2 = loss_g2_reconstruction + ALPHA * loss_g2_adverserial + BETA * loss_contrastive2\n",
        "            loss_g2.backward()\n",
        "            GENERATOR2_OPTIMIZER.step()\n",
        "\n",
        "\n",
        "            # ===== COMPUTE STATISTICS, USING TORCH METRICS =====  \n",
        "            # 1. compute losses\n",
        "\n",
        "            running_results[\"loss_generator1\"] += loss_g1.detach().item()\n",
        "            running_results[\"loss_generator1_r\"] += loss_g1_reconstruction.detach().item()\n",
        "            running_results[\"loss_generator1_adv\"] += loss_g1_adverserial.detach().item()\n",
        "            running_results[\"loss_generator2\"] += loss_g2.detach().item()\n",
        "            running_results[\"loss_generator2_r\"] += loss_g2_reconstruction.detach().item()\n",
        "            running_results[\"loss_generator2_adv\"] += loss_g2_adverserial.detach().item()\n",
        "            running_results[\"loss_contrastive\"] += (loss_contrastive1.detach().item() + loss_contrastive2.detach().item())/2\n",
        "            running_results[\"loss_discriminator\"] += loss_d.detach().item()\n",
        "\n",
        "            # 2. for each key, compute, add item to results dictionary (take average of 2 generators)\n",
        "            for key, func in metrics.items():\n",
        "                res1 = func(output1_batched, ground_truth_batched, mask_batched).detach().item()\n",
        "                res2 = func(output2_batched, ground_truth_batched, mask_batched).detach().item()\n",
        "                running_results[key] += (res1 + res2)/2\n",
        "\n",
        "            # 3. log with wandb\n",
        "            if log_wandb and (num_batches % LOG_EVERY == 0):\n",
        "\n",
        "                # generator 1\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced1_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images1 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "                \n",
        "                # generator 2\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced2_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images2 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "\n",
        "                # log images and some metadata\n",
        "                wandb.log( {\n",
        "                    \"generator1_train_images\" : images1,\n",
        "                    \"generator2_train_images\" : images2,\n",
        "                    \"lr_generator1\" : GENERATOR1_OPTIMIZER.param_groups[0]['lr'],\n",
        "                    \"lr_generator2\" : GENERATOR2_OPTIMIZER.param_groups[0]['lr'],\n",
        "                    \"lr_discriminator\" : DISCRIMINATOR_OPTIMIZER.param_groups[0]['lr']\n",
        "\n",
        "                })\n",
        "\n",
        "                # log all metrics\n",
        "                wandb.log(\n",
        "                    {key : item/num_batches for key, item in running_results.items()}\n",
        "                )\n",
        "        \n",
        "\n",
        "            # ===== HOUSEKEEPING =====\n",
        "            del loss_g2\n",
        "            del loss_g1\n",
        "            del loss_d\n",
        "            del input_batched\n",
        "\n",
        "            # print results every some batches\n",
        "            if num_batches % BATCH_EVALUATE_EVERY == 0: \n",
        "\n",
        "                args = \"\"\n",
        "                for key, val in running_results.items():\n",
        "                    args += key + \": \" + str(running_results[key]/num_batches) + \"   \"\n",
        "                print(f\"\\r{num_batches}/{len(train_dataloader)}: \" + args, end = '', flush = True)\n",
        "\n",
        "    # normalise numbers by batch\n",
        "    for key, val in running_results.items():\n",
        "        running_results[key] /= num_batches\n",
        "\n",
        "    return running_results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4CuiHVAjKitr"
      },
      "outputs": [],
      "source": [
        "def evaluate_epoch(device, validation_dataloader, training_params : dict, metrics : dict, log_wandb = True):\n",
        "\n",
        "    # ===== INITIALIZE =====\n",
        "    # models\n",
        "    generator1 = training_params[\"generator1_model\"].to(device).eval()\n",
        "    generator2 = training_params[\"generator2_model\"].to(device).eval()\n",
        "\n",
        "    # constants\n",
        "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
        "\n",
        "    # epoch statistics\n",
        "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
        "\n",
        "    # ===== EVALUATE EPOCH =====\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batches = 0\n",
        "        for index, batch in enumerate(validation_dataloader, 1):\n",
        "            \n",
        "            batches += 1\n",
        "\n",
        "            # input and ground truth\n",
        "            input_batched = batch[\"image\"]\n",
        "            ground_truth_batched = batch[\"reconstructed\"]\n",
        "            mask_batched = batch[\"mask\"]\n",
        "\n",
        "            # move tensors to device\n",
        "            input_batched = input_batched.to(device)\n",
        "            ground_truth_batched = ground_truth_batched.to(device)\n",
        "            mask_batched = mask_batched.to(device)\n",
        "\n",
        "            # ===== FORWARD PASS =====\n",
        "\n",
        "            # 1. reshape to channel first\n",
        "            input_batched = input_batched.permute(0, 3, 1, 2)\n",
        "            ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
        "            mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
        "\n",
        "            # 2. predict    \n",
        "            output1_batched, _ = generator1(input_batched)\n",
        "            output2_batched, _ = generator2(input_batched)           \n",
        "            spliced1_batched = ((1-mask_batched) * output1_batched) + (mask_batched * ground_truth_batched) \n",
        "            spliced2_batched = ((1-mask_batched) * output2_batched) + (mask_batched * ground_truth_batched) \n",
        "\n",
        "            # 3. evaluate\n",
        "            for key, func in metrics.items():\n",
        "                running_results[key] += (func(output1_batched, ground_truth_batched, mask_batched).detach().item() + func(output2_batched, ground_truth_batched, mask_batched).detach().item())/2\n",
        "\n",
        "            args = \"\"\n",
        "            for key, val in running_results.items():\n",
        "                args += key + \": \" + str(running_results[key]/batches) + \"   \"\n",
        "            print(f\"\\r{batches}/{len(validation_dataloader)}: \" + args, end = '', flush = True)\n",
        "\n",
        "            # 4. log \n",
        "            if log_wandb:\n",
        "\n",
        "                # generator 1\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced1_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images1 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "                \n",
        "                # generator 2\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced2_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images2 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "\n",
        "                # log images and some metadata\n",
        "                wandb.log( {\n",
        "                    \"generator1_val_images\" : images1,\n",
        "                    \"generator2_val_images\" : images2\n",
        "                })\n",
        "\n",
        "                # log all metrics\n",
        "                wandb.log(\n",
        "                    {f\"eval_{key}\" : item/batches for key, item in running_results.items()}\n",
        "                )\n",
        "\n",
        "    # normalise numbers by batch\n",
        "    for key, val in running_results.items():\n",
        "        running_results[key] /= batches\n",
        "\n",
        "    return running_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "O_mQDSLXaPdz"
      },
      "outputs": [],
      "source": [
        "def train_evaluate(device, train_dataset, validation_dataset, training_params: dict, metrics: dict, start_epoch = 0, log_wandb = True):\n",
        "\n",
        "    # ===== INITIALIZE =====\n",
        "    # constants\n",
        "    NUM_EPOCHS = training_params[\"num_epochs\"]\n",
        "    BATCH_SIZE = training_params[\"batch_size\"]\n",
        "    GENERATOR1_SCHEDULER = training_params[\"generator1_scheduler\"]\n",
        "    GENERATOR2_SCHEDULER = training_params[\"generator2_scheduler\"]\n",
        "    DISCRIMINATOR_SCHEDULER = training_params[\"discriminator_scheduler\"]\n",
        "    SAVE_PATHS = training_params[\"save_paths\"]\n",
        "    NUM_WORKERS = 2\n",
        "    START_EPOCH = start_epoch\n",
        "\n",
        "    # models for saving\n",
        "    generator1 = training_params[\"generator1_model\"].to(device).train()\n",
        "    generator2 = training_params[\"generator2_model\"].to(device).train()\n",
        "    discriminator = training_params[\"discriminator_model\"].to(device).train()\n",
        "\n",
        "    # variable losses\n",
        "    train_results = {list(metrics.keys())[i] : [] for i in range(len(metrics)) } \n",
        "\n",
        "    train_results[\"loss_discriminator\"] = []\n",
        "    train_results[\"loss_generator1\"] = []\n",
        "    train_results[\"loss_generator1_r\"] = []\n",
        "    train_results[\"loss_generator1_adv\"] = []\n",
        "    train_results[\"loss_generator2\"] = []\n",
        "    train_results[\"loss_generator2_r\"] = []\n",
        "    train_results[\"loss_generator2_adv\"] = []\n",
        "    train_results[\"loss_contrastive\"] = []\n",
        "\n",
        "    eval_results = {list(metrics.keys())[i] : [] for i in range(len(metrics)) } \n",
        "\n",
        "    # dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS, drop_last = True)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS, drop_last = True)\n",
        "\n",
        "    # ===== TRAIN =====\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # train\n",
        "        print(f\"\\n===== Epoch: {START_EPOCH + epoch + 1} ===== \")\n",
        "        num_batches = 0\n",
        "\n",
        "        # train every epoch\n",
        "        results = train_epoch(device, train_dataloader, training_params, metrics, log_wandb = log_wandb)\n",
        "        for key, val in results.items():\n",
        "            train_results[key].append(val)\n",
        "\n",
        "        # evaluate every epoch\n",
        "        print()\n",
        "        results = evaluate_epoch(device, validation_dataloader, training_params, metrics, log_wandb = log_wandb)\n",
        "        for key, val in results.items():\n",
        "            eval_results[key].append(val)\n",
        "\n",
        "        # ===== EPOCH RESULTS =====\n",
        "        print(f\"\\nCompleted epoch {START_EPOCH + epoch + 1}! Took {(time.time() - start)/60} min\")\n",
        "\n",
        "        # ===== HOUSEKEEPING =====\n",
        "\n",
        "        # scheduler every epoch\n",
        "        if DISCRIMINATOR_SCHEDULER is not None:\n",
        "            DISCRIMINATOR_SCHEDULER.step(eval_results[\"L2 loss (Mask)\"][epoch])\n",
        "        if GENERATOR1_SCHEDULER is not None:\n",
        "            GENERATOR1_SCHEDULER.step(eval_results[\"L2 loss (Mask)\"][epoch])\n",
        "        if GENERATOR2_SCHEDULER is not None:\n",
        "            GENERATOR2_SCHEDULER.step(eval_results[\"L2 loss (Mask)\"][epoch])\n",
        "            \n",
        "        # save every epoch\n",
        "        SAVE = f\"{SAVE_PATHS['generator1']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
        "        torch.save(generator1.state_dict(), SAVE)\n",
        "        SAVE = f\"{SAVE_PATHS['generator2']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
        "        torch.save(generator2.state_dict(), SAVE)\n",
        "        SAVE = f\"{SAVE_PATHS['discriminator']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
        "        torch.save(discriminator.state_dict(), SAVE)  \n",
        "\n",
        "        print(\"Saved models!\")\n",
        "\n",
        "    return train_results, eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8IkxVhsVsf4"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_IuN9J8Jfp9j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Epoch: 2 ===== \n",
            "75/78: Peak SnR (Whole): 27.82557814280192   L2 loss (Whole): 0.002545232068514451   L2 loss (Mask): 0.22089352036515872   L1 loss (Whole): 0.006425681061421831   L1 loss (Mask): 0.5574541946252187   LPIPS (Whole): 0.06519662516812484   loss_discriminator: 0.6970850960413615   loss_generator1: 0.5421650759379069   loss_generator1_r: 0.183538498878479   loss_generator1_adv: 0.6963548294703166   loss_generator2: 0.619266293446223   loss_generator2_r: 0.25824854185183843   loss_generator2_adv: 0.7012719861666361   loss_contrastive: 1.0415461115042368        \n",
            "7/7: Peak SnR (Whole): 30.29729965754918   L2 loss (Whole): 0.0009511519872051265   L2 loss (Mask): 0.08299987390637398   L1 loss (Whole): 0.004319941491952964   L1 loss (Mask): 0.3768979864461081   LPIPS (Whole): 0.04348696821502277      \n",
            "Completed epoch 2! Took 5.397860399881998 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 3 ===== \n",
            "75/78: Peak SnR (Whole): 30.6864847946167   L2 loss (Whole): 0.0008568978076800704   L2 loss (Mask): 0.07416117804745834   L1 loss (Whole): 0.004224703192400436   L1 loss (Mask): 0.3655960494279861   LPIPS (Whole): 0.04382125787436962   loss_discriminator: 0.6932836055755616   loss_generator1: 0.4284778114159902   loss_generator1_r: 0.07017566248774529   loss_generator1_adv: 0.7051362069447835   loss_generator2: 0.4366821352640788   loss_generator2_r: 0.07814669360717137   loss_generator2_adv: 0.7056326405207316   loss_contrastive: 0.5726582354307175        \n",
            "7/7: Peak SnR (Whole): 31.26700074332101   L2 loss (Whole): 0.0007489627085825694   L2 loss (Mask): 0.06555271707475185   L1 loss (Whole): 0.003886589115219457   L1 loss (Mask): 0.3402090562241418   LPIPS (Whole): 0.04016242761697088     \n",
            "Completed epoch 3! Took 5.594143187999725 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 4 ===== \n",
            "75/78: Peak SnR (Whole): 31.178950411478677   L2 loss (Whole): 0.0007637827071206024   L2 loss (Mask): 0.0666037215044101   L1 loss (Whole): 0.003954138600577911   L1 loss (Mask): 0.34479139586289725   LPIPS (Whole): 0.041888339271148045   loss_discriminator: 0.6892665402094523   loss_generator1: 0.42579015890757244   loss_generator1_r: 0.06451425085465114   loss_generator1_adv: 0.714771146774292   loss_generator2: 0.429160696665446   loss_generator2_r: 0.06869319215416908   loss_generator2_adv: 0.7131752189000448   loss_contrastive: 0.38851162791252136       \n",
            "7/7: Peak SnR (Whole): 31.582931791033065   L2 loss (Whole): 0.0006956612757806267   L2 loss (Mask): 0.06056171788700989   L1 loss (Whole): 0.0037227997922205497   L1 loss (Mask): 0.32412861926215036   LPIPS (Whole): 0.04048066985394273    \n",
            "Completed epoch 4! Took 5.106950823465983 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 5 ===== \n",
            "75/78: Peak SnR (Whole): 31.477859484354656   L2 loss (Whole): 0.0007133169968922933   L2 loss (Mask): 0.061799199332793556   L1 loss (Whole): 0.0037934135847414534   L1 loss (Mask): 0.32872251788775125   LPIPS (Whole): 0.039667546674609186   loss_discriminator: 0.6903901529312134   loss_generator1: 0.4213969441254934   loss_generator1_r: 0.05972565501928329   loss_generator1_adv: 0.717566282749176   loss_generator2: 0.42224951227506   loss_generator2_r: 0.06387274364630381   loss_generator2_adv: 0.7109886797269186   loss_contrastive: 0.28852888544400535       \n",
            "7/7: Peak SnR (Whole): 31.276738030569895   L2 loss (Whole): 0.0007452134575162615   L2 loss (Mask): 0.06467774829694203   L1 loss (Whole): 0.0038898552135963526   L1 loss (Mask): 0.3375091999769211   LPIPS (Whole): 0.03957321335162435     \n",
            "Completed epoch 5! Took 5.387768324216207 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 6 ===== \n",
            "75/78: Peak SnR (Whole): 31.755139172871907   L2 loss (Whole): 0.0006684686196967959   L2 loss (Mask): 0.05802203190823396   L1 loss (Whole): 0.003649994518297414   L1 loss (Mask): 0.31681266367435457   LPIPS (Whole): 0.038118591184417405   loss_discriminator: 0.6874676386515299   loss_generator1: 0.417915602127711   loss_generator1_r: 0.05604457954565684   loss_generator1_adv: 0.7193321339289347   loss_generator2: 0.42224391222000124   loss_generator2_r: 0.05999948427081108   loss_generator2_adv: 0.7200838804244996   loss_contrastive: 0.22037232180436453     \n",
            "7/7: Peak SnR (Whole): 32.05669593811035   L2 loss (Whole): 0.0006242381849525762   L2 loss (Mask): 0.054263948063765256   L1 loss (Whole): 0.003504948673902878   L1 loss (Mask): 0.30470854895455496   LPIPS (Whole): 0.0366002121674163     \n",
            "Completed epoch 6! Took 5.237956829865774 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 7 ===== \n",
            "75/78: Peak SnR (Whole): 31.907594769795736   L2 loss (Whole): 0.000645197806879878   L2 loss (Mask): 0.05578053556382656   L1 loss (Whole): 0.003574285878178974   L1 loss (Mask): 0.30899875223636625   LPIPS (Whole): 0.03711475655436516   loss_discriminator: 0.6839209731419881   loss_generator1: 0.4170661437511444   loss_generator1_r: 0.0540796160697937   loss_generator1_adv: 0.7224692257245382   loss_generator2: 0.4215429409344991   loss_generator2_r: 0.05748145505785942   loss_generator2_adv: 0.7246223560969035   loss_contrastive: 0.17511111080646516   5   \n",
            "7/7: Peak SnR (Whole): 32.36489295959473   L2 loss (Whole): 0.0005798954308764743   L2 loss (Mask): 0.050056087385330884   L1 loss (Whole): 0.0033683895765404615   L1 loss (Mask): 0.29069772149835316   LPIPS (Whole): 0.03578045165964535     \n",
            "Completed epoch 7! Took 5.568121600151062 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 8 ===== \n",
            "75/78: Peak SnR (Whole): 31.925151596069338   L2 loss (Whole): 0.0006442612630780787   L2 loss (Mask): 0.055994075164198874   L1 loss (Whole): 0.003567869469212989   L1 loss (Mask): 0.31010478297869365   LPIPS (Whole): 0.03821022724111875   loss_discriminator: 0.6824622591336568   loss_generator1: 0.4163092235724131   loss_generator1_r: 0.05467561403910319   loss_generator1_adv: 0.7203998692830403   loss_generator2: 0.42164933840433755   loss_generator2_r: 0.05731253628929456   loss_generator2_adv: 0.7258166527748108   loss_contrastive: 0.14310736497243245   \n",
            "7/7: Peak SnR (Whole): 31.892009598868235   L2 loss (Whole): 0.0006469312065746635   L2 loss (Mask): 0.056086597431983264   L1 loss (Whole): 0.0035454013351617114   L1 loss (Mask): 0.3074205496481487   LPIPS (Whole): 0.038816996450935094   \n",
            "Completed epoch 8! Took 6.089249618848165 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 9 ===== \n",
            "75/78: Peak SnR (Whole): 31.902084999084472   L2 loss (Whole): 0.0006467803311534226   L2 loss (Mask): 0.056107841059565546   L1 loss (Whole): 0.003573202776412169   L1 loss (Mask): 0.3100183500846227   LPIPS (Whole): 0.03922042253116766   loss_discriminator: 0.6817371026674907   loss_generator1: 0.4180780359109243   loss_generator1_r: 0.05497101257244746   loss_generator1_adv: 0.7238033294677735   loss_generator2: 0.4215152955055237   loss_generator2_r: 0.05724466954668363   loss_generator2_adv: 0.7261363609631857   loss_contrastive: 0.1203899817665418      \n",
            "7/7: Peak SnR (Whole): 31.95849391392299   L2 loss (Whole): 0.0006376335929547038   L2 loss (Mask): 0.05535829812288284   L1 loss (Whole): 0.0034875299648514818   L1 loss (Mask): 0.3027712255716324   LPIPS (Whole): 0.038802530616521835     \n",
            "Completed epoch 9! Took 6.123095615704854 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 10 ===== \n",
            "75/78: Peak SnR (Whole): 32.06677740732829   L2 loss (Whole): 0.0006226384259449939   L2 loss (Mask): 0.053919168735543886   L1 loss (Whole): 0.0034819312393665315   L1 loss (Mask): 0.3015769296884537   LPIPS (Whole): 0.03774749701221784   loss_discriminator: 0.6767784476280212   loss_generator1: 0.41710891485214235   loss_generator1_r: 0.05269868806004524   loss_generator1_adv: 0.7267076913515726   loss_generator2: 0.4241072408358256   loss_generator2_r: 0.05513964941104253   loss_generator2_adv: 0.7358245547612509   loss_contrastive: 0.10558467408021291      \n",
            "7/7: Peak SnR (Whole): 32.350517000470845   L2 loss (Whole): 0.0005826350866949984   L2 loss (Mask): 0.050461833232215474   L1 loss (Whole): 0.0033423283104119556   L1 loss (Mask): 0.28948249561446054   LPIPS (Whole): 0.03466938728732722   \n",
            "Completed epoch 10! Took 6.302917472521464 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 11 ===== \n",
            "75/78: Peak SnR (Whole): 32.198865801493326   L2 loss (Whole): 0.0006036265883206701   L2 loss (Mask): 0.05223396179576715   L1 loss (Whole): 0.0034341551192725696   L1 loss (Mask): 0.29718236744403836   LPIPS (Whole): 0.03635513859490554   loss_discriminator: 0.6755424451828003   loss_generator1: 0.41768620649973554   loss_generator1_r: 0.05074240565299988   loss_generator1_adv: 0.7319553510348003   loss_generator2: 0.42555726408958433   loss_generator2_r: 0.05372551793853442   loss_generator2_adv: 0.741727929910024   loss_contrastive: 0.09669533694783847    \n",
            "7/7: Peak SnR (Whole): 32.42901965550014   L2 loss (Whole): 0.0005708455781651926   L2 loss (Mask): 0.05011615662702492   L1 loss (Whole): 0.0032706646182175192   L1 loss (Mask): 0.2871428983552115   LPIPS (Whole): 0.03438496815838984      \n",
            "Completed epoch 11! Took 6.169151699542999 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 12 ===== \n",
            "75/78: Peak SnR (Whole): 32.209471549987796   L2 loss (Whole): 0.0006019835589298357   L2 loss (Mask): 0.052084198618928594   L1 loss (Whole): 0.003411201162574192   L1 loss (Mask): 0.2951760929822922   LPIPS (Whole): 0.035686511149009066   loss_discriminator: 0.6726040236155192   loss_generator1: 0.41952096184094745   loss_generator1_r: 0.050482713878154756   loss_generator1_adv: 0.7362561972935995   loss_generator2: 0.4260978404680888   loss_generator2_r: 0.053685683359702425   loss_generator2_adv: 0.7430072848002116   loss_contrastive: 0.0909332590798537   \n",
            "7/7: Peak SnR (Whole): 32.27371842520578   L2 loss (Whole): 0.0005921865605549621   L2 loss (Mask): 0.05138348468712398   L1 loss (Whole): 0.003348833981103131   L1 loss (Mask): 0.29058245037283215   LPIPS (Whole): 0.03422973544469902     \n",
            "Completed epoch 12! Took 6.275726238886516 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 13 ===== \n",
            "75/78: Peak SnR (Whole): 32.18788412729899   L2 loss (Whole): 0.0006043829720389719   L2 loss (Mask): 0.05236459071437518   L1 loss (Whole): 0.0034204794637237987   L1 loss (Mask): 0.2963484148184458   LPIPS (Whole): 0.03529974135259787   loss_discriminator: 0.6730069843928019   loss_generator1: 0.4209649733702342   loss_generator1_r: 0.0508058333893617   loss_generator1_adv: 0.7385789004961649   loss_generator2: 0.42778903325398765   loss_generator2_r: 0.053923348039388655   loss_generator2_adv: 0.745996683438619   loss_contrastive: 0.08685173854231834        \n",
            "7/7: Peak SnR (Whole): 32.26100621904646   L2 loss (Whole): 0.0005952642448911709   L2 loss (Mask): 0.05171666533819267   L1 loss (Whole): 0.003336282180888312   L1 loss (Mask): 0.2899104335478374   LPIPS (Whole): 0.033528597227164676     \n",
            "Completed epoch 13! Took 6.287554411093394 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 14 ===== \n",
            "75/78: Peak SnR (Whole): 32.12246116638184   L2 loss (Whole): 0.0006141437384455154   L2 loss (Mask): 0.05327302356561025   L1 loss (Whole): 0.0034466073568910362   L1 loss (Mask): 0.29897462646166484   LPIPS (Whole): 0.03489809333036343   loss_discriminator: 0.6738946970303853   loss_generator1: 0.42184795141220094   loss_generator1_r: 0.051824931204319   loss_generator1_adv: 0.7383746576309204   loss_generator2: 0.4288598549365997   loss_generator2_r: 0.0547211159269015   loss_generator2_adv: 0.7466081515947978   loss_contrastive: 0.08351755077640216       \n",
            "7/7: Peak SnR (Whole): 32.09238270350865   L2 loss (Whole): 0.0006187327796526786   L2 loss (Mask): 0.05392057741326945   L1 loss (Whole): 0.003406664971927447   L1 loss (Mask): 0.2968999913760594   LPIPS (Whole): 0.034238461405038834     \n",
            "Completed epoch 14! Took 6.017171621322632 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 15 ===== \n",
            "75/78: Peak SnR (Whole): 32.07862112681071   L2 loss (Whole): 0.0006200799097617467   L2 loss (Mask): 0.05379106325407823   L1 loss (Whole): 0.0034754674757520357   L1 loss (Mask): 0.3014821320772171   LPIPS (Whole): 0.035605567221840224   loss_discriminator: 0.677281354268392   loss_generator1: 0.4180471614996592   loss_generator1_r: 0.05223814437786738   loss_generator1_adv: 0.7299992497762045   loss_generator2: 0.42660990277926125   loss_generator2_r: 0.05534398213028908   loss_generator2_adv: 0.7409162839253743   loss_contrastive: 0.08085873718063037      \n",
            "7/7: Peak SnR (Whole): 31.94234343937465   L2 loss (Whole): 0.0006381881207094661   L2 loss (Mask): 0.05547402187117508   L1 loss (Whole): 0.0034885007522201966   L1 loss (Mask): 0.3031529209443501   LPIPS (Whole): 0.03537915407546929      \n",
            "Completed epoch 15! Took 6.3123810688654585 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 16 ===== \n",
            "75/78: Peak SnR (Whole): 31.932469062805175   L2 loss (Whole): 0.0006415230047423393   L2 loss (Mask): 0.05567599569757779   L1 loss (Whole): 0.0035441102599725128   L1 loss (Mask): 0.3075702971220016   LPIPS (Whole): 0.036530831108490626   loss_discriminator: 0.6809205905596415   loss_generator1: 0.4149305498600006   loss_generator1_r: 0.055009177873531975   loss_generator1_adv: 0.7182650756835938   loss_generator2: 0.43014028390248615   loss_generator2_r: 0.05634281352162361   loss_generator2_adv: 0.7460135515530905   loss_contrastive: 0.07897634396950404    \n",
            "7/7: Peak SnR (Whole): 31.892476899283274   L2 loss (Whole): 0.0006462808856407978   L2 loss (Mask): 0.0557364071054118   L1 loss (Whole): 0.0035621301836467217   L1 loss (Mask): 0.30725584498473574   LPIPS (Whole): 0.03655685856938362    \n",
            "Completed epoch 16! Took 6.385634843508402 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 17 ===== \n",
            "75/78: Peak SnR (Whole): 31.78736509958903   L2 loss (Whole): 0.0006633384673235317   L2 loss (Mask): 0.05752236691613992   L1 loss (Whole): 0.0036228598654270174   L1 loss (Mask): 0.31417674640814464   LPIPS (Whole): 0.038548666909337045   loss_discriminator: 0.6875511956214905   loss_generator1: 0.4096013343334198   loss_generator1_r: 0.05809999664624532   loss_generator1_adv: 0.7014505545298259   loss_generator2: 0.43298877676328024   loss_generator2_r: 0.05694473718603452   loss_generator2_adv: 0.7505364195505778   loss_contrastive: 0.07759458060065905      \n",
            "7/7: Peak SnR (Whole): 31.864853313991002   L2 loss (Whole): 0.0006491635327360459   L2 loss (Mask): 0.05653765265430723   L1 loss (Whole): 0.0035387478502733366   L1 loss (Mask): 0.3081960997411183   LPIPS (Whole): 0.037258440894739966   \n",
            "Completed epoch 17! Took 6.489016310373942 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 18 ===== \n",
            "75/78: Peak SnR (Whole): 31.81614195505778   L2 loss (Whole): 0.0006586622656323016   L2 loss (Mask): 0.056980647270878156   L1 loss (Whole): 0.0036123253668968874   L1 loss (Mask): 0.3125298502047857   LPIPS (Whole): 0.03866147942841053   loss_discriminator: 0.6894677448272705   loss_generator1: 0.40753143032391864   loss_generator1_r: 0.057567802121241886   loss_generator1_adv: 0.6983987728754679   loss_generator2: 0.4300822595755259   loss_generator2_r: 0.056393492420514425   loss_generator2_adv: 0.7458459623654683   loss_contrastive: 0.07650146886706352    \n",
            "7/7: Peak SnR (Whole): 31.94842529296875   L2 loss (Whole): 0.0006399364293819028   L2 loss (Mask): 0.055843302980065346   L1 loss (Whole): 0.0035244165254490717   L1 loss (Mask): 0.30765366341386524   LPIPS (Whole): 0.037023471402270455   \n",
            "Completed epoch 18! Took 6.286062057813009 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 19 ===== \n",
            "75/78: Peak SnR (Whole): 31.741324844360353   L2 loss (Whole): 0.0006696574215311557   L2 loss (Mask): 0.0579795774569114   L1 loss (Whole): 0.0036358108092099427   L1 loss (Mask): 0.314832191268603   LPIPS (Whole): 0.03942024618387222   loss_discriminator: 0.6894095007578532   loss_generator1: 0.4078489836057027   loss_generator1_r: 0.05791990290085475   loss_generator1_adv: 0.6983446319897969   loss_generator2: 0.42906970699628194   loss_generator2_r: 0.058039252012968064   loss_generator2_adv: 0.7405455485979716   loss_contrastive: 0.07572224274277688      \n",
            "7/7: Peak SnR (Whole): 31.887001991271973   L2 loss (Whole): 0.0006466434777913881   L2 loss (Mask): 0.05594654700585774   L1 loss (Whole): 0.003583449886978737   L1 loss (Mask): 0.3100157699414662   LPIPS (Whole): 0.0382173697331122      \n",
            "Completed epoch 19! Took 6.229329450925191 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 20 ===== \n",
            "75/78: Peak SnR (Whole): 31.72732587178548   L2 loss (Whole): 0.0006720849960887183   L2 loss (Mask): 0.058148132239778834   L1 loss (Whole): 0.003650509042975803   L1 loss (Mask): 0.3158886631329854   LPIPS (Whole): 0.039644745737314226   loss_discriminator: 0.6900827741622925   loss_generator1: 0.4121495453516642   loss_generator1_r: 0.057636149575312934   loss_generator1_adv: 0.707518044312795   loss_generator2: 0.4304898420969645   loss_generator2_r: 0.05866011490424474   loss_generator2_adv: 0.7421479447682698   loss_contrastive: 0.07550654898087183        \n",
            "7/7: Peak SnR (Whole): 31.83091367994036   L2 loss (Whole): 0.0006557598493860237   L2 loss (Mask): 0.05643250048160553   L1 loss (Whole): 0.003583329330597605   L1 loss (Mask): 0.3083578220435551   LPIPS (Whole): 0.038435274469000955     \n",
            "Completed epoch 20! Took 6.1640321016311646 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 21 ===== \n",
            "75/78: Peak SnR (Whole): 31.74789966583252   L2 loss (Whole): 0.0006687144256041696   L2 loss (Mask): 0.058014392803112665   L1 loss (Whole): 0.0036348978957782187   L1 loss (Mask): 0.315340473651886   LPIPS (Whole): 0.03911852459112803   loss_discriminator: 0.6907970174153646   loss_generator1: 0.41128132343292234   loss_generator1_r: 0.05745341032743454   loss_generator1_adv: 0.7061519320805868   loss_generator2: 0.42797520915667214   loss_generator2_r: 0.058575375278790795   loss_generator2_adv: 0.7372947327295939   loss_contrastive: 0.07522063995401064     \n",
            "7/7: Peak SnR (Whole): 31.568908827645437   L2 loss (Whole): 0.0006965141053245004   L2 loss (Mask): 0.05971755699387619   L1 loss (Whole): 0.0037102516840345095   L1 loss (Mask): 0.31820134605680195   LPIPS (Whole): 0.03965272302074092   \n",
            "Completed epoch 21! Took 6.032531066735586 min\n",
            "Saved models!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'Peak SnR (Whole)': [27.907797226539024,\n",
              "   30.694042340303078,\n",
              "   31.17684268951416,\n",
              "   31.49152325361203,\n",
              "   31.738535697643574,\n",
              "   31.906020298982277,\n",
              "   31.93122016466581,\n",
              "   31.90880604279347,\n",
              "   32.06623583573561,\n",
              "   32.20152477117685,\n",
              "   32.20963056270893,\n",
              "   32.18332157379542,\n",
              "   32.1223659270849,\n",
              "   32.082534447694435,\n",
              "   31.92707929855738,\n",
              "   31.78530859335875,\n",
              "   31.816888442406288,\n",
              "   31.737130482991535,\n",
              "   31.728797325721153,\n",
              "   31.741232786423122],\n",
              "  'L2 loss (Whole)': [0.00248627650039569,\n",
              "   0.0008554678881922975,\n",
              "   0.0007641306669355776,\n",
              "   0.0007111514767016977,\n",
              "   0.0006712402805883008,\n",
              "   0.0006453644473834011,\n",
              "   0.0006433354320529944,\n",
              "   0.0006456783027179205,\n",
              "   0.0006226876333433514,\n",
              "   0.0006031666198396124,\n",
              "   0.0006019604099072659,\n",
              "   0.000605048488441986,\n",
              "   0.0006141030921403748,\n",
              "   0.0006195258361120254,\n",
              "   0.0006422738780458578,\n",
              "   0.0006635738219558381,\n",
              "   0.0006584740823689991,\n",
              "   0.0006702185353974644,\n",
              "   0.0006718508028335726,\n",
              "   0.0006698065486629135],\n",
              "  'L2 loss (Mask)': [0.21577289175146666,\n",
              "   0.07402072476748472,\n",
              "   0.06664883918487109,\n",
              "   0.061638074927031994,\n",
              "   0.05827460094140126,\n",
              "   0.05581121108470819,\n",
              "   0.05588918520758549,\n",
              "   0.05603665934923368,\n",
              "   0.05393560199687878,\n",
              "   0.0522207418599954,\n",
              "   0.05203050320060589,\n",
              "   0.05243564963054199,\n",
              "   0.053275370301726535,\n",
              "   0.05373044278568182,\n",
              "   0.055735741885235675,\n",
              "   0.057505374105694965,\n",
              "   0.05699168074016388,\n",
              "   0.058040353923271865,\n",
              "   0.05811304778147202,\n",
              "   0.05810853378035319],\n",
              "  'L1 loss (Whole)': [0.006355584805043271,\n",
              "   0.0042188422446951075,\n",
              "   0.003954732412985789,\n",
              "   0.003785685919678937,\n",
              "   0.0036602047158596227,\n",
              "   0.0035747447325728643,\n",
              "   0.00356487728183707,\n",
              "   0.0035677809053315567,\n",
              "   0.0034813755475438368,\n",
              "   0.003430547552386251,\n",
              "   0.0034133041417823196,\n",
              "   0.0034228094259444145,\n",
              "   0.0034469795362570155,\n",
              "   0.003474220785742196,\n",
              "   0.0035467077769004763,\n",
              "   0.0036243734932026993,\n",
              "   0.003611065802546457,\n",
              "   0.003637501890830791,\n",
              "   0.0036505373024668256,\n",
              "   0.003637916871155493],\n",
              "  'L1 loss (Mask)': [0.551359450014738,\n",
              "   0.3650192740635994,\n",
              "   0.3449237300799443,\n",
              "   0.32819899458151597,\n",
              "   0.31776597026066905,\n",
              "   0.30912738465345824,\n",
              "   0.3097102280992728,\n",
              "   0.3096817393715565,\n",
              "   0.30159392131444734,\n",
              "   0.2970197011645024,\n",
              "   0.2950622701109984,\n",
              "   0.29662676480336064,\n",
              "   0.299042566273457,\n",
              "   0.3012990047916388,\n",
              "   0.30776660182537174,\n",
              "   0.3141096700460483,\n",
              "   0.31256877516324705,\n",
              "   0.31504253278940153,\n",
              "   0.31581156700849533,\n",
              "   0.3155989815027286],\n",
              "  'LPIPS (Whole)': [0.06449027431125824,\n",
              "   0.04376617253113251,\n",
              "   0.04180961133291324,\n",
              "   0.03961927684931419,\n",
              "   0.03816382978589107,\n",
              "   0.03713908392744951,\n",
              "   0.03821098947754273,\n",
              "   0.03917613725822706,\n",
              "   0.037679372856823296,\n",
              "   0.03632428444539889,\n",
              "   0.03573241316450712,\n",
              "   0.03525296992693956,\n",
              "   0.03488809255978618,\n",
              "   0.035624975696779214,\n",
              "   0.03660710495060835,\n",
              "   0.038623931125188485,\n",
              "   0.03867661363134781,\n",
              "   0.03942094396990843,\n",
              "   0.039616432518531114,\n",
              "   0.039119752744833626],\n",
              "  'loss_discriminator': [0.6970130564310612,\n",
              "   0.6932572248654488,\n",
              "   0.689238503193244,\n",
              "   0.6903400719165802,\n",
              "   0.6875502543571668,\n",
              "   0.6838260782070649,\n",
              "   0.6824226272411835,\n",
              "   0.6815308852073474,\n",
              "   0.6766520218971448,\n",
              "   0.6754740873972574,\n",
              "   0.6726015149018704,\n",
              "   0.6730145506369762,\n",
              "   0.6740741936060098,\n",
              "   0.6774371021833175,\n",
              "   0.6811593893246773,\n",
              "   0.6878050871384449,\n",
              "   0.6894245812526116,\n",
              "   0.6893592996475024,\n",
              "   0.6899813268429194,\n",
              "   0.6907400167905368],\n",
              "  'loss_generator1': [0.5382324972978005,\n",
              "   0.4284425316712795,\n",
              "   0.4260113288958867,\n",
              "   0.4212779823022011,\n",
              "   0.4179428674471684,\n",
              "   0.4168364352140671,\n",
              "   0.41653549594756883,\n",
              "   0.418069377159461,\n",
              "   0.41757499865996534,\n",
              "   0.4173102115209286,\n",
              "   0.4200110049583973,\n",
              "   0.4207574866521053,\n",
              "   0.4224388599395752,\n",
              "   0.41820484208755004,\n",
              "   0.41618132438415134,\n",
              "   0.40861933200787276,\n",
              "   0.4078725030024846,\n",
              "   0.4078098096144505,\n",
              "   0.41176888996209854,\n",
              "   0.41233713848468584],\n",
              "  'loss_generator1_r': [0.1796108701099188,\n",
              "   0.07007767169330364,\n",
              "   0.0645194409462886,\n",
              "   0.0595515241416601,\n",
              "   0.0563315209478904,\n",
              "   0.05412762908217234,\n",
              "   0.05461221536955772,\n",
              "   0.0548756439716388,\n",
              "   0.052685979419411756,\n",
              "   0.05073481115202109,\n",
              "   0.05044331014729463,\n",
              "   0.0508976163676916,\n",
              "   0.05183983326722414,\n",
              "   0.05217012906303772,\n",
              "   0.05510443788117323,\n",
              "   0.0580728355413064,\n",
              "   0.05753770207938475,\n",
              "   0.05796658190397116,\n",
              "   0.05760595942728031,\n",
              "   0.05755106197335781],\n",
              "  'loss_generator1_adv': [0.6965901316740574,\n",
              "   0.7053570816150079,\n",
              "   0.7152495842713577,\n",
              "   0.7177083836152003,\n",
              "   0.7188324530919393,\n",
              "   0.7219269497272296,\n",
              "   0.7209892647388654,\n",
              "   0.7239853120767153,\n",
              "   0.727667204844646,\n",
              "   0.731221650655453,\n",
              "   0.73731556840432,\n",
              "   0.7379814164760785,\n",
              "   0.7395275708956596,\n",
              "   0.7304511528748733,\n",
              "   0.7205767792004806,\n",
              "   0.6995418889400287,\n",
              "   0.6991416964775476,\n",
              "   0.6981729047420697,\n",
              "   0.7068172456362308,\n",
              "   0.7080680697392194],\n",
              "  'loss_generator2': [0.6128748261775726,\n",
              "   0.43655328376170915,\n",
              "   0.4293206215668947,\n",
              "   0.42224021370594317,\n",
              "   0.42216816315284145,\n",
              "   0.421295974117059,\n",
              "   0.42185019911863864,\n",
              "   0.42157091429600346,\n",
              "   0.42469784961296964,\n",
              "   0.4251897407647891,\n",
              "   0.426521183970647,\n",
              "   0.42751759863816774,\n",
              "   0.4294300832045384,\n",
              "   0.4269314255469885,\n",
              "   0.43164761479084307,\n",
              "   0.43197713028161955,\n",
              "   0.43059444924195606,\n",
              "   0.428872518432446,\n",
              "   0.43010663604125,\n",
              "   0.4290257367568138],\n",
              "  'loss_generator2_r': [0.2519349133930145,\n",
              "   0.0779637778416658,\n",
              "   0.06877823742345357,\n",
              "   0.06372462571240388,\n",
              "   0.06021768093491212,\n",
              "   0.057494793087244034,\n",
              "   0.057166155045613266,\n",
              "   0.05719767472682855,\n",
              "   0.05518522457434581,\n",
              "   0.05370667256796972,\n",
              "   0.05361769625391716,\n",
              "   0.05397368289339237,\n",
              "   0.05471090733622893,\n",
              "   0.05529075650832592,\n",
              "   0.05636704588929812,\n",
              "   0.05693791267008354,\n",
              "   0.056445659400942996,\n",
              "   0.05811412594257257,\n",
              "   0.05862013613566374,\n",
              "   0.05866600558734857],\n",
              "  'loss_generator2_adv': [0.7013557591499426,\n",
              "   0.7058324225437946,\n",
              "   0.713369858570588,\n",
              "   0.7112983029622298,\n",
              "   0.7195156736251636,\n",
              "   0.7241152708347027,\n",
              "   0.7265210487903693,\n",
              "   0.7263489006421505,\n",
              "   0.7369170700892423,\n",
              "   0.7410336664089789,\n",
              "   0.7439902761043646,\n",
              "   0.7453545362521441,\n",
              "   0.7477697386191442,\n",
              "   0.7416672783020215,\n",
              "   0.7489805076366816,\n",
              "   0.7485278814266889,\n",
              "   0.746766767440698,\n",
              "   0.740001801496897,\n",
              "   0.7414610240703974,\n",
              "   0.7392149911477015],\n",
              "  'loss_contrastive': [1.029429819339361,\n",
              "   0.5679805974165598,\n",
              "   0.3862276472724401,\n",
              "   0.28693519446712273,\n",
              "   0.21938837310060477,\n",
              "   0.17444384461029983,\n",
              "   0.14260824397206306,\n",
              "   0.119993096551834,\n",
              "   0.10547522918727154,\n",
              "   0.09654050616499706,\n",
              "   0.09091308201925877,\n",
              "   0.0867905649714745,\n",
              "   0.08347726914171989,\n",
              "   0.08080852690797585,\n",
              "   0.07894048519814625,\n",
              "   0.07754152124890915,\n",
              "   0.07646802698190396,\n",
              "   0.07571334396608365,\n",
              "   0.0755149108859209,\n",
              "   0.07521378081769516]},\n",
              " {'Peak SnR (Whole)': [30.29729965754918,\n",
              "   31.26700074332101,\n",
              "   31.582931791033065,\n",
              "   31.276738030569895,\n",
              "   32.05669593811035,\n",
              "   32.36489295959473,\n",
              "   31.892009598868235,\n",
              "   31.95849391392299,\n",
              "   32.350517000470845,\n",
              "   32.42901965550014,\n",
              "   32.27371842520578,\n",
              "   32.26100621904646,\n",
              "   32.09238270350865,\n",
              "   31.94234343937465,\n",
              "   31.892476899283274,\n",
              "   31.864853313991002,\n",
              "   31.94842529296875,\n",
              "   31.887001991271973,\n",
              "   31.83091367994036,\n",
              "   31.568908827645437],\n",
              "  'L2 loss (Whole)': [0.0009511519872051265,\n",
              "   0.0007489627085825694,\n",
              "   0.0006956612757806267,\n",
              "   0.0007452134575162615,\n",
              "   0.0006242381849525762,\n",
              "   0.0005798954308764743,\n",
              "   0.0006469312065746635,\n",
              "   0.0006376335929547038,\n",
              "   0.0005826350866949984,\n",
              "   0.0005708455781651926,\n",
              "   0.0005921865605549621,\n",
              "   0.0005952642448911709,\n",
              "   0.0006187327796526786,\n",
              "   0.0006381881207094661,\n",
              "   0.0006462808856407978,\n",
              "   0.0006491635327360459,\n",
              "   0.0006399364293819028,\n",
              "   0.0006466434777913881,\n",
              "   0.0006557598493860237,\n",
              "   0.0006965141053245004],\n",
              "  'L2 loss (Mask)': [0.08299987390637398,\n",
              "   0.06555271707475185,\n",
              "   0.06056171788700989,\n",
              "   0.06467774829694203,\n",
              "   0.054263948063765256,\n",
              "   0.050056087385330884,\n",
              "   0.056086597431983264,\n",
              "   0.05535829812288284,\n",
              "   0.050461833232215474,\n",
              "   0.05011615662702492,\n",
              "   0.05138348468712398,\n",
              "   0.05171666533819267,\n",
              "   0.05392057741326945,\n",
              "   0.05547402187117508,\n",
              "   0.0557364071054118,\n",
              "   0.05653765265430723,\n",
              "   0.055843302980065346,\n",
              "   0.05594654700585774,\n",
              "   0.05643250048160553,\n",
              "   0.05971755699387619],\n",
              "  'L1 loss (Whole)': [0.004319941491952964,\n",
              "   0.003886589115219457,\n",
              "   0.0037227997922205497,\n",
              "   0.0038898552135963526,\n",
              "   0.003504948673902878,\n",
              "   0.0033683895765404615,\n",
              "   0.0035454013351617114,\n",
              "   0.0034875299648514818,\n",
              "   0.0033423283104119556,\n",
              "   0.0032706646182175192,\n",
              "   0.003348833981103131,\n",
              "   0.003336282180888312,\n",
              "   0.003406664971927447,\n",
              "   0.0034885007522201966,\n",
              "   0.0035621301836467217,\n",
              "   0.0035387478502733366,\n",
              "   0.0035244165254490717,\n",
              "   0.003583449886978737,\n",
              "   0.003583329330597605,\n",
              "   0.0037102516840345095],\n",
              "  'L1 loss (Mask)': [0.3768979864461081,\n",
              "   0.3402090562241418,\n",
              "   0.32412861926215036,\n",
              "   0.3375091999769211,\n",
              "   0.30470854895455496,\n",
              "   0.29069772149835316,\n",
              "   0.3074205496481487,\n",
              "   0.3027712255716324,\n",
              "   0.28948249561446054,\n",
              "   0.2871428983552115,\n",
              "   0.29058245037283215,\n",
              "   0.2899104335478374,\n",
              "   0.2968999913760594,\n",
              "   0.3031529209443501,\n",
              "   0.30725584498473574,\n",
              "   0.3081960997411183,\n",
              "   0.30765366341386524,\n",
              "   0.3100157699414662,\n",
              "   0.3083578220435551,\n",
              "   0.31820134605680195],\n",
              "  'LPIPS (Whole)': [0.04348696821502277,\n",
              "   0.04016242761697088,\n",
              "   0.04048066985394273,\n",
              "   0.03957321335162435,\n",
              "   0.0366002121674163,\n",
              "   0.03578045165964535,\n",
              "   0.038816996450935094,\n",
              "   0.038802530616521835,\n",
              "   0.03466938728732722,\n",
              "   0.03438496815838984,\n",
              "   0.03422973544469902,\n",
              "   0.033528597227164676,\n",
              "   0.034238461405038834,\n",
              "   0.03537915407546929,\n",
              "   0.03655685856938362,\n",
              "   0.037258440894739966,\n",
              "   0.037023471402270455,\n",
              "   0.0382173697331122,\n",
              "   0.038435274469000955,\n",
              "   0.03965272302074092]})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "train_evaluate(device, train_dataset, valid_dataset, TRAINING_PARAMS, METRICS, start_epoch = 1, log_wandb = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OrCk_e1oU44V"
      },
      "outputs": [],
      "source": [
        "# train_epoch(device, DataLoader(train_dataset, batch_size = 16), TRAINING_PARAMS, METRICS, log_wandb = True)\n",
        "# evaluate_epoch(device, DataLoader(valid_dataset, batch_size = 16),, TRAINING_PARAMS, metrics, log_wandb = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ContrastiveGAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
