{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H98D0BHAEioA"
      },
      "source": [
        "# Initialize notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lpips\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchmetrics\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/home/e/e0425222/CS4243-project\")\n",
        "from utils.dataset_utils.AnimalDataset import AnimalDataset\n",
        "from utils.train_utils.train_utils import sample_batch, summary\n",
        "from utils.train_utils.model_utils import Conv2dBlock, GatedConv2dBlock, GatedUpConv2dBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pKzaTBy__baX"
      },
      "outputs": [],
      "source": [
        "train_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_train.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n",
        "\n",
        "valid_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_val.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n",
        "\n",
        "test_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_test.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # sanity check \n",
        "# sample_batch(train_dataset, sample_size = 6)\n",
        "# sample_batch(valid_dataset, sample_size = 6)\n",
        "# sample_batch(test_dataset, sample_size = 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouUSbxVkE92G"
      },
      "source": [
        "# Introduction\n",
        "This notebook aims to explore how contrastive learning can improve GAN performance for image inpainting. References are: \n",
        "1. Siamese Neural Networks for One-shot Image Recognition\n",
        "2. SiGAN: Siamese Generative Adversarial Network for Identity-Preserving Face Hallucination\n",
        "3. Free-Form Image Inpainting via Contrastive Attention Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oL0U4VbFaeJ"
      },
      "source": [
        "# Model experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "h2hEL0kCFbt_"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, activation):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        # same -> downsample -> same -> downsample\n",
        "        self.conv0 = GatedConv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv1 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv2 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv3 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # 2 x same conv\n",
        "        self.conv4 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv5 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # 4 x dilated conv\n",
        "        self.conv6 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 2, dilation = 2, activation = activation)\n",
        "        self.conv7 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 4, dilation = 4, activation = activation)\n",
        "        self.conv8 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 8, dilation = 8, activation = activation)\n",
        "        self.conv9 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 16, dilation = 16, activation = activation)\n",
        "\n",
        "        # 2 x same conv\n",
        "        self.conv10 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv11 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # upsample -> same -> upsample -> same\n",
        "        self.conv12 = GatedUpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
        "        self.conv13 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv14 = GatedUpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
        "        self.conv15 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # final\n",
        "        self.final = nn.Conv2d(hidden_dim, output_dim, kernel_size = 3, stride = 1, padding = 'same')\n",
        "\n",
        "        # for contrastive learning, 1x1 conv to compress feature map into 1 channel\n",
        "        self.conv_feature = nn.Conv2d(hidden_dim, 1, kernel_size = 1, stride = 1, padding = 'same')\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "\n",
        "        x = self.conv0(input_tensor)\n",
        "        \n",
        "        # downsample\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # middle layers\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        # dilated conv with residual skips\n",
        "        x = self.conv6(x) + x\n",
        "        x = self.conv7(x) + x\n",
        "        x = self.conv8(x) + x\n",
        "        x = self.conv9(x) + x\n",
        "\n",
        "        # extract for contrastive loss\n",
        "        x_feature = self.conv_feature(x)\n",
        "\n",
        "        # middle layers\n",
        "        x = self.conv10(x)\n",
        "        x = self.conv11(x)\n",
        "\n",
        "        # upsample\n",
        "        x = self.conv12(x)\n",
        "        x = self.conv13(x)\n",
        "        x = self.conv14(x)\n",
        "        x = self.conv15(x)\n",
        "        \n",
        "        # final\n",
        "        x = self.final(x)\n",
        "\n",
        "        return x, x_feature\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, input_shape, activation):\n",
        "\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # 5 layers down\n",
        "        self.conv0 = Conv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv1 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv2 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv3 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv4 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # linear to predict classes\n",
        "        LATENT_H = input_shape//(2**5)\n",
        "        self.linear = nn.Linear(LATENT_H**2 * hidden_dim, 1)\n",
        "\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \n",
        "        x = self.conv0(input_tensor)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        # scores\n",
        "        x = torch.flatten(x, start_dim = 1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSjQKMf6EmH2"
      },
      "source": [
        "# Training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "K26yI7ABEVmI"
      },
      "outputs": [],
      "source": [
        "MODEL_PARAMS = {\n",
        "    \"description\" : \"\"\"\n",
        "        Base structure same as the GLCIC model. Gated convolutions replace all convolutions in the generator, \n",
        "        and a 1x1 convolution is used to flatten the output of the 4th dilated convolution layer. \n",
        "        This is the latent vector used for contrastive learning.\"\"\",\n",
        "    \"hidden_dim\" : 64,\n",
        "    \"activation\" : nn.Mish,\n",
        "}\n",
        "\n",
        "SAVE_PATHS = {\n",
        "    \"generator1\" : \"/home/e/e0425222/CS4243-project/active_experiments/ContrastiveGAN/generator1/generator1\",\n",
        "    \"generator2\" : \"/home/e/e0425222/CS4243-project/active_experiments/ContrastiveGAN/generator2/generator2\",\n",
        "    \"discriminator\" : \"/home/e/e0425222/CS4243-project/active_experiments/ContrastiveGAN/discriminator/discriminator\"\n",
        "}\n",
        "\n",
        "TRAINING_PARAMS = {\n",
        "    \"num_epochs\" : 20,\n",
        "    \"batch_size\" : 128, \n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"alpha\": 0.1,\n",
        "    \"beta\" : 0.01,\n",
        "    \"ADAM_betas\" : (0.5, 0.999),\n",
        "    \"schedule_every\" : 1,\n",
        "    \"sample_size\" : 16,\n",
        "    \"log_every\" : 10,\n",
        "    \"save_paths\" : SAVE_PATHS,\n",
        "}\n",
        "\n",
        "LOGGING_CONFIG = {\n",
        "    \"model_params\" : MODEL_PARAMS,\n",
        "}\n",
        "LOGGING_CONFIG.update(TRAINING_PARAMS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UEXbNHjGvaE"
      },
      "source": [
        "# Experiment intialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbIsqCWTGvGE",
        "outputId": "a1df7c8d-a350-4914-ce06-e4b0da2b8599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model has 1.11642 million parameters\n",
            "model has 1.11642 million parameters\n",
            "model has 0.150401 million parameters\n",
            "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
            "Loading model from: /home/e/e0425222/miniconda3/envs/env/lib/python3.7/site-packages/lpips/weights/v0.1/vgg.pth\n"
          ]
        }
      ],
      "source": [
        "# 1. initialize model\n",
        "generator1 = Generator(input_dim = 4, hidden_dim = MODEL_PARAMS[\"hidden_dim\"], output_dim = 3, activation = MODEL_PARAMS[\"activation\"])\n",
        "generator2 = Generator(input_dim = 4, hidden_dim = MODEL_PARAMS[\"hidden_dim\"], output_dim = 3, activation = MODEL_PARAMS[\"activation\"])\n",
        "discriminator = Discriminator(input_dim = 3, hidden_dim = MODEL_PARAMS[\"hidden_dim\"], input_shape = 64, activation = MODEL_PARAMS[\"activation\"])\n",
        "\n",
        "summary(generator1)\n",
        "summary(generator2)\n",
        "summary(discriminator)\n",
        "\n",
        "# 2. device\n",
        "parallel = True\n",
        "device = 'cuda:0'  \n",
        "devices = [0,1,2,3]\n",
        "\n",
        "if not parallel:\n",
        "    generator1 = generator1.to(device)\n",
        "    generator2 = generator2.to(device)\n",
        "    discriminator = discriminator.to(device)\n",
        "else:\n",
        "    generator1 = nn.DataParallel(generator1, device_ids = devices)\n",
        "    generator2 = nn.DataParallel(generator2, device_ids = devices)\n",
        "    discriminator = nn.DataParallel(discriminator, device_ids = devices)\n",
        "\n",
        "# 3. initialize loss functions\n",
        "recon_loss_function = lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum()\n",
        "contrastive_loss_function_same = lambda x1, x2 : nn.functional.mse_loss(x1, x2)\n",
        "discriminator_loss_function = nn.functional.binary_cross_entropy_with_logits\n",
        "\n",
        "# 4. initialize metrics\n",
        "VGG_LPIPS = lpips.LPIPS(net = 'vgg').to(device)\n",
        "METRICS = {\n",
        "    \"Peak SnR (Whole)\" : lambda img, gt, mask : torchmetrics.functional.peak_signal_noise_ratio(img * (1-mask) + gt * mask, gt),\n",
        "    \"L2 loss (Whole)\" : lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask) + gt * mask, gt),\n",
        "    \"L2 loss (Mask)\" : lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum(),\n",
        "    \"L1 loss (Whole)\" : lambda img, gt, mask : nn.functional.l1_loss(img * (1-mask) + gt * mask, gt),\n",
        "    \"L1 loss (Mask)\" : lambda img, gt, mask : nn.functional.l1_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum(),\n",
        "    \"LPIPS (Whole)\" : (lambda img, gt, mask : VGG_LPIPS(img * (1-mask) + gt * mask, gt).mean()),\n",
        "}\n",
        "\n",
        "\n",
        "# 5. initialize optimizers\n",
        "generator1_optimizer = torch.optim.Adam(generator1.parameters(), lr = TRAINING_PARAMS[\"learning_rate\"], betas = TRAINING_PARAMS[\"ADAM_betas\"])\n",
        "generator1_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(generator1_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
        "\n",
        "generator2_optimizer = torch.optim.Adam(generator2.parameters(), lr = TRAINING_PARAMS[\"learning_rate\"], betas = TRAINING_PARAMS[\"ADAM_betas\"])\n",
        "generator2_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(generator2_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
        "\n",
        "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr = TRAINING_PARAMS[\"learning_rate\"], betas = TRAINING_PARAMS[\"ADAM_betas\"])\n",
        "discriminator_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(discriminator_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
        "\n",
        "# 6. wrap into training dictionary\n",
        "TRAINING_PARAMS[\"generator1_model\"] = generator1\n",
        "TRAINING_PARAMS[\"generator1_optimizer\"] = generator1_optimizer\n",
        "TRAINING_PARAMS[\"generator1_scheduler\"] = generator1_scheduler\n",
        "TRAINING_PARAMS[\"generator2_model\"] = generator2\n",
        "TRAINING_PARAMS[\"generator2_optimizer\"] = generator2_optimizer\n",
        "TRAINING_PARAMS[\"generator2_scheduler\"] = generator2_scheduler\n",
        "TRAINING_PARAMS[\"discriminator_model\"] = discriminator\n",
        "TRAINING_PARAMS[\"discriminator_optimizer\"] = discriminator_optimizer\n",
        "TRAINING_PARAMS[\"discriminator_scheduler\"] = discriminator_scheduler\n",
        "TRAINING_PARAMS[\"discriminator_loss_function\"] = discriminator_loss_function\n",
        "TRAINING_PARAMS[\"contrastive_loss_function\"] = contrastive_loss_function_same\n",
        "TRAINING_PARAMS[\"recon_loss_function\"] = recon_loss_function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intialize logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "6rbEmO4xoVR4",
        "outputId": "75c4f786-f5b7-42de-b20d-5e6f78eaaccf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:1xtu83t2) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0d5bbe638c74f4ca79fe95a28bad0d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='24.445 MB of 24.445 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>L1 loss (Mask)</td><td>█▅▄▃▃▃▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>L1 loss (Whole)</td><td>█▅▄▃▃▃▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>L2 loss (Mask)</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▂▂</td></tr><tr><td>L2 loss (Whole)</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▂▂</td></tr><tr><td>LPIPS (Whole)</td><td>█▆▅▄▃▃▃▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Peak SnR (Whole)</td><td>▁▃▄▅▅▆▆█████████</td></tr><tr><td>eval_L1 loss (Mask)</td><td>▇▆▇▇█▇█▁▂▂▂▂▂▂</td></tr><tr><td>eval_L1 loss (Whole)</td><td>█▆▇▇▇██▁▂▂▂▂▂▂</td></tr><tr><td>eval_L2 loss (Mask)</td><td>▇▆▇▇███▁▂▂▂▂▂▂</td></tr><tr><td>eval_L2 loss (Whole)</td><td>▇▆▇████▁▂▂▂▂▂▂</td></tr><tr><td>eval_LPIPS (Whole)</td><td>█▅▂▄▄▄▃▃█▅▅▃▂▁</td></tr><tr><td>eval_Peak SnR (Whole)</td><td>▂▃▂▁▁▁▁█▇▇▇▇▇▇</td></tr><tr><td>loss_contrastive</td><td>██▇▇▆▆▅▂▂▂▂▂▂▂▁▁</td></tr><tr><td>loss_discriminator</td><td>▂▂▅▇███▅▄▄▄▃▃▃▁▁</td></tr><tr><td>loss_generator1</td><td>█▅▄▃▃▃▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_generator1_adv</td><td>▁▁▅▅▅▆▆▆▇▆▆▆▆▇█▇</td></tr><tr><td>loss_generator1_r</td><td>█▅▄▃▃▃▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_generator2</td><td>█▅▄▃▃▃▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_generator2_adv</td><td>▁▂▅▅▅▆▆▆▇▆▆▆▆▆█▇</td></tr><tr><td>loss_generator2_r</td><td>█▅▄▃▃▃▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_discriminator</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_generator1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_generator2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>L1 loss (Mask)</td><td>0.92589</td></tr><tr><td>L1 loss (Whole)</td><td>0.01072</td></tr><tr><td>L2 loss (Mask)</td><td>0.69225</td></tr><tr><td>L2 loss (Whole)</td><td>0.00801</td></tr><tr><td>LPIPS (Whole)</td><td>0.08285</td></tr><tr><td>Peak SnR (Whole)</td><td>24.43685</td></tr><tr><td>eval_L1 loss (Mask)</td><td>0.36516</td></tr><tr><td>eval_L1 loss (Whole)</td><td>0.00418</td></tr><tr><td>eval_L2 loss (Mask)</td><td>0.07375</td></tr><tr><td>eval_L2 loss (Whole)</td><td>0.00085</td></tr><tr><td>eval_LPIPS (Whole)</td><td>0.0435</td></tr><tr><td>eval_Peak SnR (Whole)</td><td>30.72326</td></tr><tr><td>loss_contrastive</td><td>0.3599</td></tr><tr><td>loss_discriminator</td><td>0.13848</td></tr><tr><td>loss_generator1</td><td>0.14801</td></tr><tr><td>loss_generator1_adv</td><td>0.70455</td></tr><tr><td>loss_generator1_r</td><td>0.07395</td></tr><tr><td>loss_generator2</td><td>0.14727</td></tr><tr><td>loss_generator2_adv</td><td>0.70445</td></tr><tr><td>loss_generator2_r</td><td>0.07324</td></tr><tr><td>lr_discriminator</td><td>0.0001</td></tr><tr><td>lr_generator1</td><td>0.0001</td></tr><tr><td>lr_generator2</td><td>0.0001</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">different-pond-25</strong>: <a href=\"https://wandb.ai/cs4243_project/ContrastiveGAN/runs/1xtu83t2\" target=\"_blank\">https://wandb.ai/cs4243_project/ContrastiveGAN/runs/1xtu83t2</a><br/>Synced 6 W&B file(s), 60 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220412_144301-1xtu83t2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:1xtu83t2). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.14 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/e/e0425222/CS4243-project/active_experiments/ContrastiveGAN/wandb/run-20220412_145455-kic543e3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs4243_project/ContrastiveGAN/runs/kic543e3\" target=\"_blank\">worthy-night-26</a></strong> to <a href=\"https://wandb.ai/cs4243_project/ContrastiveGAN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(project=\"ContrastiveGAN\", entity=\"cs4243_project\")\n",
        "wandb.config = LOGGING_CONFIG\n",
        "\n",
        "wandb.watch(\n",
        "    (generator1, generator2, discriminator),\n",
        "    criterion = None,\n",
        "    log = 'all',\n",
        "    log_freq = 1,\n",
        "    idx = 0, \n",
        "    log_graph = False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsM_LVhrKiE8"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIMfSPuZaJpQ"
      },
      "source": [
        "## Train functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QFJP7gjJSGFH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_epoch(device, train_dataloader, training_params : dict, metrics : dict, log_wandb = True):\n",
        "    \n",
        "    # ===== INITIALIZE =====\n",
        "    # constants\n",
        "    RECONSTRUCTION_LOSS = training_params[\"recon_loss_function\"]\n",
        "    DISCRIMINATOR_LOSS_FUNCTION = training_params[\"discriminator_loss_function\"]\n",
        "    CONTRASTIVE_LOSS_FUNCTION = training_params[\"contrastive_loss_function\"]\n",
        "\n",
        "    GENERATOR1_OPTIMIZER = training_params[\"generator1_optimizer\"]\n",
        "    GENERATOR2_OPTIMIZER = training_params[\"generator2_optimizer\"]\n",
        "    DISCRIMINATOR_OPTIMIZER = training_params[\"discriminator_optimizer\"]\n",
        "\n",
        "    BATCH_EVALUATE_EVERY = 5\n",
        "    LOG_EVERY = training_params[\"log_every\"]\n",
        "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
        "    BATCH_SIZE = training_params[\"batch_size\"]\n",
        "    ALPHA = training_params[\"alpha\"]\n",
        "    BETA = training_params[\"beta\"]\n",
        "\n",
        "    # models\n",
        "    generator1 = training_params[\"generator1_model\"].to(device).train()\n",
        "    generator2 = training_params[\"generator2_model\"].to(device).train()\n",
        "    discriminator = training_params[\"discriminator_model\"].to(device).train()\n",
        "\n",
        "    # epoch metrics\n",
        "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
        "    running_results[\"loss_discriminator\"] = 0.0\n",
        "    running_results[\"loss_generator1\"] = 0.0\n",
        "    running_results[\"loss_generator1_r\"] = 0.0\n",
        "    running_results[\"loss_generator1_adv\"] = 0.0\n",
        "    running_results[\"loss_generator2\"] = 0.0\n",
        "    running_results[\"loss_generator2_r\"] = 0.0\n",
        "    running_results[\"loss_generator2_adv\"] = 0.0\n",
        "    running_results[\"loss_contrastive\"] = 0.0\n",
        "\n",
        "    # ===== TRAIN EPOCH =====\n",
        "    num_batches = 0\n",
        "    for _, batch in enumerate(train_dataloader, 1):\n",
        "\n",
        "            # ===== INITIALIZE =====\n",
        "            num_batches += 1\n",
        "\n",
        "            # input and ground truth\n",
        "            input_batched = batch[\"image\"]\n",
        "            ground_truth_batched = batch[\"reconstructed\"]\n",
        "            mask_batched = batch[\"mask\"]\n",
        "\n",
        "            # sanity check\n",
        "            assert input_batched.shape[0] == ground_truth_batched.shape[0]\n",
        "\n",
        "            # move tensors to device\n",
        "            input_batched = input_batched.to(device)\n",
        "            ground_truth_batched = ground_truth_batched.to(device)\n",
        "            mask_batched = mask_batched.to(device)\n",
        "\n",
        "            # set the gradients to zeros\n",
        "            GENERATOR1_OPTIMIZER.zero_grad()\n",
        "            GENERATOR2_OPTIMIZER.zero_grad()\n",
        "            DISCRIMINATOR_OPTIMIZER.zero_grad()\n",
        "\n",
        "            # reshape to channel first\n",
        "            input_batched = input_batched.permute(0, 3, 1, 2)\n",
        "            ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
        "            mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
        "\n",
        "            # ===== FORWARD PASS =====\n",
        "\n",
        "            # 1. train discriminator\n",
        "            \n",
        "            # 1.1 generate images\n",
        "            input_batched.requires_grad_()\n",
        "            output1_batched, _ = generator1(input_batched)\n",
        "            shuffled_indices = torch.randperm(n = BATCH_SIZE)\n",
        "            output2_batched, _ = generator2(input_batched[shuffled_indices]) # random permutation\n",
        "\n",
        "            # 1.2 splice with ground truth\n",
        "            spliced1_batched = ((1-mask_batched) * output1_batched) + (mask_batched * ground_truth_batched) \n",
        "            spliced2_batched = ((1-mask_batched[shuffled_indices]) * output2_batched) + (mask_batched[shuffled_indices] * ground_truth_batched[shuffled_indices]) \n",
        "\n",
        "            # 1.3 feed into discriminator\n",
        "            label_real = torch.ones(BATCH_SIZE * 2, 1).to(device)\n",
        "            label_fake = torch.zeros(BATCH_SIZE * 2, 1).to(device)\n",
        "\n",
        "            pred_real = discriminator(torch.cat([ground_truth_batched, ground_truth_batched[shuffled_indices]], dim = 0))\n",
        "            loss_real = DISCRIMINATOR_LOSS_FUNCTION(pred_real, label_real)\n",
        "\n",
        "            pred_fake = discriminator(torch.cat([spliced1_batched, spliced2_batched[shuffled_indices]], dim = 0))\n",
        "            loss_fake = DISCRIMINATOR_LOSS_FUNCTION(pred_fake, label_fake)\n",
        "            loss_d = ALPHA * (loss_real + loss_fake)\n",
        "            loss_d.backward()\n",
        "            DISCRIMINATOR_OPTIMIZER.step()\n",
        "\n",
        "            # 2. train generator 1 (reconstruction, adverserial, contrastive)\n",
        "            # 2.1 forward pass by generator to produce images, splice them\n",
        "            output1_batched, _ = generator1(input_batched)\n",
        "            spliced1_batched = ((1-mask_batched) * output1_batched) + (mask_batched * ground_truth_batched) \n",
        "            \n",
        "            # 2.2 reconstruction loss\n",
        "            loss_g1_reconstruction = RECONSTRUCTION_LOSS(output1_batched, ground_truth_batched, mask_batched)\n",
        "\n",
        "            # 2.3 adverserial loss\n",
        "            label_real = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "            pred1_adverserial = discriminator(spliced1_batched)\n",
        "            loss_g1_adverserial = DISCRIMINATOR_LOSS_FUNCTION(pred1_adverserial, label_real) # want it to classify all as real\n",
        "\n",
        "            # 2.4 contrastive loss\n",
        "            shuffled_indices = torch.randperm(n = BATCH_SIZE)\n",
        "            copied_input_batched = torch.cat([ground_truth_batched * mask_batched[shuffled_indices], mask_batched[shuffled_indices]], dim = 1)\n",
        "            _, x1 = generator1(input_batched)\n",
        "            _, x2 = generator2(copied_input_batched) # same image, different masks\n",
        "            loss_contrastive1 = CONTRASTIVE_LOSS_FUNCTION(x1, x2)\n",
        "\n",
        "            # 2.5 backprop\n",
        "            loss_g1 = loss_g1_reconstruction + ALPHA * loss_g1_adverserial + BETA * loss_contrastive1\n",
        "            loss_g1.backward()\n",
        "            GENERATOR1_OPTIMIZER.step()\n",
        "\n",
        "            # 3.  train generator 2 (reconstruction, adverserial, contrastive)\n",
        "            # 3.1 forward pass by generator to produce images, reconstruction loss\n",
        "            output2_batched, _ = generator2(input_batched)\n",
        "            spliced2_batched = ((1-mask_batched) * output2_batched) + (mask_batched * ground_truth_batched) \n",
        "            loss_g2_reconstruction = RECONSTRUCTION_LOSS(output2_batched, ground_truth_batched, mask_batched)\n",
        "\n",
        "            # 3.2 adverserial loss\n",
        "            label_real = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "            pred2_adverserial = discriminator(spliced2_batched)\n",
        "            loss_g2_adverserial = DISCRIMINATOR_LOSS_FUNCTION(pred2_adverserial, label_real) # want it to classify all as real\n",
        "\n",
        "            # 3.3 contrastive loss\n",
        "            shuffled_indices = torch.randperm(n = BATCH_SIZE)\n",
        "            copied_input_batched = torch.cat([ground_truth_batched * mask_batched[shuffled_indices], mask_batched[shuffled_indices]], dim = 1)\n",
        "            _, x1 = generator1(input_batched)\n",
        "            _, x2 = generator2(copied_input_batched) # same image, different masks\n",
        "            loss_contrastive2 = CONTRASTIVE_LOSS_FUNCTION(x1, x2)\n",
        "\n",
        "            # 3.4 backprop\n",
        "            loss_g2 = loss_g2_reconstruction + ALPHA * loss_g2_adverserial + BETA * loss_contrastive2\n",
        "            loss_g2.backward()\n",
        "            GENERATOR2_OPTIMIZER.step()\n",
        "\n",
        "\n",
        "            # ===== COMPUTE STATISTICS, USING TORCH METRICS =====  \n",
        "            # 1. compute losses\n",
        "\n",
        "            running_results[\"loss_generator1\"] += loss_g1.detach().item()\n",
        "            running_results[\"loss_generator1_r\"] += loss_g1_reconstruction.detach().item()\n",
        "            running_results[\"loss_generator1_adv\"] += loss_g1_adverserial.detach().item()\n",
        "            running_results[\"loss_generator2\"] += loss_g2.detach().item()\n",
        "            running_results[\"loss_generator2_r\"] += loss_g2_reconstruction.detach().item()\n",
        "            running_results[\"loss_generator2_adv\"] += loss_g2_adverserial.detach().item()\n",
        "            running_results[\"loss_contrastive\"] += (loss_contrastive1.detach().item() + loss_contrastive2.detach().item())/2\n",
        "            running_results[\"loss_discriminator\"] += loss_d.detach().item()\n",
        "\n",
        "            # 2. for each key, compute, add item to results dictionary (take average of 2 generators)\n",
        "            for key, func in metrics.items():\n",
        "                res1 = func(output1_batched, ground_truth_batched, mask_batched).detach().item()\n",
        "                res2 = func(output2_batched, ground_truth_batched, mask_batched).detach().item()\n",
        "                running_results[key] += (res1 + res2)/2\n",
        "\n",
        "            # 3. log with wandb\n",
        "            if log_wandb and (num_batches % LOG_EVERY == 0):\n",
        "\n",
        "                # generator 1\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced1_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images1 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "                \n",
        "                # generator 2\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced2_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images2 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "\n",
        "                # log images and some metadata\n",
        "                wandb.log( {\n",
        "                    \"generator1_train_images\" : images1,\n",
        "                    \"generator2_train_images\" : images2,\n",
        "                    \"lr_generator1\" : GENERATOR1_OPTIMIZER.param_groups[0]['lr'],\n",
        "                    \"lr_generator2\" : GENERATOR2_OPTIMIZER.param_groups[0]['lr'],\n",
        "                    \"lr_discriminator\" : DISCRIMINATOR_OPTIMIZER.param_groups[0]['lr']\n",
        "\n",
        "                })\n",
        "\n",
        "                # log all metrics\n",
        "                wandb.log(\n",
        "                    {key : item/num_batches for key, item in running_results.items()}\n",
        "                )\n",
        "        \n",
        "\n",
        "            # ===== HOUSEKEEPING =====\n",
        "            del loss_g2\n",
        "            del loss_g1\n",
        "            del loss_d\n",
        "            del input_batched\n",
        "\n",
        "            # print results every some batches\n",
        "            if num_batches % BATCH_EVALUATE_EVERY == 0: \n",
        "\n",
        "                args = \"\"\n",
        "                for key, val in running_results.items():\n",
        "                    args += key + \": \" + str(running_results[key]/num_batches) + \"   \"\n",
        "                print(f\"\\r{num_batches}/{len(train_dataloader)}: \" + args, end = '', flush = True)\n",
        "\n",
        "    # normalise numbers by batch\n",
        "    for key, val in running_results.items():\n",
        "        running_results[key] /= num_batches\n",
        "\n",
        "    return running_results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4CuiHVAjKitr"
      },
      "outputs": [],
      "source": [
        "def evaluate_epoch(device, validation_dataloader, training_params : dict, metrics : dict, log_wandb = True):\n",
        "\n",
        "    # ===== INITIALIZE =====\n",
        "    # models\n",
        "    generator1 = training_params[\"generator1_model\"].to(device).eval()\n",
        "    generator2 = training_params[\"generator2_model\"].to(device).eval()\n",
        "\n",
        "    # constants\n",
        "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
        "\n",
        "    # epoch statistics\n",
        "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
        "\n",
        "    # ===== EVALUATE EPOCH =====\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batches = 0\n",
        "        for index, batch in enumerate(validation_dataloader, 1):\n",
        "            \n",
        "            batches += 1\n",
        "\n",
        "            # input and ground truth\n",
        "            input_batched = batch[\"image\"]\n",
        "            ground_truth_batched = batch[\"reconstructed\"]\n",
        "            mask_batched = batch[\"mask\"]\n",
        "\n",
        "            # move tensors to device\n",
        "            input_batched = input_batched.to(device)\n",
        "            ground_truth_batched = ground_truth_batched.to(device)\n",
        "            mask_batched = mask_batched.to(device)\n",
        "\n",
        "            # ===== FORWARD PASS =====\n",
        "\n",
        "            # 1. reshape to channel first\n",
        "            input_batched = input_batched.permute(0, 3, 1, 2)\n",
        "            ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
        "            mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
        "\n",
        "            # 2. predict    \n",
        "            output1_batched, _ = generator1(input_batched)\n",
        "            output2_batched, _ = generator2(input_batched)           \n",
        "            spliced1_batched = ((1-mask_batched) * output1_batched) + (mask_batched * ground_truth_batched) \n",
        "            spliced2_batched = ((1-mask_batched) * output2_batched) + (mask_batched * ground_truth_batched) \n",
        "\n",
        "            # 3. evaluate\n",
        "            for key, func in metrics.items():\n",
        "                running_results[key] += (func(output1_batched, ground_truth_batched, mask_batched).detach().item() + func(output2_batched, ground_truth_batched, mask_batched).detach().item())/2\n",
        "\n",
        "            args = \"\"\n",
        "            for key, val in running_results.items():\n",
        "                args += key + \": \" + str(running_results[key]/batches) + \"   \"\n",
        "            print(f\"\\r{batches}/{len(validation_dataloader)}: \" + args, end = '', flush = True)\n",
        "\n",
        "            # 4. log \n",
        "            if log_wandb:\n",
        "\n",
        "                # generator 1\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced1_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images1 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "                \n",
        "                # generator 2\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced2_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images2 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "\n",
        "                # log images and some metadata\n",
        "                wandb.log( {\n",
        "                    \"generator1_val_images\" : images1,\n",
        "                    \"generator2_val_images\" : images2\n",
        "                })\n",
        "\n",
        "                # log all metrics\n",
        "                wandb.log(\n",
        "                    {f\"eval_{key}\" : item/batches for key, item in running_results.items()}\n",
        "                )\n",
        "\n",
        "    # normalise numbers by batch\n",
        "    for key, val in running_results.items():\n",
        "        running_results[key] /= batches\n",
        "\n",
        "    return running_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "O_mQDSLXaPdz"
      },
      "outputs": [],
      "source": [
        "def train_evaluate(device, train_dataset, validation_dataset, training_params: dict, metrics: dict, start_epoch = 0, log_wandb = True):\n",
        "\n",
        "    # ===== INITIALIZE =====\n",
        "    # constants\n",
        "    NUM_EPOCHS = training_params[\"num_epochs\"]\n",
        "    BATCH_SIZE = training_params[\"batch_size\"]\n",
        "    GENERATOR1_SCHEDULER = training_params[\"generator1_scheduler\"]\n",
        "    GENERATOR2_SCHEDULER = training_params[\"generator2_scheduler\"]\n",
        "    DISCRIMINATOR_SCHEDULER = training_params[\"discriminator_scheduler\"]\n",
        "    SAVE_PATHS = training_params[\"save_paths\"]\n",
        "    NUM_WORKERS = 2\n",
        "    START_EPOCH = start_epoch\n",
        "\n",
        "    # models for saving\n",
        "    generator1 = training_params[\"generator1_model\"].to(device).train()\n",
        "    generator2 = training_params[\"generator2_model\"].to(device).train()\n",
        "    discriminator = training_params[\"discriminator_model\"].to(device).train()\n",
        "\n",
        "    # variable losses\n",
        "    train_results = {list(metrics.keys())[i] : [] for i in range(len(metrics)) } \n",
        "\n",
        "    train_results[\"loss_discriminator\"] = []\n",
        "    train_results[\"loss_generator1\"] = []\n",
        "    train_results[\"loss_generator1_r\"] = []\n",
        "    train_results[\"loss_generator1_adv\"] = []\n",
        "    train_results[\"loss_generator2\"] = []\n",
        "    train_results[\"loss_generator2_r\"] = []\n",
        "    train_results[\"loss_generator2_adv\"] = []\n",
        "    train_results[\"loss_contrastive\"] = []\n",
        "\n",
        "    eval_results = {list(metrics.keys())[i] : [] for i in range(len(metrics)) } \n",
        "\n",
        "    # dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS, drop_last = True)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS, drop_last = True)\n",
        "\n",
        "    # ===== TRAIN =====\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # train\n",
        "        print(f\"\\n===== Epoch: {START_EPOCH + epoch + 1} ===== \")\n",
        "        num_batches = 0\n",
        "\n",
        "        # train every epoch\n",
        "        results = train_epoch(device, train_dataloader, training_params, metrics, log_wandb = log_wandb)\n",
        "        for key, val in results.items():\n",
        "            train_results[key].append(val)\n",
        "\n",
        "        # evaluate every epoch\n",
        "        print()\n",
        "        results = evaluate_epoch(device, validation_dataloader, training_params, metrics, log_wandb = log_wandb)\n",
        "        for key, val in results.items():\n",
        "            eval_results[key].append(val)\n",
        "\n",
        "        # ===== EPOCH RESULTS =====\n",
        "        print(f\"\\nCompleted epoch {START_EPOCH + epoch + 1}! Took {(time.time() - start)/60} min\")\n",
        "\n",
        "        # ===== HOUSEKEEPING =====\n",
        "\n",
        "        # scheduler every epoch\n",
        "        if DISCRIMINATOR_SCHEDULER is not None:\n",
        "            DISCRIMINATOR_SCHEDULER.step(eval_results[\"L2 loss (Mask)\"][epoch])\n",
        "        if GENERATOR1_SCHEDULER is not None:\n",
        "            GENERATOR1_SCHEDULER.step(eval_results[\"L2 loss (Mask)\"][epoch])\n",
        "        if GENERATOR2_SCHEDULER is not None:\n",
        "            GENERATOR2_SCHEDULER.step(eval_results[\"L2 loss (Mask)\"][epoch])\n",
        "            \n",
        "        # save every epoch\n",
        "        SAVE = f\"{SAVE_PATHS['generator1']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
        "        torch.save(generator1.state_dict(), SAVE)\n",
        "        SAVE = f\"{SAVE_PATHS['generator2']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
        "        torch.save(generator2.state_dict(), SAVE)\n",
        "        SAVE = f\"{SAVE_PATHS['discriminator']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
        "        torch.save(discriminator.state_dict(), SAVE)  \n",
        "\n",
        "        print(\"Saved models!\")\n",
        "\n",
        "    return train_results, eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8IkxVhsVsf4"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_IuN9J8Jfp9j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Epoch: 1 ===== \n",
            "75/78: Peak SnR (Whole): 29.69761454264323   L2 loss (Whole): 0.0010840763822974016   L2 loss (Mask): 0.09360502238074939   L1 loss (Whole): 0.004824343187113603   L1 loss (Mask): 0.4166347036759059   LPIPS (Whole): 0.04652049817144871   loss_discriminator: 0.13929161647955576   loss_generator1: 0.16685781478881836   loss_generator1_r: 0.08998176654179892   loss_generator1_adv: 0.7000286650657653   loss_generator2: 0.17413731396198273   loss_generator2_r: 0.09722827821969986   loss_generator2_adv: 0.7004667178789774   loss_contrastive: 0.6867771184444428     \n",
            "7/7: Peak SnR (Whole): 29.981473922729492   L2 loss (Whole): 0.001026145860253434   L2 loss (Mask): 0.0886506416967937   L1 loss (Whole): 0.004660572790141616   L1 loss (Mask): 0.4027462771960667   LPIPS (Whole): 0.04408731896962438      \n",
            "Completed epoch 1! Took 5.056918668746948 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 2 ===== \n",
            "75/78: Peak SnR (Whole): 30.621152381896973   L2 loss (Whole): 0.0008680199338899305   L2 loss (Mask): 0.07525804037849108   L1 loss (Whole): 0.004258692832663656   L1 loss (Mask): 0.3692718901236852   LPIPS (Whole): 0.043613181809584296   loss_discriminator: 0.13854588468869528   loss_generator1: 0.14738649626572928   loss_generator1_r: 0.07328368365764618   loss_generator1_adv: 0.7008272759119669   loss_generator2: 0.15136438608169556   loss_generator2_r: 0.07723239709933599   loss_generator2_adv: 0.7011486252148946   loss_contrastive: 0.4018604701757431      \n",
            "7/7: Peak SnR (Whole): 30.3375825881958   L2 loss (Whole): 0.0009567648729509008   L2 loss (Mask): 0.08269774434821946   L1 loss (Whole): 0.004529084444844297   L1 loss (Mask): 0.3913977359022413   LPIPS (Whole): 0.04576945331479822   5   \n",
            "Completed epoch 2! Took 4.673006772994995 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 3 ===== \n",
            "75/78: Peak SnR (Whole): 31.073789469401042   L2 loss (Whole): 0.0007825771137140691   L2 loss (Mask): 0.06789596964915594   L1 loss (Whole): 0.004023945804995795   L1 loss (Mask): 0.34910107572873433   LPIPS (Whole): 0.04296805108586947   loss_discriminator: 0.13730297684669496   loss_generator1: 0.1408478158712387   loss_generator1_r: 0.06680226996541024   loss_generator1_adv: 0.7130644655227661   loss_generator2: 0.14306883931159972   loss_generator2_r: 0.06898966933290164   loss_generator2_adv: 0.7135058410962423   loss_contrastive: 0.2733841206630071     \n",
            "7/7: Peak SnR (Whole): 30.881874629429408   L2 loss (Whole): 0.0008198646579070815   L2 loss (Mask): 0.07186684278505188   L1 loss (Whole): 0.004111947258934379   L1 loss (Mask): 0.3604250933442797   LPIPS (Whole): 0.043379651116473336     \n",
            "Completed epoch 3! Took 4.982959977785746 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 4 ===== \n",
            "75/78: Peak SnR (Whole): 31.268133379618327   L2 loss (Whole): 0.0007477868356121083   L2 loss (Mask): 0.06478742967049281   L1 loss (Whole): 0.003915561228059233   L1 loss (Mask): 0.3392184474070867   LPIPS (Whole): 0.04216606010993322   loss_discriminator: 0.13474456548690797   loss_generator1: 0.13991446495056153   loss_generator1_r: 0.06386887217561404   loss_generator1_adv: 0.7403416283925375   loss_generator2: 0.14190378884474436   loss_generator2_r: 0.06570598716537157   loss_generator2_adv: 0.7418641249338785   loss_contrastive: 0.20114072650671005    \n",
            "7/7: Peak SnR (Whole): 30.920605523245676   L2 loss (Whole): 0.0008198350509961269   L2 loss (Mask): 0.07096128538250923   L1 loss (Whole): 0.004125000293632703   L1 loss (Mask): 0.3570785458598818   LPIPS (Whole): 0.04264771565794945   \n",
            "Completed epoch 4! Took 5.085633095105489 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 5 ===== \n",
            "75/78: Peak SnR (Whole): 31.436634826660157   L2 loss (Whole): 0.0007207245523265253   L2 loss (Mask): 0.06227821705241998   L1 loss (Whole): 0.0038381367037072776   L1 loss (Mask): 0.33167335549990334   LPIPS (Whole): 0.04177523915966352   loss_discriminator: 0.13131623884042104   loss_generator1: 0.13867208649714788   loss_generator1_r: 0.06143935774763425   loss_generator1_adv: 0.7564280676841736   loss_generator2: 0.1437902009487152   loss_generator2_r: 0.0631170763572057   loss_generator2_adv: 0.790834101041158   loss_contrastive: 0.15898177206516265     \n",
            "7/7: Peak SnR (Whole): 30.41080297742571   L2 loss (Whole): 0.0009668183088901319   L2 loss (Mask): 0.08390053840620178   L1 loss (Whole): 0.0045088667954717365   L1 loss (Mask): 0.3913235919816153   LPIPS (Whole): 0.04657690093985626    \n",
            "Completed epoch 5! Took 5.253197753429413 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 6 ===== \n",
            "75/78: Peak SnR (Whole): 31.54161652882894   L2 loss (Whole): 0.0007026423292700202   L2 loss (Mask): 0.06076191355784734   L1 loss (Whole): 0.003769357797379295   L1 loss (Mask): 0.32593359271685285   LPIPS (Whole): 0.04185807752112548   loss_discriminator: 0.13131175180276236   loss_generator1: 0.13274197300275167   loss_generator1_r: 0.05888980487982432   loss_generator1_adv: 0.7255959129333496   loss_generator2: 0.14750238915284475   loss_generator2_r: 0.06263402223587036   loss_generator2_adv: 0.8357819239298503   loss_contrastive: 0.12913747097055117   8   \n",
            "7/7: Peak SnR (Whole): 31.144776480538503   L2 loss (Whole): 0.0007803241439562823   L2 loss (Mask): 0.06766404078475066   L1 loss (Whole): 0.004019379166753164   L1 loss (Mask): 0.3486721153770174   LPIPS (Whole): 0.04278014626886163    \n",
            "Completed epoch 6! Took 4.931849094231923 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 7 ===== \n",
            "75/78: Peak SnR (Whole): 31.601202138264973   L2 loss (Whole): 0.0006928350062419971   L2 loss (Mask): 0.06012111954391003   L1 loss (Whole): 0.0037348767866690956   L1 loss (Mask): 0.3241053940852483   LPIPS (Whole): 0.04295240953564644   loss_discriminator: 0.13021661738554638   loss_generator1: 0.13281854103008905   loss_generator1_r: 0.057246297349532446   loss_generator1_adv: 0.7452938055992127   loss_generator2: 0.14935651510953904   loss_generator2_r: 0.06299594173828761   loss_generator2_adv: 0.8532008290290832   loss_contrastive: 0.10416759108503659   \n",
            "7/7: Peak SnR (Whole): 30.89370482308524   L2 loss (Whole): 0.0008447713127160179   L2 loss (Mask): 0.07276746018656663   L1 loss (Whole): 0.0041801420073690155   L1 loss (Mask): 0.3602063144956316   LPIPS (Whole): 0.042410510193024366   \n",
            "Completed epoch 7! Took 5.00344234307607 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 8 ===== \n",
            "75/78: Peak SnR (Whole): 31.673339131673178   L2 loss (Whole): 0.0006816617837951829   L2 loss (Mask): 0.0590915792932113   L1 loss (Whole): 0.0036853827846546968   L1 loss (Mask): 0.3194701321919759   LPIPS (Whole): 0.04143878154456616   loss_discriminator: 0.12868349701166154   loss_generator1: 0.1332165163755417   loss_generator1_r: 0.056140775630871456   loss_generator1_adv: 0.7621197422345479   loss_generator2: 0.14908253510793051   loss_generator2_r: 0.062042382955551145   loss_generator2_adv: 0.8617658106486003   loss_contrastive: 0.08636672471960385     \n",
            "7/7: Peak SnR (Whole): 30.819223948887416   L2 loss (Whole): 0.0008578391321602144   L2 loss (Mask): 0.07437844973589693   L1 loss (Whole): 0.004172619531995484   L1 loss (Mask): 0.3618271286998476   LPIPS (Whole): 0.04255865540887628    \n",
            "Completed epoch 8! Took 4.866465588410695 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 9 ===== \n",
            "75/78: Peak SnR (Whole): 31.79139331817627   L2 loss (Whole): 0.0006634470917439709   L2 loss (Mask): 0.05747656216224035   L1 loss (Whole): 0.0036259991225476066   L1 loss (Mask): 0.3141398157676061   LPIPS (Whole): 0.04013082993527253   loss_discriminator: 0.12859427640835444   loss_generator1: 0.13468998034795127   loss_generator1_r: 0.054628477841615675   loss_generator1_adv: 0.7929938880602518   loss_generator2: 0.1430115403731664   loss_generator2_r: 0.060324646482865014   loss_generator2_adv: 0.8192569907506307   loss_contrastive: 0.07616537287831307    \n",
            "7/7: Peak SnR (Whole): 31.35687759944371   L2 loss (Whole): 0.0007420287791839135   L2 loss (Mask): 0.06456043943762779   L1 loss (Whole): 0.0038563700758719017   L1 loss (Mask): 0.3354294491665704   LPIPS (Whole): 0.04019407342587199      \n",
            "Completed epoch 9! Took 4.852287872632345 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 10 ===== \n",
            "75/78: Peak SnR (Whole): 31.836432762145996   L2 loss (Whole): 0.0006553838200246294   L2 loss (Mask): 0.05693111687898636   L1 loss (Whole): 0.0035938482933367292   L1 loss (Mask): 0.3121476904551188   LPIPS (Whole): 0.03902833752334118   loss_discriminator: 0.12865631232659022   loss_generator1: 0.13717510670423508   loss_generator1_r: 0.05536353637774785   loss_generator1_adv: 0.8110662817955017   loss_generator2: 0.1389999532699585   loss_generator2_r: 0.05849869738022486   loss_generator2_adv: 0.7979737250010173   loss_contrastive: 0.07044114803274472       \n",
            "7/7: Peak SnR (Whole): 30.935390744890487   L2 loss (Whole): 0.0008336227952635714   L2 loss (Mask): 0.07219167559274606   L1 loss (Whole): 0.0041392592746498326   L1 loss (Mask): 0.3583915148462568   LPIPS (Whole): 0.043321040858115466   \n",
            "Completed epoch 10! Took 4.849491194883982 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 11 ===== \n",
            "75/78: Peak SnR (Whole): 31.970701179504395   L2 loss (Whole): 0.0006357920557881394   L2 loss (Mask): 0.0549952860424916   L1 loss (Whole): 0.0035372554309045273   L1 loss (Mask): 0.3059748621781667   LPIPS (Whole): 0.037151869609951975   loss_discriminator: 0.12988065411647162   loss_generator1: 0.1357771501938502   loss_generator1_r: 0.05373860751589139   loss_generator1_adv: 0.8139727226893108   loss_generator2: 0.13532406955957413   loss_generator2_r: 0.056251964569091796   loss_generator2_adv: 0.7843192299207051   loss_contrastive: 0.06407241307199002    \n",
            "7/7: Peak SnR (Whole): 31.228819165910995   L2 loss (Whole): 0.0007669852639082819   L2 loss (Mask): 0.0667183907436473   L1 loss (Whole): 0.00393974786025605   L1 loss (Mask): 0.34274253036294666   LPIPS (Whole): 0.03770304231771401     \n",
            "Completed epoch 11! Took 4.946525526046753 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 12 ===== \n",
            "75/78: Peak SnR (Whole): 32.03447699228923   L2 loss (Whole): 0.0006262330571189523   L2 loss (Mask): 0.05424899612863859   L1 loss (Whole): 0.0035084710192556184   L1 loss (Mask): 0.30391707758108777   LPIPS (Whole): 0.036502126082777975   loss_discriminator: 0.13054729203383128   loss_generator1: 0.13541237155596414   loss_generator1_r: 0.052996170222759244   loss_generator1_adv: 0.8181547753016154   loss_generator2: 0.1321351314584414   loss_generator2_r: 0.05550182203451792   loss_generator2_adv: 0.7603282809257508   loss_contrastive: 0.0600601044545571    6   \n",
            "7/7: Peak SnR (Whole): 31.396874019077845   L2 loss (Whole): 0.0007332512011219348   L2 loss (Mask): 0.0637574874396835   L1 loss (Whole): 0.003814813680946827   L1 loss (Mask): 0.33166389380182537   LPIPS (Whole): 0.03802241770816701   \n",
            "Completed epoch 12! Took 4.848731382687887 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 13 ===== \n",
            "75/78: Peak SnR (Whole): 32.0606708017985   L2 loss (Whole): 0.000623139370388041   L2 loss (Mask): 0.054080267722407975   L1 loss (Whole): 0.003480389507797857   L1 loss (Mask): 0.3020387711127599   LPIPS (Whole): 0.03515484715501467   loss_discriminator: 0.12858266572157542   loss_generator1: 0.1355763668815295   loss_generator1_r: 0.052729470382134117   loss_generator1_adv: 0.8226663494110107   loss_generator2: 0.13516818940639497   loss_generator2_r: 0.055431065062681834   loss_generator2_adv: 0.7915870197614034   loss_contrastive: 0.05793411393960317        \n",
            "7/7: Peak SnR (Whole): 31.736851828438894   L2 loss (Whole): 0.0006737221098904099   L2 loss (Mask): 0.059032069253070016   L1 loss (Whole): 0.0036384821869432926   L1 loss (Mask): 0.31883426223482403   LPIPS (Whole): 0.03483697652284588   \n",
            "Completed epoch 13! Took 4.8219865997632345 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 14 ===== \n",
            "75/78: Peak SnR (Whole): 32.144687461853025   L2 loss (Whole): 0.0006104922554610918   L2 loss (Mask): 0.053023525228103004   L1 loss (Whole): 0.003439034262361626   L1 loss (Mask): 0.29868575115998586   LPIPS (Whole): 0.033098239054282504   loss_discriminator: 0.13040931562582653   loss_generator1: 0.13507835706075033   loss_generator1_r: 0.052244505286216734   loss_generator1_adv: 0.8228110877672831   loss_generator2: 0.1311798823873202   loss_generator2_r: 0.053802545169989266   loss_generator2_adv: 0.7682443507512411   loss_contrastive: 0.05528204128146172   \n",
            "7/7: Peak SnR (Whole): 31.9709837777274   L2 loss (Whole): 0.0006373399935130562   L2 loss (Mask): 0.05533579736948013   L1 loss (Whole): 0.0035297874793676393   L1 loss (Mask): 0.3064086692673819   LPIPS (Whole): 0.033415451645851135       \n",
            "Completed epoch 14! Took 4.764420350392659 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 15 ===== \n",
            "75/78: Peak SnR (Whole): 32.167944005330405   L2 loss (Whole): 0.0006076969388717165   L2 loss (Mask): 0.05278535164892673   L1 loss (Whole): 0.003427657688347002   L1 loss (Mask): 0.29771401604016623   LPIPS (Whole): 0.0326695811500152   loss_discriminator: 0.13149008204539617   loss_generator1: 0.1336653059720993   loss_generator1_r: 0.05203104590376218   loss_generator1_adv: 0.8111200475692749   loss_generator2: 0.13142833342154822   loss_generator2_r: 0.053539657394091285   loss_generator2_adv: 0.7736699255307515   loss_contrastive: 0.05219685499866804       \n",
            "7/7: Peak SnR (Whole): 31.563248089381627   L2 loss (Whole): 0.000706266491241487   L2 loss (Mask): 0.06136670761874744   L1 loss (Whole): 0.0037336035831166165   L1 loss (Mask): 0.32445244065352846   LPIPS (Whole): 0.034580085293522904   \n",
            "Completed epoch 15! Took 4.784111992518107 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 16 ===== \n",
            "75/78: Peak SnR (Whole): 32.135794474283855   L2 loss (Whole): 0.0006118349762012561   L2 loss (Mask): 0.05294566012918949   L1 loss (Whole): 0.0034504739598681528   L1 loss (Mask): 0.298603481054306   LPIPS (Whole): 0.03260977984716495   loss_discriminator: 0.13158475756645202   loss_generator1: 0.13389977286259333   loss_generator1_r: 0.05245163127779961   loss_generator1_adv: 0.8095342659950256   loss_generator2: 0.1318504383166631   loss_generator2_r: 0.053439688980579374   loss_generator2_adv: 0.779159292380015   loss_contrastive: 0.04947647785147031   8   \n",
            "7/7: Peak SnR (Whole): 31.641594069344656   L2 loss (Whole): 0.0006974542401232091   L2 loss (Mask): 0.060723931661673954   L1 loss (Whole): 0.0037477606175733463   L1 loss (Mask): 0.3263722594295229   LPIPS (Whole): 0.03364308231643268   \n",
            "Completed epoch 16! Took 5.004769659042358 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 17 ===== \n",
            "75/78: Peak SnR (Whole): 32.16800020853678   L2 loss (Whole): 0.0006073677085805684   L2 loss (Mask): 0.05265669775505861   L1 loss (Whole): 0.0034327633570258816   L1 loss (Mask): 0.2976337943474452   LPIPS (Whole): 0.03301510773599148   loss_discriminator: 0.1330210812886556   loss_generator1: 0.1327437103788058   loss_generator1_r: 0.05264560883243879   loss_generator1_adv: 0.7962315408388774   loss_generator2: 0.12937555015087127   loss_generator2_r: 0.05266778667767843   loss_generator2_adv: 0.7623261634508769   loss_contrastive: 0.04750460309286912        \n",
            "7/7: Peak SnR (Whole): 31.63119465964181   L2 loss (Whole): 0.0006936032732483   L2 loss (Mask): 0.060130774176546505   L1 loss (Whole): 0.0037041304666282876   L1 loss (Mask): 0.3209235093423298   LPIPS (Whole): 0.034622338467410634      \n",
            "Completed epoch 17! Took 5.079028864701589 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 18 ===== \n",
            "75/78: Peak SnR (Whole): 32.16917273203532   L2 loss (Whole): 0.0006071049681243797   L2 loss (Mask): 0.05258532921473185   L1 loss (Whole): 0.0034276715293526648   L1 loss (Mask): 0.2969017312924067   LPIPS (Whole): 0.033390854895114896   loss_discriminator: 0.13302542040745416   loss_generator1: 0.13185997148354847   loss_generator1_r: 0.052707942873239516   loss_generator1_adv: 0.7868974057833353   loss_generator2: 0.1297818234562874   loss_generator2_r: 0.052462715556224185   loss_generator2_adv: 0.7685638348261515   loss_contrastive: 0.04625051669776439     \n",
            "7/7: Peak SnR (Whole): 31.98988696507045   L2 loss (Whole): 0.0006351247429847717   L2 loss (Mask): 0.0553666004644973   L1 loss (Whole): 0.003535746081200029   L1 loss (Mask): 0.308194848043578   LPIPS (Whole): 0.03323532428060259     6   \n",
            "Completed epoch 18! Took 5.381736524899801 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 19 ===== \n",
            "75/78: Peak SnR (Whole): 32.221821009318035   L2 loss (Whole): 0.0005993453948758543   L2 loss (Mask): 0.0521109297623237   L1 loss (Whole): 0.0034006952183942   L1 loss (Mask): 0.2956637489795685   LPIPS (Whole): 0.03309470567852259   loss_discriminator: 0.13219767014185588   loss_generator1: 0.1315164237221082   loss_generator1_r: 0.052145489305257794   loss_generator1_adv: 0.7891714334487915   loss_generator2: 0.12999953478574752   loss_generator2_r: 0.0520763702193896   loss_generator2_adv: 0.774698281288147   loss_contrastive: 0.04535625467697779          \n",
            "7/7: Peak SnR (Whole): 31.9880154473441   L2 loss (Whole): 0.0006335212175534772   L2 loss (Mask): 0.05457828619650432   L1 loss (Whole): 0.00352954092834677   L1 loss (Mask): 0.3041254047836576   LPIPS (Whole): 0.03324635473213026   96   \n",
            "Completed epoch 19! Took 5.334724458058675 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 20 ===== \n",
            "75/78: Peak SnR (Whole): 32.24767781575521   L2 loss (Whole): 0.0005964661354664713   L2 loss (Mask): 0.051726634403069814   L1 loss (Whole): 0.0033927302372952304   L1 loss (Mask): 0.2942149019241333   LPIPS (Whole): 0.03286251643051704   loss_discriminator: 0.13158648530642192   loss_generator1: 0.13144400238990783   loss_generator1_r: 0.05188519070545832   loss_generator1_adv: 0.7911035346984864   loss_generator2: 0.1303403745094935   loss_generator2_r: 0.05156807810068131   loss_generator2_adv: 0.7832371703783672   loss_contrastive: 0.044851707195242244      \n",
            "7/7: Peak SnR (Whole): 32.00368676866804   L2 loss (Whole): 0.0006313437354817454   L2 loss (Mask): 0.053982200367110114   L1 loss (Whole): 0.0035658800209473285   L1 loss (Mask): 0.3048850510801588   LPIPS (Whole): 0.03341808170080185    \n",
            "Completed epoch 20! Took 5.1798819979031885 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 21 ===== \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_21447/792332311.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAINING_PARAMS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMETRICS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_wandb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_21447/2776339055.py\u001b[0m in \u001b[0;36mtrain_evaluate\u001b[0;34m(device, train_dataset, validation_dataset, training_params, metrics, start_epoch, log_wandb)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# train every epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_wandb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_wandb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtrain_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_21447/922966667.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(device, train_dataloader, training_params, metrics, log_wandb)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# 3.  train generator 2 (reconstruction, adverserial, contrastive)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# 3.1 forward pass by generator to produce images, reconstruction loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0moutput2_batched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mspliced2_batched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmask_batched\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0moutput2_batched\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmask_batched\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mground_truth_batched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mloss_g2_reconstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRECONSTRUCTION_LOSS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput2_batched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth_batched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_batched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/env/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/env/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mreplicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/env/lib/python3.7/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m    146\u001b[0m                     \u001b[0;31m# parameters in replicas are no longer leaves,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0;31m# so setattr them as non-parameter attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplica\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m                     \u001b[0;31m# expose the parameter for DDP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                     \u001b[0mreplica\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_former_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_modules'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "train_evaluate(device, train_dataset, valid_dataset, TRAINING_PARAMS, METRICS, start_epoch = 0, log_wandb = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrCk_e1oU44V"
      },
      "outputs": [],
      "source": [
        "# train_epoch(device, DataLoader(train_dataset, batch_size = 16), TRAINING_PARAMS, METRICS, log_wandb = True)\n",
        "# evaluate_epoch(device, DataLoader(valid_dataset, batch_size = 16),, TRAINING_PARAMS, metrics, log_wandb = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "fdEKy-uZU6fu",
        "outputId": "87ad1772-6436-4fb6-80df-2db4b44cd03c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ContrastiveGAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
