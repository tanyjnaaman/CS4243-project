{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H98D0BHAEioA"
      },
      "source": [
        "# Initialize notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lpips\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchmetrics\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/home/e/e0425222/CS4243-project\")\n",
        "from utils.dataset_utils.AnimalDataset import AnimalDataset\n",
        "from utils.train_utils.train_utils import visualize_results, sample_batch, summary\n",
        "from utils.train_utils.model_utils import Conv2dBlock, GatedConv2dBlock, GatedUpConv2dBlock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pKzaTBy__baX"
      },
      "outputs": [],
      "source": [
        "train_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_train.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n",
        "\n",
        "valid_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_val.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n",
        "\n",
        "test_dataset = AnimalDataset(index_file_path = \"/home/e/e0425222/CS4243-project/dataset/frogs_test.txt\",\n",
        "    root_dir_path = \"/home/e/e0425222/CS4243-project/dataset/frog_images\",\n",
        "    local_dir_path = \"/home/e/e0425222/CS4243-project/dataset/preprocessed_64\",\n",
        "    file_prefix = \"frogs_\",\n",
        "    image_dimension = 64,\n",
        "    concat_mask = True,\n",
        "    random_noise = False,\n",
        "    require_init = False,\n",
        "    drops = [])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # sanity check \n",
        "# sample_batch(train_dataset, sample_size = 6)\n",
        "# sample_batch(valid_dataset, sample_size = 6)\n",
        "# sample_batch(test_dataset, sample_size = 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouUSbxVkE92G"
      },
      "source": [
        "# Introduction\n",
        "This notebook aims to explore how contrastive learning can improve GAN performance for image inpainting. References are: \n",
        "1. Siamese Neural Networks for One-shot Image Recognition\n",
        "2. SiGAN: Siamese Generative Adversarial Network for Identity-Preserving Face Hallucination\n",
        "3. Free-Form Image Inpainting via Contrastive Attention Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oL0U4VbFaeJ"
      },
      "source": [
        "# Model experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "h2hEL0kCFbt_"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, activation):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        # same -> downsample -> same -> downsample\n",
        "        self.conv0 = GatedConv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv1 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv2 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv3 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # 2 x same conv\n",
        "        self.conv4 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv5 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # 4 x dilated conv\n",
        "        self.conv6 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 2, dilation = 2, activation = activation)\n",
        "        self.conv7 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 4, dilation = 4, activation = activation)\n",
        "        self.conv8 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 8, dilation = 8, activation = activation)\n",
        "        self.conv9 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 16, dilation = 16, activation = activation)\n",
        "\n",
        "        # 2 x same conv\n",
        "        self.conv10 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv11 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # upsample -> same -> upsample -> same\n",
        "        self.conv12 = GatedUpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
        "        self.conv13 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv14 = GatedUpConv2dBlock(hidden_dim, hidden_dim, scale_factor = (2,2), kernel_size = 3, stride = 1, padding = 1, dilation = 1, mode = 'nearest')\n",
        "        self.conv15 = GatedConv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 1, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # final\n",
        "        self.final = nn.Conv2d(hidden_dim, output_dim, kernel_size = 3, stride = 1, padding = 'same')\n",
        "\n",
        "        # for contrastive learning, 1x1 conv to compress feature map into 1 channel\n",
        "        self.conv_feature = nn.Conv2d(hidden_dim, 1, kernel_size = 1, stride = 1, padding = 'same')\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "\n",
        "        x = self.conv0(input_tensor)\n",
        "        \n",
        "        # downsample\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # middle layers\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        # dilated conv with residual skips\n",
        "        x = self.conv6(x) + x\n",
        "        x = self.conv7(x) + x\n",
        "        x = self.conv8(x) + x\n",
        "        x = self.conv9(x) + x\n",
        "\n",
        "        # extract for contrastive loss\n",
        "        x_feature = self.conv_feature(x)\n",
        "\n",
        "        # middle layers\n",
        "        x = self.conv10(x)\n",
        "        x = self.conv11(x)\n",
        "\n",
        "        # upsample\n",
        "        x = self.conv12(x)\n",
        "        x = self.conv13(x)\n",
        "        x = self.conv14(x)\n",
        "        x = self.conv15(x)\n",
        "        \n",
        "        # final\n",
        "        x = self.final(x)\n",
        "\n",
        "        return x, x_feature\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, input_shape, activation):\n",
        "\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # 5 layers down\n",
        "        self.conv0 = Conv2dBlock(input_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv1 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv2 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv3 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "        self.conv4 = Conv2dBlock(hidden_dim, hidden_dim, kernel_size = 3, stride = 2, padding = 1, dilation = 1, activation = activation)\n",
        "\n",
        "        # linear to predict classes\n",
        "        LATENT_H = input_shape//(2**5)\n",
        "        self.linear = nn.Linear(LATENT_H**2 * hidden_dim, 1)\n",
        "\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \n",
        "        x = self.conv0(input_tensor)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        # scores\n",
        "        x = torch.flatten(x, start_dim = 1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSjQKMf6EmH2"
      },
      "source": [
        "# Training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "K26yI7ABEVmI"
      },
      "outputs": [],
      "source": [
        "MODEL_PARAMS = {\n",
        "    \"description\" : \"\"\"\n",
        "        Base structure same as the GLCIC model. Gated convolutions replace all convolutions in the generator, \n",
        "        and a 1x1 convolution is used to flatten the output of the 4th dilated convolution layer. \n",
        "        This is the latent vector used for contrastive learning.\"\"\",\n",
        "    \"hidden_dim\" : 64,\n",
        "    \"activation\" : nn.Mish,\n",
        "}\n",
        "\n",
        "SAVE_PATHS = {\n",
        "    \"generator1\" : \"/home/e/e0425222/CS4243-project/active_experiments/ContrastiveGAN/generator1/generator1\",\n",
        "    \"generator2\" : \"/home/e/e0425222/CS4243-project/active_experiments/ContrastiveGAN/generator2/generator2\",\n",
        "    \"discriminator\" : \"/home/e/e0425222/CS4243-project/active_experiments/ContrastiveGAN/discriminator/discriminator\"\n",
        "}\n",
        "\n",
        "TRAINING_PARAMS = {\n",
        "    \"num_epochs\" : 20,\n",
        "    \"batch_size\" : 128, \n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"alpha\": 0.5,\n",
        "    \"beta\" : 0.01,\n",
        "    \"ADAM_betas\" : (0.5, 0.999),\n",
        "    \"schedule_every\" : 1,\n",
        "    \"sample_size\" : 16,\n",
        "    \"log_every\" : 10,\n",
        "    \"save_paths\" : SAVE_PATHS,\n",
        "}\n",
        "\n",
        "LOGGING_CONFIG = {\n",
        "    \"model_params\" : MODEL_PARAMS,\n",
        "}\n",
        "LOGGING_CONFIG.update(TRAINING_PARAMS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UEXbNHjGvaE"
      },
      "source": [
        "# Experiment intialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbIsqCWTGvGE",
        "outputId": "a1df7c8d-a350-4914-ce06-e4b0da2b8599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model has 1.11642 million parameters\n",
            "model has 1.11642 million parameters\n",
            "model has 0.150401 million parameters\n",
            "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
            "Loading model from: /home/e/e0425222/miniconda3/envs/env/lib/python3.7/site-packages/lpips/weights/v0.1/vgg.pth\n"
          ]
        }
      ],
      "source": [
        "# 1. initialize model\n",
        "generator1 = Generator(input_dim = 4, hidden_dim = MODEL_PARAMS[\"hidden_dim\"], output_dim = 3, activation = MODEL_PARAMS[\"activation\"])\n",
        "generator2 = Generator(input_dim = 4, hidden_dim = MODEL_PARAMS[\"hidden_dim\"], output_dim = 3, activation = MODEL_PARAMS[\"activation\"])\n",
        "discriminator = Discriminator(input_dim = 3, hidden_dim = MODEL_PARAMS[\"hidden_dim\"], input_shape = 64, activation = MODEL_PARAMS[\"activation\"])\n",
        "\n",
        "summary(generator1)\n",
        "summary(generator2)\n",
        "summary(discriminator)\n",
        "\n",
        "# 2. device\n",
        "parallel = True\n",
        "device = 'cuda:0'  \n",
        "devices = [0,1,2,3]\n",
        "\n",
        "if not parallel:\n",
        "    generator1 = generator1.to(device)\n",
        "    generator2 = generator2.to(device)\n",
        "    discriminator = discriminator.to(device)\n",
        "else:\n",
        "    generator1 = nn.DataParallel(generator1, device_ids = devices)\n",
        "    generator2 = nn.DataParallel(generator2, device_ids = devices)\n",
        "    discriminator = nn.DataParallel(discriminator, device_ids = devices)\n",
        "\n",
        "# 3. initialize loss functions\n",
        "recon_loss_function = lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum()\n",
        "contrastive_loss_function_same = lambda x1, x2 : nn.functional.mse_loss(x1, x2)\n",
        "discriminator_loss_function = nn.functional.binary_cross_entropy_with_logits\n",
        "\n",
        "# 4. initialize metrics\n",
        "VGG_LPIPS = lpips.LPIPS(net = 'vgg').to(device)\n",
        "METRICS = {\n",
        "    \"Peak SnR (Whole)\" : lambda img, gt, mask : torchmetrics.functional.peak_signal_noise_ratio(img * (1-mask) + gt * mask, gt),\n",
        "    \"L2 loss (Whole)\" : lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask) + gt * mask, gt),\n",
        "    \"L2 loss (Mask)\" : lambda img, gt, mask : nn.functional.mse_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum(),\n",
        "    \"L1 loss (Whole)\" : lambda img, gt, mask : nn.functional.l1_loss(img * (1-mask) + gt * mask, gt),\n",
        "    \"L1 loss (Mask)\" : lambda img, gt, mask : nn.functional.l1_loss(img * (1-mask), gt * (1-mask), reduction = 'sum')/(1-mask).sum(),\n",
        "    \"LPIPS (Whole)\" : (lambda img, gt, mask : VGG_LPIPS(img * (1-mask) + gt * mask, gt).mean()),\n",
        "}\n",
        "\n",
        "\n",
        "# 5. initialize optimizers\n",
        "generator1_optimizer = torch.optim.Adam(generator1.parameters(), lr = TRAINING_PARAMS[\"learning_rate\"], betas = TRAINING_PARAMS[\"ADAM_betas\"])\n",
        "generator1_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(generator1_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
        "\n",
        "generator2_optimizer = torch.optim.Adam(generator2.parameters(), lr = TRAINING_PARAMS[\"learning_rate\"], betas = TRAINING_PARAMS[\"ADAM_betas\"])\n",
        "generator2_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(generator2_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
        "\n",
        "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr = TRAINING_PARAMS[\"learning_rate\"], betas = TRAINING_PARAMS[\"ADAM_betas\"])\n",
        "discriminator_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(discriminator_optimizer, mode = 'min', factor = 0.5, patience = 1, threshold = 1e-6)\n",
        "\n",
        "# 6. wrap into training dictionary\n",
        "TRAINING_PARAMS[\"generator1_model\"] = generator1\n",
        "TRAINING_PARAMS[\"generator1_optimizer\"] = generator1_optimizer\n",
        "TRAINING_PARAMS[\"generator1_scheduler\"] = generator1_scheduler\n",
        "TRAINING_PARAMS[\"generator2_model\"] = generator2\n",
        "TRAINING_PARAMS[\"generator2_optimizer\"] = generator2_optimizer\n",
        "TRAINING_PARAMS[\"generator2_scheduler\"] = generator2_scheduler\n",
        "TRAINING_PARAMS[\"discriminator_model\"] = discriminator\n",
        "TRAINING_PARAMS[\"discriminator_optimizer\"] = discriminator_optimizer\n",
        "TRAINING_PARAMS[\"discriminator_scheduler\"] = discriminator_scheduler\n",
        "TRAINING_PARAMS[\"discriminator_loss_function\"] = discriminator_loss_function\n",
        "TRAINING_PARAMS[\"contrastive_loss_function\"] = contrastive_loss_function_same\n",
        "TRAINING_PARAMS[\"recon_loss_function\"] = recon_loss_function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intialize logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "6rbEmO4xoVR4",
        "outputId": "75c4f786-f5b7-42de-b20d-5e6f78eaaccf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:kic543e3) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbf7084064954f85a00af25b46f85948",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='236.626 MB of 236.626 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0,…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>L1 loss (Mask)</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>L1 loss (Whole)</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>L2 loss (Mask)</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>L2 loss (Whole)</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>LPIPS (Whole)</td><td>█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Peak SnR (Whole)</td><td>▁▆▆▇▇▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>eval_L1 loss (Mask)</td><td>██▆▆▅▅▄▄▇▇▃▄▆▅▄▅▃▃▅▅▃▄▂▃▂▂▂▁▃▂▂▃▃▂▁▁▁▁▁▁</td></tr><tr><td>eval_L1 loss (Whole)</td><td>██▇▇▅▄▄▄▇▇▂▃▆▅▅▅▃▃▆▅▃▄▂▃▂▂▁▁▃▂▂▃▂▂▂▁▁▁▁▁</td></tr><tr><td>eval_L2 loss (Mask)</td><td>██▆▆▅▄▄▄▆▇▂▃▅▅▅▅▃▃▄▄▃▄▂▃▂▂▂▁▃▂▂▂▃▂▁▁▁▁▁▁</td></tr><tr><td>eval_L2 loss (Whole)</td><td>██▆▆▅▄▄▄▆▇▂▃▅▅▅▅▃▃▅▄▃▄▂▃▂▂▂▁▃▂▂▂▃▂▁▁▁▁▁▁</td></tr><tr><td>eval_LPIPS (Whole)</td><td>▇▇██▆▆▇▆██▆▆▅▆▇▆▅▅▇▆▄▄▄▄▃▂▁▂▂▂▁▂▂▂▂▂▂▁▂▂</td></tr><tr><td>eval_Peak SnR (Whole)</td><td>▁▁▃▃▄▄▅▅▃▃▇▆▃▄▄▄▆▆▄▄▆▅▇▆▆▇▇█▆▆▇▇▆▇██████</td></tr><tr><td>loss_contrastive</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_discriminator</td><td>████▇▇▇▆▅▃▃▃▃▃▂▂▂▂▁▂▂▂▃▃▂▁▂▂▄▃▃▃▄▄▄▄▄▄▃▃</td></tr><tr><td>loss_generator1</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_generator1_adv</td><td>▁▁▁▁▁▂▂▃▃▄▄▃▃▃▄▄▅▆▇▇▇▇███████▇▇▇▇▇▆▆▆▆▆▆</td></tr><tr><td>loss_generator1_r</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_generator2</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_generator2_adv</td><td>▁▁▁▁▁▂▂▃▃▄▄▆▆▇█▇▇▆▆▅▅▅▃▃▄▅▅▄▄▄▅▅▃▄▄▄▅▄▄▄</td></tr><tr><td>loss_generator2_r</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_discriminator</td><td>█████████████████▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_generator1</td><td>█████████████████▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_generator2</td><td>█████████████████▄▄▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>L1 loss (Mask)</td><td>0.29432</td></tr><tr><td>L1 loss (Whole)</td><td>0.00339</td></tr><tr><td>L2 loss (Mask)</td><td>0.05178</td></tr><tr><td>L2 loss (Whole)</td><td>0.0006</td></tr><tr><td>LPIPS (Whole)</td><td>0.03283</td></tr><tr><td>Peak SnR (Whole)</td><td>32.24807</td></tr><tr><td>eval_L1 loss (Mask)</td><td>0.30489</td></tr><tr><td>eval_L1 loss (Whole)</td><td>0.00357</td></tr><tr><td>eval_L2 loss (Mask)</td><td>0.05398</td></tr><tr><td>eval_L2 loss (Whole)</td><td>0.00063</td></tr><tr><td>eval_LPIPS (Whole)</td><td>0.03342</td></tr><tr><td>eval_Peak SnR (Whole)</td><td>32.00369</td></tr><tr><td>loss_contrastive</td><td>0.04488</td></tr><tr><td>loss_discriminator</td><td>0.13166</td></tr><tr><td>loss_generator1</td><td>0.13126</td></tr><tr><td>loss_generator1_adv</td><td>0.78895</td></tr><tr><td>loss_generator1_r</td><td>0.05192</td></tr><tr><td>loss_generator2</td><td>0.1302</td></tr><tr><td>loss_generator2_adv</td><td>0.78124</td></tr><tr><td>loss_generator2_r</td><td>0.05163</td></tr><tr><td>lr_discriminator</td><td>1e-05</td></tr><tr><td>lr_generator1</td><td>1e-05</td></tr><tr><td>lr_generator2</td><td>1e-05</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">worthy-night-26</strong>: <a href=\"https://wandb.ai/cs4243_project/ContrastiveGAN/runs/kic543e3\" target=\"_blank\">https://wandb.ai/cs4243_project/ContrastiveGAN/runs/kic543e3</a><br/>Synced 6 W&B file(s), 564 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220412_145455-kic543e3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:kic543e3). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.14 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/e/e0425222/CS4243-project/active_experiments/ContrastiveGAN/wandb/run-20220412_163931-6et3yafp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/cs4243_project/ContrastiveGAN/runs/6et3yafp\" target=\"_blank\">dauntless-wind-27</a></strong> to <a href=\"https://wandb.ai/cs4243_project/ContrastiveGAN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(project=\"ContrastiveGAN\", entity=\"cs4243_project\")\n",
        "wandb.config = LOGGING_CONFIG\n",
        "\n",
        "wandb.watch(\n",
        "    (generator1, generator2, discriminator),\n",
        "    criterion = None,\n",
        "    log = 'all',\n",
        "    log_freq = 1,\n",
        "    idx = 0, \n",
        "    log_graph = False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsM_LVhrKiE8"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIMfSPuZaJpQ"
      },
      "source": [
        "## Train functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "QFJP7gjJSGFH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_epoch(device, train_dataloader, training_params : dict, metrics : dict, log_wandb = True):\n",
        "    \n",
        "    # ===== INITIALIZE =====\n",
        "    # constants\n",
        "    RECONSTRUCTION_LOSS = training_params[\"recon_loss_function\"]\n",
        "    DISCRIMINATOR_LOSS_FUNCTION = training_params[\"discriminator_loss_function\"]\n",
        "    CONTRASTIVE_LOSS_FUNCTION = training_params[\"contrastive_loss_function\"]\n",
        "\n",
        "    GENERATOR1_OPTIMIZER = training_params[\"generator1_optimizer\"]\n",
        "    GENERATOR2_OPTIMIZER = training_params[\"generator2_optimizer\"]\n",
        "    DISCRIMINATOR_OPTIMIZER = training_params[\"discriminator_optimizer\"]\n",
        "\n",
        "    BATCH_EVALUATE_EVERY = 5\n",
        "    LOG_EVERY = training_params[\"log_every\"]\n",
        "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
        "    BATCH_SIZE = training_params[\"batch_size\"]\n",
        "    ALPHA = training_params[\"alpha\"]\n",
        "    BETA = training_params[\"beta\"]\n",
        "\n",
        "    # models\n",
        "    generator1 = training_params[\"generator1_model\"].to(device).train()\n",
        "    generator2 = training_params[\"generator2_model\"].to(device).train()\n",
        "    discriminator = training_params[\"discriminator_model\"].to(device).train()\n",
        "\n",
        "    # epoch metrics\n",
        "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
        "    running_results[\"loss_discriminator\"] = 0.0\n",
        "    running_results[\"loss_generator1\"] = 0.0\n",
        "    running_results[\"loss_generator1_r\"] = 0.0\n",
        "    running_results[\"loss_generator1_adv\"] = 0.0\n",
        "    running_results[\"loss_generator2\"] = 0.0\n",
        "    running_results[\"loss_generator2_r\"] = 0.0\n",
        "    running_results[\"loss_generator2_adv\"] = 0.0\n",
        "    running_results[\"loss_contrastive\"] = 0.0\n",
        "\n",
        "    # ===== TRAIN EPOCH =====\n",
        "    num_batches = 0\n",
        "    for _, batch in enumerate(train_dataloader, 1):\n",
        "\n",
        "            # ===== INITIALIZE =====\n",
        "            num_batches += 1\n",
        "\n",
        "            # input and ground truth\n",
        "            input_batched = batch[\"image\"]\n",
        "            ground_truth_batched = batch[\"reconstructed\"]\n",
        "            mask_batched = batch[\"mask\"]\n",
        "\n",
        "            # sanity check\n",
        "            assert input_batched.shape[0] == ground_truth_batched.shape[0]\n",
        "\n",
        "            # move tensors to device\n",
        "            input_batched = input_batched.to(device)\n",
        "            ground_truth_batched = ground_truth_batched.to(device)\n",
        "            mask_batched = mask_batched.to(device)\n",
        "\n",
        "            # set the gradients to zeros\n",
        "            GENERATOR1_OPTIMIZER.zero_grad()\n",
        "            GENERATOR2_OPTIMIZER.zero_grad()\n",
        "            DISCRIMINATOR_OPTIMIZER.zero_grad()\n",
        "\n",
        "            # reshape to channel first\n",
        "            input_batched = input_batched.permute(0, 3, 1, 2)\n",
        "            ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
        "            mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
        "\n",
        "            # ===== FORWARD PASS =====\n",
        "\n",
        "            # 1. train discriminator\n",
        "            \n",
        "            # 1.1 generate images\n",
        "            input_batched.requires_grad_()\n",
        "            output1_batched, _ = generator1(input_batched)\n",
        "            shuffled_indices = torch.randperm(n = BATCH_SIZE)\n",
        "            output2_batched, _ = generator2(input_batched[shuffled_indices]) # random permutation\n",
        "\n",
        "            # 1.2 splice with ground truth\n",
        "            spliced1_batched = ((1-mask_batched) * output1_batched) + (mask_batched * ground_truth_batched) \n",
        "            spliced2_batched = ((1-mask_batched[shuffled_indices]) * output2_batched) + (mask_batched[shuffled_indices] * ground_truth_batched[shuffled_indices]) \n",
        "\n",
        "            # 1.3 feed into discriminator\n",
        "            label_real = torch.ones(BATCH_SIZE * 2, 1).to(device)\n",
        "            label_fake = torch.zeros(BATCH_SIZE * 2, 1).to(device)\n",
        "\n",
        "            pred_real = discriminator(torch.cat([ground_truth_batched, ground_truth_batched[shuffled_indices]], dim = 0))\n",
        "            loss_real = DISCRIMINATOR_LOSS_FUNCTION(pred_real, label_real)\n",
        "\n",
        "            pred_fake = discriminator(torch.cat([spliced1_batched, spliced2_batched[shuffled_indices]], dim = 0))\n",
        "            loss_fake = DISCRIMINATOR_LOSS_FUNCTION(pred_fake, label_fake)\n",
        "            loss_d = ALPHA * (loss_real + loss_fake)\n",
        "            loss_d.backward()\n",
        "            DISCRIMINATOR_OPTIMIZER.step()\n",
        "\n",
        "            # 2. train generator 1 (reconstruction, adverserial, contrastive)\n",
        "            # 2.1 forward pass by generator to produce images, splice them\n",
        "            output1_batched, _ = generator1(input_batched)\n",
        "            spliced1_batched = ((1-mask_batched) * output1_batched) + (mask_batched * ground_truth_batched) \n",
        "            \n",
        "            # 2.2 reconstruction loss\n",
        "            loss_g1_reconstruction = RECONSTRUCTION_LOSS(output1_batched, ground_truth_batched, mask_batched)\n",
        "\n",
        "            # 2.3 adverserial loss\n",
        "            label_real = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "            pred1_adverserial = discriminator(spliced1_batched)\n",
        "            loss_g1_adverserial = DISCRIMINATOR_LOSS_FUNCTION(pred1_adverserial, label_real) # want it to classify all as real\n",
        "\n",
        "            # 2.4 contrastive loss\n",
        "            shuffled_indices = torch.randperm(n = BATCH_SIZE)\n",
        "            copied_input_batched = torch.cat([ground_truth_batched * mask_batched[shuffled_indices], mask_batched[shuffled_indices]], dim = 1)\n",
        "            _, x1 = generator1(input_batched)\n",
        "            _, x2 = generator2(copied_input_batched) # same image, different masks\n",
        "            loss_contrastive1 = CONTRASTIVE_LOSS_FUNCTION(x1, x2)\n",
        "\n",
        "            # 2.5 backprop\n",
        "            loss_g1 = loss_g1_reconstruction + ALPHA * loss_g1_adverserial + BETA * loss_contrastive1\n",
        "            loss_g1.backward()\n",
        "            GENERATOR1_OPTIMIZER.step()\n",
        "\n",
        "            # 3.  train generator 2 (reconstruction, adverserial, contrastive)\n",
        "            # 3.1 forward pass by generator to produce images, reconstruction loss\n",
        "            output2_batched, _ = generator2(input_batched)\n",
        "            spliced2_batched = ((1-mask_batched) * output2_batched) + (mask_batched * ground_truth_batched) \n",
        "            loss_g2_reconstruction = RECONSTRUCTION_LOSS(output2_batched, ground_truth_batched, mask_batched)\n",
        "\n",
        "            # 3.2 adverserial loss\n",
        "            label_real = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "            pred2_adverserial = discriminator(spliced2_batched)\n",
        "            loss_g2_adverserial = DISCRIMINATOR_LOSS_FUNCTION(pred2_adverserial, label_real) # want it to classify all as real\n",
        "\n",
        "            # 3.3 contrastive loss\n",
        "            shuffled_indices = torch.randperm(n = BATCH_SIZE)\n",
        "            copied_input_batched = torch.cat([ground_truth_batched * mask_batched[shuffled_indices], mask_batched[shuffled_indices]], dim = 1)\n",
        "            _, x1 = generator1(input_batched)\n",
        "            _, x2 = generator2(copied_input_batched) # same image, different masks\n",
        "            loss_contrastive2 = CONTRASTIVE_LOSS_FUNCTION(x1, x2)\n",
        "\n",
        "            # 3.4 backprop\n",
        "            loss_g2 = loss_g2_reconstruction + ALPHA * loss_g2_adverserial + BETA * loss_contrastive2\n",
        "            loss_g2.backward()\n",
        "            GENERATOR2_OPTIMIZER.step()\n",
        "\n",
        "\n",
        "            # ===== COMPUTE STATISTICS, USING TORCH METRICS =====  \n",
        "            # 1. compute losses\n",
        "\n",
        "            running_results[\"loss_generator1\"] += loss_g1.detach().item()\n",
        "            running_results[\"loss_generator1_r\"] += loss_g1_reconstruction.detach().item()\n",
        "            running_results[\"loss_generator1_adv\"] += loss_g1_adverserial.detach().item()\n",
        "            running_results[\"loss_generator2\"] += loss_g2.detach().item()\n",
        "            running_results[\"loss_generator2_r\"] += loss_g2_reconstruction.detach().item()\n",
        "            running_results[\"loss_generator2_adv\"] += loss_g2_adverserial.detach().item()\n",
        "            running_results[\"loss_contrastive\"] += (loss_contrastive1.detach().item() + loss_contrastive2.detach().item())/2\n",
        "            running_results[\"loss_discriminator\"] += loss_d.detach().item()\n",
        "\n",
        "            # 2. for each key, compute, add item to results dictionary (take average of 2 generators)\n",
        "            for key, func in metrics.items():\n",
        "                res1 = func(output1_batched, ground_truth_batched, mask_batched).detach().item()\n",
        "                res2 = func(output2_batched, ground_truth_batched, mask_batched).detach().item()\n",
        "                running_results[key] += (res1 + res2)/2\n",
        "\n",
        "            # 3. log with wandb\n",
        "            if log_wandb and (num_batches % LOG_EVERY == 0):\n",
        "\n",
        "                # generator 1\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced1_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images1 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "                \n",
        "                # generator 2\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced2_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images2 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "\n",
        "                # log images and some metadata\n",
        "                wandb.log( {\n",
        "                    \"generator1_train_images\" : images1,\n",
        "                    \"generator2_train_images\" : images2,\n",
        "                    \"lr_generator1\" : GENERATOR1_OPTIMIZER.param_groups[0]['lr'],\n",
        "                    \"lr_generator2\" : GENERATOR2_OPTIMIZER.param_groups[0]['lr'],\n",
        "                    \"lr_discriminator\" : DISCRIMINATOR_OPTIMIZER.param_groups[0]['lr']\n",
        "\n",
        "                })\n",
        "\n",
        "                # log all metrics\n",
        "                wandb.log(\n",
        "                    {key : item/num_batches for key, item in running_results.items()}\n",
        "                )\n",
        "        \n",
        "\n",
        "            # ===== HOUSEKEEPING =====\n",
        "            del loss_g2\n",
        "            del loss_g1\n",
        "            del loss_d\n",
        "            del input_batched\n",
        "\n",
        "            # print results every some batches\n",
        "            if num_batches % BATCH_EVALUATE_EVERY == 0: \n",
        "\n",
        "                args = \"\"\n",
        "                for key, val in running_results.items():\n",
        "                    args += key + \": \" + str(running_results[key]/num_batches) + \"   \"\n",
        "                print(f\"\\r{num_batches}/{len(train_dataloader)}: \" + args, end = '', flush = True)\n",
        "\n",
        "    # normalise numbers by batch\n",
        "    for key, val in running_results.items():\n",
        "        running_results[key] /= num_batches\n",
        "\n",
        "    return running_results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "4CuiHVAjKitr"
      },
      "outputs": [],
      "source": [
        "def evaluate_epoch(device, validation_dataloader, training_params : dict, metrics : dict, log_wandb = True):\n",
        "\n",
        "    # ===== INITIALIZE =====\n",
        "    # models\n",
        "    generator1 = training_params[\"generator1_model\"].to(device).eval()\n",
        "    generator2 = training_params[\"generator2_model\"].to(device).eval()\n",
        "\n",
        "    # constants\n",
        "    SAMPLE_SIZE = training_params[\"sample_size\"]\n",
        "\n",
        "    # epoch statistics\n",
        "    running_results = {list(metrics.keys())[i] : 0.0 for i in range(len(metrics)) } \n",
        "\n",
        "    # ===== EVALUATE EPOCH =====\n",
        "\n",
        "    with torch.no_grad():\n",
        "        batches = 0\n",
        "        for index, batch in enumerate(validation_dataloader, 1):\n",
        "            \n",
        "            batches += 1\n",
        "\n",
        "            # input and ground truth\n",
        "            input_batched = batch[\"image\"]\n",
        "            ground_truth_batched = batch[\"reconstructed\"]\n",
        "            mask_batched = batch[\"mask\"]\n",
        "\n",
        "            # move tensors to device\n",
        "            input_batched = input_batched.to(device)\n",
        "            ground_truth_batched = ground_truth_batched.to(device)\n",
        "            mask_batched = mask_batched.to(device)\n",
        "\n",
        "            # ===== FORWARD PASS =====\n",
        "\n",
        "            # 1. reshape to channel first\n",
        "            input_batched = input_batched.permute(0, 3, 1, 2)\n",
        "            ground_truth_batched = ground_truth_batched.permute(0, 3, 1, 2)\n",
        "            mask_batched = mask_batched.permute(0, 3, 1, 2)\n",
        "\n",
        "            # 2. predict    \n",
        "            output1_batched, _ = generator1(input_batched)\n",
        "            output2_batched, _ = generator2(input_batched)           \n",
        "            spliced1_batched = ((1-mask_batched) * output1_batched) + (mask_batched * ground_truth_batched) \n",
        "            spliced2_batched = ((1-mask_batched) * output2_batched) + (mask_batched * ground_truth_batched) \n",
        "\n",
        "            # 3. evaluate\n",
        "            for key, func in metrics.items():\n",
        "                running_results[key] += (func(output1_batched, ground_truth_batched, mask_batched).detach().item() + func(output2_batched, ground_truth_batched, mask_batched).detach().item())/2\n",
        "\n",
        "            args = \"\"\n",
        "            for key, val in running_results.items():\n",
        "                args += key + \": \" + str(running_results[key]/batches) + \"   \"\n",
        "            print(f\"\\r{batches}/{len(validation_dataloader)}: \" + args, end = '', flush = True)\n",
        "\n",
        "            # 4. log \n",
        "            if log_wandb:\n",
        "\n",
        "                # generator 1\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced1_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images1 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "                \n",
        "                # generator 2\n",
        "                batched_predictions = torch.cat([\n",
        "                    input_batched[:SAMPLE_SIZE, 0:3,:,:], # can be 4 channels\n",
        "                    ground_truth_batched[:SAMPLE_SIZE,:,:,:],  # 3 channels\n",
        "                    spliced2_batched[:SAMPLE_SIZE,:,:,:]], dim = 0) \n",
        "                \n",
        "                image_array = torchvision.utils.make_grid(batched_predictions, nrow = SAMPLE_SIZE, padding = 50)\n",
        "                images2 = wandb.Image(\n",
        "                    image_array, \n",
        "                    caption = \"1st row: Damaged, 2nd row: Ground truth, 3rd row: spliced\")\n",
        "\n",
        "                # log images and some metadata\n",
        "                wandb.log( {\n",
        "                    \"generator1_val_images\" : images1,\n",
        "                    \"generator2_val_images\" : images2\n",
        "                })\n",
        "\n",
        "                # log all metrics\n",
        "                wandb.log(\n",
        "                    {f\"eval_{key}\" : item/batches for key, item in running_results.items()}\n",
        "                )\n",
        "\n",
        "    # normalise numbers by batch\n",
        "    for key, val in running_results.items():\n",
        "        running_results[key] /= batches\n",
        "\n",
        "    return running_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "O_mQDSLXaPdz"
      },
      "outputs": [],
      "source": [
        "def train_evaluate(device, train_dataset, validation_dataset, training_params: dict, metrics: dict, start_epoch = 0, log_wandb = True):\n",
        "\n",
        "    # ===== INITIALIZE =====\n",
        "    # constants\n",
        "    NUM_EPOCHS = training_params[\"num_epochs\"]\n",
        "    BATCH_SIZE = training_params[\"batch_size\"]\n",
        "    GENERATOR1_SCHEDULER = training_params[\"generator1_scheduler\"]\n",
        "    GENERATOR2_SCHEDULER = training_params[\"generator2_scheduler\"]\n",
        "    DISCRIMINATOR_SCHEDULER = training_params[\"discriminator_scheduler\"]\n",
        "    SAVE_PATHS = training_params[\"save_paths\"]\n",
        "    NUM_WORKERS = 2\n",
        "    START_EPOCH = start_epoch\n",
        "\n",
        "    # models for saving\n",
        "    generator1 = training_params[\"generator1_model\"].to(device).train()\n",
        "    generator2 = training_params[\"generator2_model\"].to(device).train()\n",
        "    discriminator = training_params[\"discriminator_model\"].to(device).train()\n",
        "\n",
        "    # variable losses\n",
        "    train_results = {list(metrics.keys())[i] : [] for i in range(len(metrics)) } \n",
        "\n",
        "    train_results[\"loss_discriminator\"] = []\n",
        "    train_results[\"loss_generator1\"] = []\n",
        "    train_results[\"loss_generator1_r\"] = []\n",
        "    train_results[\"loss_generator1_adv\"] = []\n",
        "    train_results[\"loss_generator2\"] = []\n",
        "    train_results[\"loss_generator2_r\"] = []\n",
        "    train_results[\"loss_generator2_adv\"] = []\n",
        "    train_results[\"loss_contrastive\"] = []\n",
        "\n",
        "    eval_results = {list(metrics.keys())[i] : [] for i in range(len(metrics)) } \n",
        "\n",
        "    # dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS, drop_last = True)\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS, drop_last = True)\n",
        "\n",
        "    # ===== TRAIN =====\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # train\n",
        "        print(f\"\\n===== Epoch: {START_EPOCH + epoch + 1} ===== \")\n",
        "        num_batches = 0\n",
        "\n",
        "        # train every epoch\n",
        "        results = train_epoch(device, train_dataloader, training_params, metrics, log_wandb = log_wandb)\n",
        "        for key, val in results.items():\n",
        "            train_results[key].append(val)\n",
        "\n",
        "        # evaluate every epoch\n",
        "        print()\n",
        "        results = evaluate_epoch(device, validation_dataloader, training_params, metrics, log_wandb = log_wandb)\n",
        "        for key, val in results.items():\n",
        "            eval_results[key].append(val)\n",
        "\n",
        "        # ===== EPOCH RESULTS =====\n",
        "        print(f\"\\nCompleted epoch {START_EPOCH + epoch + 1}! Took {(time.time() - start)/60} min\")\n",
        "\n",
        "        # ===== HOUSEKEEPING =====\n",
        "\n",
        "        # scheduler every epoch\n",
        "        if DISCRIMINATOR_SCHEDULER is not None:\n",
        "            DISCRIMINATOR_SCHEDULER.step(eval_results[\"L2 loss (Mask)\"][epoch])\n",
        "        if GENERATOR1_SCHEDULER is not None:\n",
        "            GENERATOR1_SCHEDULER.step(eval_results[\"L2 loss (Mask)\"][epoch])\n",
        "        if GENERATOR2_SCHEDULER is not None:\n",
        "            GENERATOR2_SCHEDULER.step(eval_results[\"L2 loss (Mask)\"][epoch])\n",
        "            \n",
        "        # save every epoch\n",
        "        SAVE = f\"{SAVE_PATHS['generator1']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
        "        torch.save(generator1.state_dict(), SAVE)\n",
        "        SAVE = f\"{SAVE_PATHS['generator2']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
        "        torch.save(generator2.state_dict(), SAVE)\n",
        "        SAVE = f\"{SAVE_PATHS['discriminator']}_epoch{START_EPOCH + epoch + 1}.pt\"\n",
        "        torch.save(discriminator.state_dict(), SAVE)  \n",
        "\n",
        "        print(\"Saved models!\")\n",
        "\n",
        "    return train_results, eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8IkxVhsVsf4"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_IuN9J8Jfp9j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Epoch: 2 ===== \n",
            "75/78: Peak SnR (Whole): 30.71053050994873   L2 loss (Whole): 0.0008522870379965753   L2 loss (Mask): 0.07410091881950696   L1 loss (Whole): 0.004203918441198766   L1 loss (Mask): 0.3655221309264501   LPIPS (Whole): 0.042867486774921415   loss_discriminator: 0.1389591598510742   loss_generator1: 0.15367983877658845   loss_generator1_r: 0.0771857231358687   loss_generator1_adv: 0.7015039857228597   loss_generator2: 0.1474274315436681   loss_generator2_r: 0.07101611450314521   loss_generator2_adv: 0.7009817926088969   loss_contrastive: 0.6328426249821981       \n",
            "7/7: Peak SnR (Whole): 31.09635434831892   L2 loss (Whole): 0.0007771012439791645   L2 loss (Mask): 0.06753567659429141   L1 loss (Whole): 0.0039976856704535225   L1 loss (Mask): 0.3474681334836142   LPIPS (Whole): 0.04218321852385998    \n",
            "Completed epoch 2! Took 5.123102037111918 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 3 ===== \n",
            "75/78: Peak SnR (Whole): 31.144846852620443   L2 loss (Whole): 0.0007694351855510225   L2 loss (Mask): 0.06672343604266644   L1 loss (Whole): 0.0039806637462849416   L1 loss (Mask): 0.345191352168719   LPIPS (Whole): 0.04185678973793983   loss_discriminator: 0.1384680547316869   loss_generator1: 0.14338070134321848   loss_generator1_r: 0.06875928501288096   loss_generator1_adv: 0.7036904486020407   loss_generator2: 0.139194033741951   loss_generator2_r: 0.06468758707245191   loss_generator2_adv: 0.7026708952585856   loss_contrastive: 0.4245862589279811       \n",
            "7/7: Peak SnR (Whole): 31.335096631731307   L2 loss (Whole): 0.0007338924624491483   L2 loss (Mask): 0.06398662179708481   L1 loss (Whole): 0.003815450905156987   L1 loss (Mask): 0.33248997798987795   LPIPS (Whole): 0.03836120132889066   \n",
            "Completed epoch 3! Took 5.168569835027059 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 4 ===== \n",
            "75/78: Peak SnR (Whole): 31.463612276713054   L2 loss (Whole): 0.0007155714184045791   L2 loss (Mask): 0.0618422519415617   L1 loss (Whole): 0.003822048696068426   L1 loss (Mask): 0.3303051946560542   LPIPS (Whole): 0.0403278798609972   loss_discriminator: 0.1380446974436442   loss_generator1: 0.13831490377585093   loss_generator1_r: 0.06420059978961945   loss_generator1_adv: 0.7095330762863159   loss_generator2: 0.13328845292329788   loss_generator2_r: 0.05948390409350395   loss_generator2_adv: 0.7064294703801473   loss_contrastive: 0.31612988193829855   1   \n",
            "7/7: Peak SnR (Whole): 31.85880974360875   L2 loss (Whole): 0.0006519649843020099   L2 loss (Mask): 0.05652721944664206   L1 loss (Whole): 0.0035953201253765394   L1 loss (Mask): 0.31175120174884796   LPIPS (Whole): 0.03789890690573624    \n",
            "Completed epoch 4! Took 5.142155917485555 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 5 ===== \n",
            "75/78: Peak SnR (Whole): 31.699874610900878   L2 loss (Whole): 0.0006774744938593358   L2 loss (Mask): 0.05870520030458768   L1 loss (Whole): 0.0036942347915222247   L1 loss (Mask): 0.3201098924875259   LPIPS (Whole): 0.03890234805643558   loss_discriminator: 0.13742960810661317   loss_generator1: 0.1351131120324135   loss_generator1_r: 0.060957488964001334   loss_generator1_adv: 0.7169780961672465   loss_generator2: 0.13026926984389622   loss_generator2_r: 0.05645291164517403   loss_generator2_adv: 0.7136389843622843   loss_contrastive: 0.24551348835229875      \n",
            "7/7: Peak SnR (Whole): 31.578236307416642   L2 loss (Whole): 0.0006948028250397849   L2 loss (Mask): 0.05943320531930242   L1 loss (Whole): 0.003730066237039864   L1 loss (Mask): 0.3190245968954904   LPIPS (Whole): 0.03829991285290037     \n",
            "Completed epoch 5! Took 4.9914986252784725 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 6 ===== \n",
            "5/78: Peak SnR (Whole): 31.704420471191405   L2 loss (Whole): 0.0006756984279491007   L2 loss (Mask): 0.05882653333246708   L1 loss (Whole): 0.003663200722075999   L1 loss (Mask): 0.31889734864234925   LPIPS (Whole): 0.0381472609937191   loss_discriminator: 0.1370965212583542   loss_generator1: 0.13606440424919128   loss_generator1_r: 0.06086156368255615   loss_generator1_adv: 0.730527150630951   loss_generator2: 0.13181667625904084   loss_generator2_r: 0.056791502982378006   loss_generator2_adv: 0.728773546218872   loss_contrastive: 0.21489689946174623   "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Network error (ReadTimeout), entering retry loop.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "75/78: Peak SnR (Whole): 31.9257675298055   L2 loss (Whole): 0.0006435682492641112   L2 loss (Mask): 0.05581380488971869   L1 loss (Whole): 0.0035833495234449705   L1 loss (Mask): 0.3107676794131597   LPIPS (Whole): 0.03798622121413549   loss_discriminator: 0.1365488823254903   loss_generator1: 0.13222080320119858   loss_generator1_r: 0.057326476176579794   loss_generator1_adv: 0.7291651407877604   loss_generator2: 0.12894901216030122   loss_generator2_r: 0.05430113360285759   loss_generator2_adv: 0.726722104549408   loss_contrastive: 0.19767392347256343      \n",
            "7/7: Peak SnR (Whole): 32.40780626024519   L2 loss (Whole): 0.0005743799390204783   L2 loss (Mask): 0.05006111493068082   L1 loss (Whole): 0.0033240020607731174   L1 loss (Mask): 0.2897902599402836   LPIPS (Whole): 0.035420920965926986    \n",
            "Completed epoch 6! Took 5.051364807287852 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 7 ===== \n",
            "75/78: Peak SnR (Whole): 32.003800684611   L2 loss (Whole): 0.000632371447669963   L2 loss (Mask): 0.05466257880131404   L1 loss (Whole): 0.003552566021680832   L1 loss (Mask): 0.307097016175588   LPIPS (Whole): 0.0376548387358586   loss_discriminator: 0.13520141780376435   loss_generator1: 0.13129558006922404   loss_generator1_r: 0.055835285286108656   loss_generator1_adv: 0.7381316200892131   loss_generator2: 0.1284793751438459   loss_generator2_r: 0.05348987231651942   loss_generator2_adv: 0.7334366750717163   loss_contrastive: 0.1646482346455256   954905   \n",
            "7/7: Peak SnR (Whole): 32.428293909345356   L2 loss (Whole): 0.0005735124369883644   L2 loss (Mask): 0.04937875936073916   L1 loss (Whole): 0.0033233314918886337   L1 loss (Mask): 0.2861614567892892   LPIPS (Whole): 0.03574412901486669    \n",
            "Completed epoch 7! Took 5.10421195824941 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 8 ===== \n",
            "75/78: Peak SnR (Whole): 32.15422273000081   L2 loss (Whole): 0.0006101312926815202   L2 loss (Mask): 0.052788634772102036   L1 loss (Whole): 0.0034677676980694135   L1 loss (Mask): 0.3000219511985779   LPIPS (Whole): 0.036886309683322904   loss_discriminator: 0.1329014140367508   loss_generator1: 0.1317808187007904   loss_generator1_r: 0.05401798208554586   loss_generator1_adv: 0.7638446116447448   loss_generator2: 0.1280656224489212   loss_generator2_r: 0.05155928745865822   loss_generator2_adv: 0.7513201721509297   loss_contrastive: 0.13763457437356313      \n",
            "7/7: Peak SnR (Whole): 31.98050022125244   L2 loss (Whole): 0.0006331060680427722   L2 loss (Mask): 0.05461998683001314   L1 loss (Whole): 0.0034962641407868694   L1 loss (Mask): 0.3016721499817712   LPIPS (Whole): 0.03604691502239023      \n",
            "Completed epoch 8! Took 5.237961415449778 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 9 ===== \n",
            "75/78: Peak SnR (Whole): 32.2030005645752   L2 loss (Whole): 0.00060334260284435   L2 loss (Mask): 0.05224367479483286   L1 loss (Whole): 0.00343940616119653   L1 loss (Mask): 0.2978248194853465   LPIPS (Whole): 0.03719742606083552   loss_discriminator: 0.13231786251068114   loss_generator1: 0.1324604711929957   loss_generator1_r: 0.05307491024335225   loss_generator1_adv: 0.7820861752827962   loss_generator2: 0.12721493820349375   loss_generator2_r: 0.051412439346313475   loss_generator2_adv: 0.7462411991755168   loss_contrastive: 0.11776604036490122   65     \n",
            "7/7: Peak SnR (Whole): 32.53041294642857   L2 loss (Whole): 0.000556826067622751   L2 loss (Mask): 0.048829932031886916   L1 loss (Whole): 0.0032855646152581486   L1 loss (Mask): 0.2880394629069737   LPIPS (Whole): 0.035069052928260395     \n",
            "Completed epoch 9! Took 4.760503665606181 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 10 ===== \n",
            "75/78: Peak SnR (Whole): 32.21876786549886   L2 loss (Whole): 0.0006010397500358522   L2 loss (Mask): 0.052156419108311335   L1 loss (Whole): 0.003436010459748407   L1 loss (Mask): 0.2981427691380183   LPIPS (Whole): 0.03766816827158133   loss_discriminator: 0.13205215811729432   loss_generator1: 0.13319690237442652   loss_generator1_r: 0.052909849186738335   loss_generator1_adv: 0.7925909360249838   loss_generator2: 0.1270255523920059   loss_generator2_r: 0.05140298902988434   loss_generator2_adv: 0.7459746686617533   loss_contrastive: 0.10265263244509697      \n",
            "7/7: Peak SnR (Whole): 32.446840150015696   L2 loss (Whole): 0.0005691472075081297   L2 loss (Mask): 0.0491833532495158   L1 loss (Whole): 0.0033075838083667414   L1 loss (Mask): 0.285821938088962   LPIPS (Whole): 0.03558608304177012      \n",
            "Completed epoch 10! Took 4.798035228252411 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 11 ===== \n",
            "75/78: Peak SnR (Whole): 32.3993625386556   L2 loss (Whole): 0.0005768249573884532   L2 loss (Mask): 0.05012068018317223   L1 loss (Whole): 0.003343565915711224   L1 loss (Mask): 0.29054862916469576   LPIPS (Whole): 0.03617977055410544   loss_discriminator: 0.1295533773303032   loss_generator1: 0.13106765786806743   loss_generator1_r: 0.051106473157803214   loss_generator1_adv: 0.7906336498260498   loss_generator2: 0.12825447350740432   loss_generator2_r: 0.049134887208541235   loss_generator2_adv: 0.7822166228294373   loss_contrastive: 0.08978705222407977     \n",
            "7/7: Peak SnR (Whole): 32.6214975629534   L2 loss (Whole): 0.0005461316031869501   L2 loss (Mask): 0.04699949600866863   L1 loss (Whole): 0.003242827735708228   L1 loss (Mask): 0.2790029261793409   LPIPS (Whole): 0.03419241708304201       \n",
            "Completed epoch 11! Took 4.8534430980682375 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 12 ===== \n",
            "75/78: Peak SnR (Whole): 32.43799017588297   L2 loss (Whole): 0.0005713601301734646   L2 loss (Mask): 0.04944287493824959   L1 loss (Whole): 0.0033256533229723572   L1 loss (Mask): 0.28774898409843447   LPIPS (Whole): 0.035461220604678   loss_discriminator: 0.12656736036141714   loss_generator1: 0.13248067647218703   loss_generator1_r: 0.0503324364622434   loss_generator1_adv: 0.8136791078249613   loss_generator2: 0.13077148516972859   loss_generator2_r: 0.048553313414255776   loss_generator2_adv: 0.8143816033999125   loss_contrastive: 0.0780168857673804   318   \n",
            "7/7: Peak SnR (Whole): 32.76002938406808   L2 loss (Whole): 0.0005292653783856492   L2 loss (Mask): 0.04547923510628087   L1 loss (Whole): 0.003165267940078463   L1 loss (Mask): 0.2720458741698946   LPIPS (Whole): 0.032870562481028695    \n",
            "Completed epoch 12! Took 4.891799481709798 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 13 ===== \n",
            "75/78: Peak SnR (Whole): 32.41748124440511   L2 loss (Whole): 0.0005748998352404063   L2 loss (Mask): 0.04971214863161246   L1 loss (Whole): 0.0033309017059703666   L1 loss (Mask): 0.2880824641386668   LPIPS (Whole): 0.03477310426533222   loss_discriminator: 0.12439418236414591   loss_generator1: 0.13516263405481974   loss_generator1_r: 0.050335399160782494   loss_generator1_adv: 0.8414305567741394   loss_generator2: 0.13344910472631455   loss_generator2_r: 0.04908889810244242   loss_generator2_adv: 0.8367597270011902   loss_contrastive: 0.06842048928141593    \n",
            "7/7: Peak SnR (Whole): 32.293819836207796   L2 loss (Whole): 0.0005889751898523952   L2 loss (Mask): 0.0507542791643313   L1 loss (Whole): 0.003400827192568353   L1 loss (Mask): 0.29302269007478443   LPIPS (Whole): 0.03322538480694805      \n",
            "Completed epoch 13! Took 4.730450014273326 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 14 ===== \n",
            "75/78: Peak SnR (Whole): 32.29194087982178   L2 loss (Whole): 0.0005910416729360198   L2 loss (Mask): 0.051293940966327986   L1 loss (Whole): 0.003385611952592929   L1 loss (Mask): 0.2938182761271795   LPIPS (Whole): 0.033494425316651665   loss_discriminator: 0.12751659631729126   loss_generator1: 0.13550864646832148   loss_generator1_r: 0.051866583426793414   loss_generator1_adv: 0.8303851127624512   loss_generator2: 0.13441727489233016   loss_generator2_r: 0.05072129850586255   loss_generator2_adv: 0.8309403610229492   loss_contrastive: 0.060274466450015704   \n",
            "7/7: Peak SnR (Whole): 32.21692780085972   L2 loss (Whole): 0.0006001405584226761   L2 loss (Mask): 0.0522747848715101   L1 loss (Whole): 0.003397507526512657   L1 loss (Mask): 0.29592682846954893   LPIPS (Whole): 0.030884580154504095      \n",
            "Completed epoch 14! Took 4.983915928999583 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 15 ===== \n",
            "75/78: Peak SnR (Whole): 32.30187747955322   L2 loss (Whole): 0.0005892485834192485   L2 loss (Mask): 0.051251194526751837   L1 loss (Whole): 0.003378011065845688   L1 loss (Mask): 0.29382003168265025   LPIPS (Whole): 0.031002231389284134   loss_discriminator: 0.13459351162115732   loss_generator1: 0.12968114386002222   loss_generator1_r: 0.052349521269400914   loss_generator1_adv: 0.767964874903361   loss_generator2: 0.12705619225899378   loss_generator2_r: 0.05015286778410276   loss_generator2_adv: 0.7636880842844646   loss_contrastive: 0.05348238522807757    \n",
            "7/7: Peak SnR (Whole): 32.561112540108816   L2 loss (Whole): 0.0005538216875200826   L2 loss (Mask): 0.04840588729296412   L1 loss (Whole): 0.0032663901303229587   L1 loss (Mask): 0.2855009010859898   LPIPS (Whole): 0.02978864511741059    \n",
            "Completed epoch 15! Took 5.116799541314443 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 16 ===== \n",
            "75/78: Peak SnR (Whole): 32.44165925343832   L2 loss (Whole): 0.0005698766148028274   L2 loss (Mask): 0.04933653416732947   L1 loss (Whole): 0.003316489003288249   L1 loss (Mask): 0.287117901245753   LPIPS (Whole): 0.03104137888799111   loss_discriminator: 0.13212105482816697   loss_generator1: 0.12730799476305643   loss_generator1_r: 0.04983695482214292   loss_generator1_adv: 0.7696678407986959   loss_generator2: 0.12935536473989487   loss_generator2_r: 0.04883611351251602   loss_generator2_adv: 0.8001535820960999   loss_contrastive: 0.05040724754333496        \n",
            "7/7: Peak SnR (Whole): 32.48928887503488   L2 loss (Whole): 0.0005636644865652281   L2 loss (Mask): 0.048718257141964774   L1 loss (Whole): 0.003275659949784832   L1 loss (Mask): 0.28305610162871225   LPIPS (Whole): 0.029216391167470386   \n",
            "Completed epoch 16! Took 4.845855557918549 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 17 ===== \n",
            "75/78: Peak SnR (Whole): 32.547101135253904   L2 loss (Whole): 0.0005567523377249017   L2 loss (Mask): 0.04833813120921453   L1 loss (Whole): 0.0032522355299443007   L1 loss (Mask): 0.2823567984501521   LPIPS (Whole): 0.029818767681717873   loss_discriminator: 0.1309535636504491   loss_generator1: 0.12474666476249695   loss_generator1_r: 0.04913333937525749   loss_generator1_adv: 0.7512543733914693   loss_generator2: 0.13200496445099513   loss_generator2_r: 0.047542923043171564   loss_generator2_adv: 0.8397524825731914   loss_contrastive: 0.04873389864961306       \n",
            "7/7: Peak SnR (Whole): 32.54679788861956   L2 loss (Whole): 0.0005551673218308549   L2 loss (Mask): 0.0485136014010225   L1 loss (Whole): 0.0032176127091848423   L1 loss (Mask): 0.2811168432235718   LPIPS (Whole): 0.02877308308546032   7   \n",
            "Completed epoch 17! Took 4.909551525115967 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 18 ===== \n",
            "75/78: Peak SnR (Whole): 32.40448183695475   L2 loss (Whole): 0.0005753300255552555   L2 loss (Mask): 0.04968348433574041   L1 loss (Whole): 0.0033177599729970098   L1 loss (Mask): 0.286538393497467   LPIPS (Whole): 0.03034334828456243   loss_discriminator: 0.1320633350809415   loss_generator1: 0.12759789605935415   loss_generator1_r: 0.050384498685598376   loss_generator1_adv: 0.7673909219106039   loss_generator2: 0.13016109108924867   loss_generator2_r: 0.04898246998588244   loss_generator2_adv: 0.8070478447278341   loss_contrastive: 0.04740692978103955      \n",
            "7/7: Peak SnR (Whole): 32.76269721984863   L2 loss (Whole): 0.0005292190929009978   L2 loss (Mask): 0.04568872760449137   L1 loss (Whole): 0.003141056263952383   L1 loss (Mask): 0.27122722779001507   LPIPS (Whole): 0.028376898728311062      \n",
            "Completed epoch 18! Took 4.783241625626882 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 19 ===== \n",
            "75/78: Peak SnR (Whole): 32.45714831034343   L2 loss (Whole): 0.0005682027553363392   L2 loss (Mask): 0.04913475555678209   L1 loss (Whole): 0.00328752795389543   L1 loss (Mask): 0.28429817616939546   LPIPS (Whole): 0.03097507040947676   loss_discriminator: 0.13316128492355347   loss_generator1: 0.1290082213282585   loss_generator1_r: 0.04978933533032735   loss_generator1_adv: 0.7875697620709737   loss_generator2: 0.1279016097386678   loss_generator2_r: 0.04848017578323682   loss_generator2_adv: 0.7895952876408895   loss_contrastive: 0.04619050788382689        \n",
            "7/7: Peak SnR (Whole): 32.47834055764334   L2 loss (Whole): 0.000564847099927387   L2 loss (Mask): 0.048984539828130176   L1 loss (Whole): 0.0032582340562450035   L1 loss (Mask): 0.28261240252426695   LPIPS (Whole): 0.030069033349198953   \n",
            "Completed epoch 19! Took 5.3896879474322 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 20 ===== \n",
            "75/78: Peak SnR (Whole): 32.358059043884275   L2 loss (Whole): 0.0005813548046474656   L2 loss (Mask): 0.05025458842515945   L1 loss (Whole): 0.003321457311200599   L1 loss (Mask): 0.2871203408638636   LPIPS (Whole): 0.03164019793272019   loss_discriminator: 0.13257072269916534   loss_generator1: 0.12918283879756928   loss_generator1_r: 0.0508225683371226   loss_generator1_adv: 0.7790453839302063   loss_generator2: 0.12860441267490386   loss_generator2_r: 0.04968660851319631   loss_generator2_adv: 0.7846063168843588   loss_contrastive: 0.04564510514338811        \n",
            "7/7: Peak SnR (Whole): 32.38106182643345   L2 loss (Whole): 0.0005777616544427085   L2 loss (Mask): 0.049660534997071536   L1 loss (Whole): 0.003294433151105685   L1 loss (Mask): 0.28311149988855633   LPIPS (Whole): 0.030398478864559104    \n",
            "Completed epoch 20! Took 4.96390946706136 min\n",
            "Saved models!\n",
            "\n",
            "===== Epoch: 21 ===== \n",
            "75/78: Peak SnR (Whole): 32.47911348978678   L2 loss (Whole): 0.0005654995645939683   L2 loss (Mask): 0.04907092807193597   L1 loss (Whole): 0.0032645830077429614   L1 loss (Mask): 0.2832759360472361   LPIPS (Whole): 0.031371532504757244   loss_discriminator: 0.13248163143793742   loss_generator1: 0.128286847670873   loss_generator1_r: 0.04964390630523364   loss_generator1_adv: 0.7819203710556031   loss_generator2: 0.12787982414166132   loss_generator2_r: 0.0484979498386383   loss_generator2_adv: 0.7893136580785115   loss_contrastive: 0.04507047784825166   9     \n",
            "7/7: Peak SnR (Whole): 32.60321113041469   L2 loss (Whole): 0.0005496112114217665   L2 loss (Mask): 0.04772375923182283   L1 loss (Whole): 0.0031804525393194388   L1 loss (Mask): 0.27625399827957153   LPIPS (Whole): 0.030090121685394218     \n",
            "Completed epoch 21! Took 4.755514661471049 min\n",
            "Saved models!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'Peak SnR (Whole)': [30.735400676727295,\n",
              "   31.151127619621082,\n",
              "   31.459579272147938,\n",
              "   31.692329492324436,\n",
              "   31.941340311979637,\n",
              "   32.015968934083595,\n",
              "   32.15032104345468,\n",
              "   32.21265817299867,\n",
              "   32.22223741580279,\n",
              "   32.407617079905975,\n",
              "   32.43529154704167,\n",
              "   32.44203859720475,\n",
              "   32.28815243794368,\n",
              "   32.29551962094429,\n",
              "   32.43759193175878,\n",
              "   32.547223115578674,\n",
              "   32.40239258301564,\n",
              "   32.45111541258983,\n",
              "   32.36448396780552,\n",
              "   32.47878331404466],\n",
              "  'L2 loss (Whole)': [0.0008477815314948272,\n",
              "   0.0007682936407852536,\n",
              "   0.0007162326573728559,\n",
              "   0.000678657080219963,\n",
              "   0.0006414199552427118,\n",
              "   0.0006306035218772311,\n",
              "   0.0006106423933464938,\n",
              "   0.000601833061059901,\n",
              "   0.0006005199799964873,\n",
              "   0.0005756769083419408,\n",
              "   0.0005716364603125466,\n",
              "   0.0005718370276810721,\n",
              "   0.0005914723873362304,\n",
              "   0.0005900680027060354,\n",
              "   0.0005704453308284521,\n",
              "   0.0005567698168213097,\n",
              "   0.0005756215189019433,\n",
              "   0.0005689643790575269,\n",
              "   0.0005804705687273198,\n",
              "   0.0005655413986413549],\n",
              "  'L2 loss (Mask)': [0.07373524488260348,\n",
              "   0.06662650645161286,\n",
              "   0.061916654404157244,\n",
              "   0.058816965048511825,\n",
              "   0.05561074564376703,\n",
              "   0.05453603554708071,\n",
              "   0.05278587627869386,\n",
              "   0.05209868733221904,\n",
              "   0.05212810996155708,\n",
              "   0.050025152878310435,\n",
              "   0.049458971103796594,\n",
              "   0.04943629201406088,\n",
              "   0.051310923213186935,\n",
              "   0.051306902359311395,\n",
              "   0.04937743858840221,\n",
              "   0.04836623289455206,\n",
              "   0.04972984278813387,\n",
              "   0.04920164161385634,\n",
              "   0.05014964976371863,\n",
              "   0.04908398128090761],\n",
              "  'L1 loss (Whole)': [0.0041910058263546,\n",
              "   0.00397670672287066,\n",
              "   0.003823987724713217,\n",
              "   0.003697084239087044,\n",
              "   0.0035786481995063904,\n",
              "   0.003546872547863481,\n",
              "   0.003472686352697798,\n",
              "   0.003436865223846279,\n",
              "   0.0034346004771904494,\n",
              "   0.003339414922102617,\n",
              "   0.0033256963629705403,\n",
              "   0.0033214614481832353,\n",
              "   0.003386854223954754,\n",
              "   0.00338155109112939,\n",
              "   0.0033173875298160007,\n",
              "   0.003251928088362687,\n",
              "   0.0033177486113392008,\n",
              "   0.003289034097598722,\n",
              "   0.0033198153907552552,\n",
              "   0.0032640758227222622],\n",
              "  'L1 loss (Mask)': [0.3645346919313455,\n",
              "   0.34486251247998995,\n",
              "   0.33055923649897945,\n",
              "   0.32040322380952346,\n",
              "   0.31025376400122273,\n",
              "   0.3067561183602382,\n",
              "   0.3001828508881422,\n",
              "   0.29751938027449143,\n",
              "   0.298121079038351,\n",
              "   0.2902136863424228,\n",
              "   0.2877045621474584,\n",
              "   0.2871977157699756,\n",
              "   0.29381167124479246,\n",
              "   0.2940395646370374,\n",
              "   0.28714424008742356,\n",
              "   0.2824895842335163,\n",
              "   0.2866517362686304,\n",
              "   0.2844319802064162,\n",
              "   0.28681311106834656,\n",
              "   0.28328331980185634],\n",
              "  'LPIPS (Whole)': [0.04284137239058813,\n",
              "   0.04178853488216797,\n",
              "   0.04033240131460703,\n",
              "   0.038898660825231135,\n",
              "   0.03795895281319435,\n",
              "   0.03757037900579281,\n",
              "   0.03698300899794468,\n",
              "   0.03723859844299463,\n",
              "   0.03763127496513801,\n",
              "   0.03613677551635565,\n",
              "   0.035453427833719894,\n",
              "   0.03469279933816347,\n",
              "   0.033410603717829175,\n",
              "   0.031008672315436296,\n",
              "   0.0310142318216654,\n",
              "   0.029784997137120135,\n",
              "   0.030333849816368177,\n",
              "   0.030937571962101337,\n",
              "   0.03160861969137421,\n",
              "   0.03135950329641883],\n",
              "  'loss_discriminator': [0.1389465631964879,\n",
              "   0.1384630283484092,\n",
              "   0.13803096000964826,\n",
              "   0.1374278847987835,\n",
              "   0.13652741889923048,\n",
              "   0.13526880798431543,\n",
              "   0.1328320759229171,\n",
              "   0.13240333990408823,\n",
              "   0.13198665319344935,\n",
              "   0.12964560597752914,\n",
              "   0.12647725202334234,\n",
              "   0.12434681667349277,\n",
              "   0.12772353738546371,\n",
              "   0.13444485610876328,\n",
              "   0.13196733374244127,\n",
              "   0.13088634763008508,\n",
              "   0.13207943785266998,\n",
              "   0.13315749817933792,\n",
              "   0.1324878338819895,\n",
              "   0.13241761750899828],\n",
              "  'loss_generator1': [0.1532676133971948,\n",
              "   0.14319809621725327,\n",
              "   0.13845685105293226,\n",
              "   0.13513408629940107,\n",
              "   0.13189326494168013,\n",
              "   0.13111584518964475,\n",
              "   0.13200010244662946,\n",
              "   0.13224374244992548,\n",
              "   0.13309504674413267,\n",
              "   0.131287020081893,\n",
              "   0.13241781160617486,\n",
              "   0.1348663669748184,\n",
              "   0.13504505759248367,\n",
              "   0.1297475683192412,\n",
              "   0.1272338455877243,\n",
              "   0.12467644697962663,\n",
              "   0.12766801479917306,\n",
              "   0.1288045133726719,\n",
              "   0.12926568558964974,\n",
              "   0.1282232241370739],\n",
              "  'loss_generator1_r': [0.07679920275815022,\n",
              "   0.0686214317400486,\n",
              "   0.06430993678096013,\n",
              "   0.06105505402844686,\n",
              "   0.05709176935637609,\n",
              "   0.05574473284949095,\n",
              "   0.05401137332694653,\n",
              "   0.05291208543647558,\n",
              "   0.052881815924476355,\n",
              "   0.051026870807011925,\n",
              "   0.0503085270906106,\n",
              "   0.05006331950426102,\n",
              "   0.05187937574317822,\n",
              "   0.052384808468513,\n",
              "   0.0498925102635836,\n",
              "   0.04915246730431532,\n",
              "   0.050441680619349845,\n",
              "   0.04984991825543917,\n",
              "   0.05071305158810738,\n",
              "   0.049662306618232],\n",
              "  'loss_generator1_adv': [0.7017882466316223,\n",
              "   0.7035126242882166,\n",
              "   0.7099999342209253,\n",
              "   0.7163111834953992,\n",
              "   0.728303713676257,\n",
              "   0.7373054776436243,\n",
              "   0.7661463152139615,\n",
              "   0.7815786875211276,\n",
              "   0.7918785069233332,\n",
              "   0.7936442272785382,\n",
              "   0.8133070163237743,\n",
              "   0.8412003891590314,\n",
              "   0.8256402306067638,\n",
              "   0.7682836139813448,\n",
              "   0.7683771092158097,\n",
              "   0.7503636020880479,\n",
              "   0.7675246390012594,\n",
              "   0.7849250168372424,\n",
              "   0.7809689786189642,\n",
              "   0.7811007140538632],\n",
              "  'loss_generator2': [0.14705864187234488,\n",
              "   0.1390912781159083,\n",
              "   0.133362575314748,\n",
              "   0.1303250074195556,\n",
              "   0.1286880900271428,\n",
              "   0.12823105001678833,\n",
              "   0.1282542271491809,\n",
              "   0.12696198708353898,\n",
              "   0.1270192342881973,\n",
              "   0.12847168466601616,\n",
              "   0.1307799570644513,\n",
              "   0.13311229082636344,\n",
              "   0.13403312489390373,\n",
              "   0.12724880711772504,\n",
              "   0.12941998711381203,\n",
              "   0.1319546943100599,\n",
              "   0.13009420925608048,\n",
              "   0.12778164389041755,\n",
              "   0.12870505862893203,\n",
              "   0.127845315883557],\n",
              "  'loss_generator2_r': [0.07067128700705674,\n",
              "   0.06463158116317712,\n",
              "   0.05952337202735436,\n",
              "   0.05657887606857679,\n",
              "   0.05412972193115797,\n",
              "   0.05332733824467047,\n",
              "   0.05156037923044119,\n",
              "   0.051285289227962494,\n",
              "   0.05137440399863781,\n",
              "   0.04902343494960895,\n",
              "   0.04860941511698258,\n",
              "   0.04880926452386074,\n",
              "   0.05074247068319565,\n",
              "   0.05022899625010979,\n",
              "   0.04886236691322082,\n",
              "   0.0475799984847888,\n",
              "   0.04901800495691788,\n",
              "   0.048553364972273506,\n",
              "   0.04958624793932988,\n",
              "   0.04850565594358322],\n",
              "  'loss_generator2_adv': [0.7012726970207996,\n",
              "   0.7024601697921753,\n",
              "   0.7069204831734682,\n",
              "   0.7130397680478219,\n",
              "   0.7258996566136678,\n",
              "   0.7326371616277939,\n",
              "   0.753238340983024,\n",
              "   0.7450160246628982,\n",
              "   0.7462217494463309,\n",
              "   0.7855247648862692,\n",
              "   0.8139206843498425,\n",
              "   0.8362031624867365,\n",
              "   0.8269051336325132,\n",
              "   0.7648599514594445,\n",
              "   0.8005438469923459,\n",
              "   0.8388801125379709,\n",
              "   0.8060287458774371,\n",
              "   0.7876620858143537,\n",
              "   0.7866179606853387,\n",
              "   0.788891777014121],\n",
              "  'loss_contrastive': [0.6274834235127156,\n",
              "   0.42195391922425,\n",
              "   0.314703779342847,\n",
              "   0.2445032492471047,\n",
              "   0.1969762010834156,\n",
              "   0.16402783474096885,\n",
              "   0.13720547068768588,\n",
              "   0.11744409976288295,\n",
              "   0.1024016002431894,\n",
              "   0.08957485157327774,\n",
              "   0.07785267158387563,\n",
              "   0.06828576364578345,\n",
              "   0.06008984005221954,\n",
              "   0.05341053345742134,\n",
              "   0.05034280565973276,\n",
              "   0.048715035717647806,\n",
              "   0.047359848800951086,\n",
              "   0.0462079573995792,\n",
              "   0.04563742066518618,\n",
              "   0.04506626085211069]},\n",
              " {'Peak SnR (Whole)': [31.09635434831892,\n",
              "   31.335096631731307,\n",
              "   31.85880974360875,\n",
              "   31.578236307416642,\n",
              "   32.40780626024519,\n",
              "   32.428293909345356,\n",
              "   31.98050022125244,\n",
              "   32.53041294642857,\n",
              "   32.446840150015696,\n",
              "   32.6214975629534,\n",
              "   32.76002938406808,\n",
              "   32.293819836207796,\n",
              "   32.21692780085972,\n",
              "   32.561112540108816,\n",
              "   32.48928887503488,\n",
              "   32.54679788861956,\n",
              "   32.76269721984863,\n",
              "   32.47834055764334,\n",
              "   32.38106182643345,\n",
              "   32.60321113041469],\n",
              "  'L2 loss (Whole)': [0.0007771012439791645,\n",
              "   0.0007338924624491483,\n",
              "   0.0006519649843020099,\n",
              "   0.0006948028250397849,\n",
              "   0.0005743799390204783,\n",
              "   0.0005735124369883644,\n",
              "   0.0006331060680427722,\n",
              "   0.000556826067622751,\n",
              "   0.0005691472075081297,\n",
              "   0.0005461316031869501,\n",
              "   0.0005292653783856492,\n",
              "   0.0005889751898523952,\n",
              "   0.0006001405584226761,\n",
              "   0.0005538216875200826,\n",
              "   0.0005636644865652281,\n",
              "   0.0005551673218308549,\n",
              "   0.0005292190929009978,\n",
              "   0.000564847099927387,\n",
              "   0.0005777616544427085,\n",
              "   0.0005496112114217665],\n",
              "  'L2 loss (Mask)': [0.06753567659429141,\n",
              "   0.06398662179708481,\n",
              "   0.05652721944664206,\n",
              "   0.05943320531930242,\n",
              "   0.05006111493068082,\n",
              "   0.04937875936073916,\n",
              "   0.05461998683001314,\n",
              "   0.048829932031886916,\n",
              "   0.0491833532495158,\n",
              "   0.04699949600866863,\n",
              "   0.04547923510628087,\n",
              "   0.0507542791643313,\n",
              "   0.0522747848715101,\n",
              "   0.04840588729296412,\n",
              "   0.048718257141964774,\n",
              "   0.0485136014010225,\n",
              "   0.04568872760449137,\n",
              "   0.048984539828130176,\n",
              "   0.049660534997071536,\n",
              "   0.04772375923182283],\n",
              "  'L1 loss (Whole)': [0.0039976856704535225,\n",
              "   0.003815450905156987,\n",
              "   0.0035953201253765394,\n",
              "   0.003730066237039864,\n",
              "   0.0033240020607731174,\n",
              "   0.0033233314918886337,\n",
              "   0.0034962641407868694,\n",
              "   0.0032855646152581486,\n",
              "   0.0033075838083667414,\n",
              "   0.003242827735708228,\n",
              "   0.003165267940078463,\n",
              "   0.003400827192568353,\n",
              "   0.003397507526512657,\n",
              "   0.0032663901303229587,\n",
              "   0.003275659949784832,\n",
              "   0.0032176127091848423,\n",
              "   0.003141056263952383,\n",
              "   0.0032582340562450035,\n",
              "   0.003294433151105685,\n",
              "   0.0031804525393194388],\n",
              "  'L1 loss (Mask)': [0.3474681334836142,\n",
              "   0.33248997798987795,\n",
              "   0.31175120174884796,\n",
              "   0.3190245968954904,\n",
              "   0.2897902599402836,\n",
              "   0.2861614567892892,\n",
              "   0.3016721499817712,\n",
              "   0.2880394629069737,\n",
              "   0.285821938088962,\n",
              "   0.2790029261793409,\n",
              "   0.2720458741698946,\n",
              "   0.29302269007478443,\n",
              "   0.29592682846954893,\n",
              "   0.2855009010859898,\n",
              "   0.28305610162871225,\n",
              "   0.2811168432235718,\n",
              "   0.27122722779001507,\n",
              "   0.28261240252426695,\n",
              "   0.28311149988855633,\n",
              "   0.27625399827957153],\n",
              "  'LPIPS (Whole)': [0.04218321852385998,\n",
              "   0.03836120132889066,\n",
              "   0.03789890690573624,\n",
              "   0.03829991285290037,\n",
              "   0.035420920965926986,\n",
              "   0.03574412901486669,\n",
              "   0.03604691502239023,\n",
              "   0.035069052928260395,\n",
              "   0.03558608304177012,\n",
              "   0.03419241708304201,\n",
              "   0.032870562481028695,\n",
              "   0.03322538480694805,\n",
              "   0.030884580154504095,\n",
              "   0.02978864511741059,\n",
              "   0.029216391167470386,\n",
              "   0.02877308308546032,\n",
              "   0.028376898728311062,\n",
              "   0.030069033349198953,\n",
              "   0.030398478864559104,\n",
              "   0.030090121685394218]})"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "train_evaluate(device, train_dataset, valid_dataset, TRAINING_PARAMS, METRICS, start_epoch = 1, log_wandb = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrCk_e1oU44V"
      },
      "outputs": [],
      "source": [
        "# train_epoch(device, DataLoader(train_dataset, batch_size = 16), TRAINING_PARAMS, METRICS, log_wandb = True)\n",
        "# evaluate_epoch(device, DataLoader(valid_dataset, batch_size = 16),, TRAINING_PARAMS, metrics, log_wandb = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ContrastiveGAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
